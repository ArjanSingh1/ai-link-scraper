#!/usr/bin/env python3
"""
AI Link Scraper - Main Entry Point

This script monitors Slack channels for shared links, scrapes their content,
generates AI summaries, and saves them in an organized folder structure.
"""

import argparse
import os
import sys
from datetime import datetime, timedelta
from config.settings import settings
from src.utils import setup_logging, save_summary_to_file, format_summary_data
from src.slack_client import SlackClient
from src.web_scraper import WebScraper
from src.summarizer import Summarizer

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='AI Link Scraper - Automatically scrape and summarize links from Slack'
    )
    
    parser.add_argument(
        '--start-date',
        type=str,
        help='Start date for processing messages (YYYY-MM-DD format)'
    )
    
    parser.add_argument(
        '--end-date', 
        type=str,
        help='End date for processing messages (YYYY-MM-DD format)'
    )
    
    parser.add_argument(
        '--limit',
        type=int,
        help='Maximum number of messages to process'
    )
    
    parser.add_argument(
        '--output-folder',
        type=str,
        default=settings.OUTPUT_FOLDER,
        help='Output folder for summaries'
    )
    
    parser.add_argument(
        '--max-links',
        type=int,
        default=settings.MAX_LINKS_PER_RUN,
        help='Maximum number of links to process'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Enable verbose logging'
    )
    
    parser.add_argument(
        '--send-to-slack',
        action='store_true',
        help='Send summaries back to Slack channel as messages'
    )
    
    parser.add_argument(
        '--upload-to-slack',
        action='store_true',
        help='Upload summary files to Slack channel'
    )
    
    parser.add_argument(
        '--share-digest',
        action='store_true',
        help='Send a digest of recent summaries to Slack'
    )
    
    parser.add_argument(
        '--share-all-summaries',
        action='store_true',
        help='Create and share a complete summary collection file to Slack'
    )
    
    parser.add_argument(
        '--check-mentions',
        action='store_true',
        help='Check for recent mentions and respond with link summaries'
    )
    
    parser.add_argument(
        '--check-all-channels',
        action='store_true',
        help='Check for mentions across all accessible channels (not just the configured channel)'
    )
    
    parser.add_argument(
        '--create-weekly-document',
        action='store_true',
        help='Create a weekly summary document (PDF/HTML) with all summaries from the past week'
    )
    
    parser.add_argument(
        '--scrape-to-drive',
        action='store_true',
        help='Scrape all links (without summarizing) and upload to Google Drive as CSV/JSON/HTML/PDF'
    )
    
    parser.add_argument(
        '--drive-folder-name',
        type=str,
        help='Custom folder name for Google Drive upload (default: auto-generated with timestamp)'
    )
    
    parser.add_argument(
        '--pdf-detailed',
        action='store_true',
        help='Include full content in PDF report (default: includes content previews)'
    )
    
    return parser.parse_args()

def parse_date(date_string):
    """Parse date string to datetime object"""
    try:
        return datetime.strptime(date_string, '%Y-%m-%d')
    except ValueError:
        raise ValueError(f"Invalid date format: {date_string}. Use YYYY-MM-DD format.")

def main():
    """Main execution function"""
    try:
        # Parse arguments
        args = parse_arguments()
        
        # Setup logging
        log_level = "DEBUG" if args.verbose else settings.LOG_LEVEL
        os.makedirs('logs', exist_ok=True)
        logger = setup_logging(log_level)
        
        logger.info("Starting AI Link Scraper")
        logger.info(f"Output folder: {args.output_folder}")
        
        # Validate settings
        settings.validate()
        logger.info("Configuration validated successfully")
        
        # Parse dates if provided
        start_date = None
        end_date = None
        
        if args.start_date:
            start_date = parse_date(args.start_date)
            logger.info(f"Start date: {start_date}")
            
        if args.end_date:
            end_date = parse_date(args.end_date)
            logger.info(f"End date: {end_date}")
        
        # Initialize components
        logger.info("Initializing Slack client...")
        slack_client = SlackClient()
        
        # Test Slack connection
        if not slack_client.test_connection():
            logger.error("Failed to connect to Slack. Please check your configuration.")
            return 1
        
        # Get channel info
        channel_info = slack_client.get_channel_info()
        if channel_info:
            logger.info(f"Connected to channel: {channel_info.get('name', 'Unknown')}")
        
        # Handle mention checking (separate from regular processing)
        if args.check_mentions:
            logger.info("Checking for recent mentions in configured channel...")
            mentions_processed = slack_client.check_for_mentions(start_date=start_date)
            logger.info(f"Processed {mentions_processed} mentions")
            return 0
        
        if args.check_all_channels:
            logger.info("Checking for recent mentions across all accessible channels...")
            mentions_processed = slack_client.check_all_channels_for_mentions()
            logger.info(f"Processed {mentions_processed} mentions across all channels")
            return 0
        
        # Handle Google Drive scraping (separate from regular processing)
        if args.scrape_to_drive:
            logger.info("Scraping links for Google Drive export...")
            from src.link_processor import LinkProcessor
            
            # Fetch messages from Slack
            logger.info("Fetching messages from Slack...")
            messages = slack_client.get_channel_messages(
                start_date=start_date,
                end_date=end_date,
                limit=args.limit
            )
            
            if not messages:
                logger.warning("No messages found in the specified time range")
                return 0
            
            # Extract links from messages
            logger.info("Extracting links from messages...")
            links_data = slack_client.extract_links_from_messages(messages)
            
            if not links_data:
                logger.warning("No links found in messages")
                return 0
            
            # Limit number of links to process
            if len(links_data) > args.max_links:
                logger.info(f"Limiting to {args.max_links} links (found {len(links_data)})")
                links_data = links_data[:args.max_links]
            
            # Process links for Google Drive
            processor = LinkProcessor()
            processed_data = processor.scrape_links_for_drive(links_data, 'scraped_links')
            
            if processed_data:
                # Upload to Google Drive
                upload_result = processor.upload_to_google_drive(
                    processed_data, 
                    args.drive_folder_name or "ai-link-scraper"
                )
                
                if upload_result:
                    logger.info("=== GOOGLE DRIVE EXPORT COMPLETE ===")
                    logger.info(f"Total links processed: {upload_result['total_items']}")
                    if upload_result.get('folder_link'):
                        logger.info(f"üìÅ Google Drive folder: {upload_result['folder_link']}")
                    if upload_result.get('csv'):
                        logger.info(f"üìä CSV file: {upload_result['csv']['web_link']}")
                    if upload_result.get('json'):
                        logger.info(f"üìÑ JSON file: {upload_result['json']['web_link']}")
                    if upload_result.get('html'):
                        logger.info(f"üåê HTML report: {upload_result['html']['web_link']}")
                    if upload_result.get('pdf'):
                        logger.info(f"üìã PDF report: {upload_result['pdf']['web_link']}")
                    return 0
                else:
                    logger.error("Failed to upload to Google Drive")
                    return 1
            else:
                logger.error("Failed to process links for Google Drive")
                return 1
        
        # Regular processing continues below...
        
        # Fetch messages from Slack
        logger.info("Fetching messages from Slack...")
        messages = slack_client.get_channel_messages(
            start_date=start_date,
            end_date=end_date,
            limit=args.limit
        )
        
        if not messages:
            logger.warning("No messages found in the specified time range")
            return 0
        
        # Extract links from messages
        logger.info("Extracting links from messages...")
        links_data = slack_client.extract_links_from_messages(messages)
        
        if not links_data:
            logger.warning("No links found in messages")
            return 0
        
        # Limit number of links to process
        if len(links_data) > args.max_links:
            logger.info(f"Limiting to {args.max_links} links (found {len(links_data)})")
            links_data = links_data[:args.max_links]
        
        # Extract unique URLs
        urls = list(set([link['url'] for link in links_data]))
        logger.info(f"Processing {len(urls)} unique URLs")
        
        # Initialize web scraper and scrape content
        logger.info("Initializing web scraper...")
        scraper = WebScraper()
        
        logger.info("Scraping web content...")
        scraped_data = scraper.batch_scrape(urls)
        
        if not scraped_data:
            logger.warning("No content was successfully scraped")
            return 0
        
        successful_scrapes = [data for data in scraped_data if data.get('status') == 'success']
        logger.info(f"Successfully scraped {len(successful_scrapes)} out of {len(urls)} URLs")
        
        # Initialize summarizer and generate summaries
        logger.info("Initializing AI summarizer...")
        summarizer = Summarizer()
        
        logger.info("Generating summaries...")
        summaries = summarizer.batch_summarize(successful_scrapes)
        
        if not summaries:
            logger.warning("No summaries were generated")
            return 0
        
        # Save summaries to files
        logger.info(f"Saving {len(summaries)} summaries to {args.output_folder}")
        saved_files = []
        
        for summary in summaries:
            # Find corresponding Slack message data
            slack_data = next(
                (link for link in links_data if link['url'] == summary['url']),
                {}
            )
            
            # Format summary data
            formatted_summary = format_summary_data(
                url=summary['url'],
                title=summary['title'],
                summary=summary['summary'],
                slack_message_id=slack_data.get('slack_message_id'),
                word_count=summary.get('word_count', 0),
                tags=summary.get('tags', [])
            )
            
            # Add additional metadata
            formatted_summary['slack_user'] = slack_data.get('user')
            formatted_summary['slack_timestamp'] = slack_data.get('timestamp').isoformat() if slack_data.get('timestamp') else None
            formatted_summary['original_message'] = slack_data.get('message_text')
            
            # Save to file
            filepath = save_summary_to_file(formatted_summary, args.output_folder)
            saved_files.append(filepath)
            logger.info(f"Saved summary: {os.path.basename(filepath)}")
            
            # Send to Slack if requested
            if args.send_to_slack:
                logger.info(f"Sending summary to Slack: {formatted_summary['title']}")
                message_ts = slack_client.send_summary_to_channel(formatted_summary, reply_to_message=True)
                if message_ts:
                    logger.info(f"‚úÖ Successfully sent to Slack")
                else:
                    logger.warning(f"‚ùå Failed to send to Slack")
            
            # Upload file to Slack if requested
            if args.upload_to_slack:
                logger.info(f"Uploading file to Slack: {os.path.basename(filepath)}")
                comment = f"üìÑ Summary for: {formatted_summary['title']}"
                thread_ts = formatted_summary.get('slack_message_id') if args.send_to_slack else None
                file_id = slack_client.upload_file_to_channel(
                    filepath, 
                    title=f"Summary - {formatted_summary['title']}", 
                    comment=comment,
                    thread_ts=thread_ts
                )
                if file_id:
                    logger.info(f"‚úÖ Successfully uploaded to Slack")
                else:
                    logger.warning(f"‚ùå Failed to upload to Slack")
        
        # Summary report
        logger.info("=== PROCESSING COMPLETE ===")
        logger.info(f"Messages processed: {len(messages)}")
        logger.info(f"Links found: {len(links_data)}")
        logger.info(f"URLs scraped: {len(urls)}")
        logger.info(f"Successful scrapes: {len(successful_scrapes)}")
        logger.info(f"Summaries generated: {len(summaries)}")
        logger.info(f"Files saved: {len(saved_files)}")
        logger.info(f"Output folder: {os.path.abspath(args.output_folder)}")
        
        if args.send_to_slack:
            logger.info("üì® Summaries sent to Slack as messages")
        if args.upload_to_slack:
            logger.info("üìé Summary files uploaded to Slack")
        
        # Handle additional sharing options
        if args.share_digest:
            logger.info("Creating summary digest...")
            digest_ts = slack_client.send_summary_digest(args.output_folder, max_summaries=5)
            if digest_ts:
                logger.info("‚úÖ Summary digest sent to Slack")
            else:
                logger.warning("‚ùå Failed to send summary digest")
        
        if args.share_all_summaries:
            logger.info("Creating complete summary collection...")
            file_id = slack_client.share_complete_summary_folder(args.output_folder)
            if file_id:
                logger.info("‚úÖ Complete summary collection shared to Slack")
            else:
                logger.warning("‚ùå Failed to share complete summary collection")
        
        if args.create_weekly_document:
            logger.info("Creating weekly summary document...")
            from src.summary_organizer import create_weekly_document
            from datetime import datetime, timedelta
            
            # Create weekly document for the past 7 days
            start_date = datetime.now() - timedelta(days=7)
            document_path = create_weekly_document(args.output_folder, start_date)
            
            if document_path:
                logger.info(f"‚úÖ Weekly summary document created: {os.path.basename(document_path)}")
                
                # If send_to_slack is also specified, upload the document
                if args.send_to_slack:
                    logger.info("Uploading weekly document to Slack...")
                    comment = f"üìä Weekly AI Link Summary for week of {start_date.strftime('%B %d, %Y')}"
                    file_id = slack_client.upload_file_to_channel(
                        document_path,
                        title=f"Weekly Summary - {start_date.strftime('%Y-%m-%d')}",
                        comment=comment
                    )
                    if file_id:
                        logger.info("‚úÖ Weekly document uploaded to Slack")
                    else:
                        logger.warning("‚ùå Failed to upload weekly document to Slack")
            else:
                logger.warning("‚ùå Failed to create weekly document")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}", exc_info=True)
        return 1

if __name__ == "__main__":
    sys.exit(main())
