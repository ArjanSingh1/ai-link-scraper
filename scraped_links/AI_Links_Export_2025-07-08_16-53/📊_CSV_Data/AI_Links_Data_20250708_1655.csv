url,title,category,word_count,scraped_at,domain,slack_user,slack_user_id,slack_timestamp,slack_channel,content_type,brief_description,article_summary,full_content,formatted_content,completeness_ratio
https://arxiv.org/abs/2412.18925,"HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs",Computer Science,933,2025-07-08T16:54:13.188779,arxiv.org,Morgann Thain,U05H9P2TGG0,2025-01-03T18:24:18.350639,,article,,,"rdf:RDF xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#""
xmlns:dc=""http://purl.org/dc/elements/1.1/""
xmlns:trackback=""http://madskills.com/public/xml/rss/module/trackback/"">
<rdf:Description
rdf:about=""/abs/2412.18925""
dc:identifier=""/abs/2412.18925""
dc:title=""HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs""
trackback:ping=""/trackback/2412.18925"" />
</rdf:RDF>

Computer Science > Computation and Language

Computer Science > Computation and Language arXiv:2412.18925 [Submitted on 25 Dec 2024]

Title:HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs

Title: HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs Authors: Junying Chen Zhenyang Cai Xidong Wang Wanlong Liu Rongsheng Wang Jianye Hou Benyou Wang View a PDF of the paper titled HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs, by Junying Chen and 7 other authors View PDF
Abstract:The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains. Abstract: The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains. CONTEXT Subjects: Computation and Language (cs. CL); Artificial Intelligence (cs. AI); Machine Learning (cs. LG) Cite as: arXiv:2412.18925 [cs. CL] arXiv:2412.18925v1 [cs. CL] for this version) https://doi.org/10.48550/arXiv.2412.18925 Focus to learn more tooltip description arXiv-issued DOI via DataCite

Submission history

Submission history From: Junying Chen [ view email
Wed, 25 Dec 2024 15:12:34 UTC (1,151 KB)
end leftcolumn Full-text links:

Access Paper:

Access Paper: View a PDF of the paper titled HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs, by Junying Chen and 7 other authors • View PDF
View PDF • TeX Source
TeX Source • Other Formats
Other Formats view license end full-text Current browse context: < prev next >
recent 2024-12 Change to browse by:

References & Citations

References & Citations • NASA ADS
NASA ADS • Google Scholar
Google Scholar • Semantic Scholar
Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation

BibTeX formatted citation loading... Data provided by:

Bookmark

Bookmark end extra-services LABS AREA Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer What is the Explorer? Connected Papers Toggle Connected Papers What is Connected Papers? Litmaps Toggle Litmaps What is Litmaps? scite.ai Toggle scite Smart Citations What are Smart Citations? Code, Data, Media

Code, Data and Media Associated with this Article

Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv What is alphaXiv? Links to Code Toggle CatalyzeX Code Finder for Papers What is CatalyzeX? DagsHub Toggle DagsHub What is DagsHub? GotitPub Toggle Gotit.pub What is GotitPub? Huggingface Toggle Hugging Face What is Huggingface? Links to Code Toggle Papers with Code What is Papers with Code? ScienceCast Toggle ScienceCast What is ScienceCast? Demos

Replicate Toggle Replicate What is Replicate? Spaces Toggle Hugging Face Spaces What is Spaces? Spaces Toggle TXYZ. AI What is TXYZ. AI? Related Papers

Recommenders and Search Tools

Recommenders and Search Tools Link to Influence Flower Influence Flower What are Influence Flowers? Core recommender toggle CORE Recommender What is CORE? • Author
Author • Venue
• Institution

Institution • Topic
About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs END LABS AREA Which authors of this paper are endorsers? Disable MathJax What is MathJax?","**HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs**



rdf:RDF xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#""
xmlns:dc=""http://purl.org/dc/elements/1.1/""
xmlns:trackback=""http://madskills.com/public/xml/rss/module/trackback/"">
<rdf:Description
rdf:about=""/abs/2412.18925""
dc:identifier=""/abs/2412.18925""
dc:title=""HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs""
trackback:ping=""/trackback/2412.18925"" />
</rdf:RDF>

Computer Science > Computation and Language

Computer Science > Computation and Language arXiv:2412.18925 [Submitted on 25 Dec 2024]

Title:HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs

Key points:

• Title: HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs Authors: Junying Chen Zhenyang Cai Xidong Wang Wanlong Liu Rongsheng Wang Jianye Hou Benyou Wang View a PDF of the paper titled HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs, by Junying Chen and 7 other authors View PDF
Abstract:The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM.

• Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored.

• The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare.

• However, verifying medical reasoning is challenging, unlike those in mathematics.

• To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs.

• This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further.

Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains. Abstract: The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains. CONTEXT Subjects: Computation and Language (cs. CL); Artificial Intelligence (cs. AI); Machine Learning (cs. LG) Cite as: arXiv:2412.18925 [cs. CL] arXiv:2412.18925v1 [cs. CL] for this version) https://doi.org/10.48550/arXiv.2412.18925 Focus to learn more tooltip description arXiv-issued DOI via DataCite

Submission history

Submission history From: Junying Chen [ view email
Wed, 25 Dec 2024 15:12:34 UTC (1,151 KB)
end leftcolumn Full-text links:

Access Paper:

Access Paper: View a PDF of the paper titled HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs, by Junying Chen and 7 other authors • View PDF
## View PDF • TeX Source
TeX Source • Other Formats
Other Formats view license end full-text Current browse context: < prev next >
recent 2024-12 Change to browse by:

References & Citations

References & Citations • NASA ADS
## NASA ADS • Google Scholar
Google Scholar • Semantic Scholar
Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation

BibTeX formatted citation loading... Data provided by:

Bookmark

Bookmark end extra-services LABS AREA Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer What is the Explorer? Connected Papers Toggle Connected Papers What is Connected Papers? Litmaps Toggle Litmaps What is Litmaps? scite.ai Toggle scite Smart Citations What are Smart Citations? Code, Data, Media

Code, Data and Media Associated with this Article

Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv What is alphaXiv? Links to Code Toggle CatalyzeX Code Finder for Papers What is CatalyzeX? DagsHub Toggle DagsHub What is DagsHub? GotitPub Toggle Gotit.pub What is GotitPub? Huggingface Toggle Hugging Face What is Huggingface? Links to Code Toggle Papers with Code What is Papers with Code? ScienceCast Toggle ScienceCast What is ScienceCast? Demos

Replicate Toggle Replicate What is Replicate? Spaces Toggle Hugging Face Spaces What is Spaces? Spaces Toggle TXYZ. AI What is TXYZ. AI? Related Papers

Recommenders and Search Tools

Recommenders and Search Tools Link to Influence Flower Influence Flower What are Influence Flowers? Core recommender toggle CORE Recommender What is CORE? • Author
Author • Venue
• Institution

Institution • Topic
About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Key points:

• arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

• Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy.

• arXiv is committed to these values and only works with partners that adhere to them.

• Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy.

• arXiv is committed to these values and only works with partners that adhere to them.

• Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs END LABS AREA Which authors of this paper are endorsers? Disable MathJax What is MathJax?",1.0
https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-8B,FreedomIntelligence/HuatuoGPT-o1-8B · Hugging Face,Artificial Intelligence,514,2025-07-08T16:54:23.746014,huggingface.co,Morgann Thain,U05H9P2TGG0,2025-01-03T18:24:18.350639,,website,"A dedicated page for the HuatuoGPT-o1-8B medical language model, offering usage information and GitHub resources.",,"HTML_TAG_START

HuatuoGPT-o1-8B

HuatuoGPT-o1-8B GitHub

Introduction

Introduction HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response. HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response. For more information, visit our GitHub repository:
https://github.com/FreedomIntelligence/HuatuoGPT-o1. For more information, visit our GitHub repository: https://github.com/FreedomIntelligence/HuatuoGPT-o1

Model Info

Model Info Backbone Supported Languages HuatuoGPT-o1-8B LLaMA-3.1-8B English HF Link HuatuoGPT-o1-70B LLaMA-3.1-70B English HF Link HuatuoGPT-o1-7B Qwen2.5-7B English & Chinese HF Link HuatuoGPT-o1-72B Qwen2.5-72B English & Chinese HF Link

Usage

You can use HuatuoGPT-o1 in the same way as Llama-3.1-8B-Instruct. You can deploy it with tools like vllm or Sglang, or perform direct inference:

You can use HuatuoGPT-o1 in the same way as
Llama-3.1-8B-Instruct

Llama-3.1-8B-Instruct. You can deploy it with tools like Sglang, or perform direct inference:
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-8B"",torch_dtype=""auto"",device_map=""auto"")
tokenizer = AutoTokenizer.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-8B"")

input_text = ""How to stop a cough?""
messages = [{""role"": ""user"", ""content"": input_text}]

inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True
), return_tensors=""pt"").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=2048)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-8B"",torch_dtype=""auto"",device_map=""auto"")
tokenizer = AutoTokenizer.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-8B"")

input_text = ""How to stop a cough?""
messages = [{""role"": ""user"", ""content"": input_text}]

inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True
), return_tensors=""pt"").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=2048)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained( ""FreedomIntelligence/HuatuoGPT-o1-8B"",torch_dtype= ""auto"",device_map= ""auto"" )
tokenizer = AutoTokenizer.from_pretrained( ""FreedomIntelligence/HuatuoGPT-o1-8B"" )

input_text = ""How to stop a cough?"" messages = [{ ""role"" ""user"" ""content"": input_text}]

inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=,add_generation_prompt= ), return_tensors= ).to(model.device)
outputs = model.generate(**inputs, max_new_tokens= (tokenizer.decode(outputs[ ], skip_special_tokens= HuatuoGPT-o1 adopts a thinks-before-it-answers approach, with outputs formatted as:

HuatuoGPT-o1 adopts a thinks-before-it-answers approach, with outputs formatted as:
## Thinking
[Reasoning process]

## Final Response
[Output]

## Thinking
[Reasoning process]

## Final Response
[Output]

## Thinking
[Reasoning process]

## Final Response
[Output]

📖 Citation

📖 Citation
@misc{chen2024huatuogpto1medicalcomplexreasoning,
title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs},
author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
year={2024},
eprint={2412.18925},
archivePrefix={arXiv},
primaryClass={cs. CL},
url={https://arxiv.org/abs/2412.18925},
}

@misc{chen2024huatuogpto1medicalcomplexreasoning,
title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs},
author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
year={2024},
eprint={2412.18925},
archivePrefix={arXiv},
primaryClass={cs. CL},
url={https://arxiv.org/abs/2412.18925},
}

@misc{chen2024huatuogpto1medicalcomplexreasoning,
title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs},
author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
year={2024},
eprint={2412.18925},
archivePrefix={arXiv},
primaryClass={cs. CL},
url={https://arxiv.org/abs/2412.18925},
} HTML_TAG_END Downloads last month HTML_TAG_START HTML_TAG_END Safetensors Model size 8.03B params Tensor type Files info Inference Providers Text Generation This model isn't deployed by any Inference Provider. Ask for provider support

Model tree for FreedomIntelligence/HuatuoGPT-o1-8B

Model tree for FreedomIntelligence/HuatuoGPT-o1-8B Base model meta-llama/Llama-3.1-8B Finetuned meta-llama/Llama-3.1-8B-Instruct Finetuned this model Adapters 1 model Merges 8 models Quantizations 15 models

Datasets used to train
FreedomIntelligence/HuatuoGPT-o1-8B

Datasets used to train FreedomIntelligence/HuatuoGPT-o1-8B

Spaces using
FreedomIntelligence/HuatuoGPT-o1-8B
8

Spaces using FreedomIntelligence/HuatuoGPT-o1-8B

Collection including
FreedomIntelligence/HuatuoGPT-o1-8B

Collection including FreedomIntelligence/HuatuoGPT-o1-8B 4 items Updated Dec 30, 2024","**Website Description:**

The FreedomIntelligence/HuatuoGPT-o1-8B page on Hugging Face presents a sophisticated medical language model designed for advanced medical reasoning. This model, HuatuoGPT-o1, is capable of generating complex thought processes and refining its reasoning to deliver accurate responses. The website provides detailed information about the model's capabilities, usage instructions, and links to its GitHub repository for further exploration and integration.

Targeted towards developers, researchers, and healthcare professionals, the site offers essential resources for deploying the HuatuoGPT-o1 model in various applications. Key features include support for multiple languages, deployment options with popular tools, and comprehensive model information, making it a valuable resource for those looking to enhance their medical AI applications.",1.0
https://www.theunwindai.com,unwind ai,Artificial Intelligence,611,2025-07-08T16:54:34.180206,www.theunwindai.com,Cristin Connerney,U059KFGJUSU,2025-01-07T10:29:13.976839,,website,"Unwind AI provides the latest news, tools, and tutorials for AI developers.",,"unwind ai

unwind ai Latest AI news, tools and tutorials for AI Developers

Latest AI news, tools and tutorials for AI Developers Real-time Updates ️

Real-time Updates ️

Archive

Archive AI Blogs AI Agent Concepts AI Tutorial Daily Unwind Weekly Unwind Daily Unwind Daily Unwind

Opensoure Alternative to Claude Code

Opensoure Alternative to Claude Code PLUS: Google's MCP toolbox for AI agents with databases, Vibe code full-stack apps with one sentence

PLUS: Google's MCP toolbox for AI agents with databases, Vibe code full-stack apps with one sentence Shubham Saboo, +1 Daily Unwind Daily Unwind

OpenAI Releases New Deep Research API

OpenAI Releases New Deep Research API PLUS: China's ByteDance opensources AI SWE Agent, Scrape web content behind auth walls

PLUS: China's ByteDance opensources AI SWE Agent, Scrape web content behind auth walls Shubham Saboo, +1 Daily Unwind Daily Unwind

Turn Any LLM Into an AI Voice Agent

Turn Any LLM Into an AI Voice Agent PLUS: OpenAI Codex, Claude Code and Gemini CLI in any app, Opensource AI browser extension

PLUS: OpenAI Codex, Claude Code and Gemini CLI in any app, Opensource AI browser extension Shubham Saboo, +1 Daily Unwind Daily Unwind

Vibe Code with Cursor on your Phone

Vibe Code with Cursor on your Phone PLUS: China’s Baidu opensourced ERNIE 4.5 models, GitHub Copilot Chat opensourced

PLUS: China’s Baidu opensourced ERNIE 4.5 models, GitHub Copilot Chat opensourced Shubham Saboo, +1 Daily Unwind Daily Unwind

Build and Run Custom AI Coding Agents in Terminal

Build and Run Custom AI Coding Agents in Terminal PLUS: Connect local LLMs to MCP servers, Vision-first AI browser agent

PLUS: Connect local LLMs to MCP servers, Vision-first AI browser agent Shubham Saboo, +1 AI Blogs AI Blogs

Global AI Agent Hackathon Recap

Global AI Agent Hackathon Recap 6 Opensource AI Agent apps so good, We had to add them to Awesome LLM Apps GitHub Repo 🌟

6 Opensource AI Agent apps so good, We had to add them to Awesome LLM Apps GitHub Repo 🌟 Shubham Saboo, +1 Daily Unwind Daily Unwind

OpenAI Hack to Cut Cost by 33%

OpenAI Hack to Cut Cost by 33% PLUS: Google's opensource on-device multimodal model, Rent an autonomous Linux computer

PLUS: Google's opensource on-device multimodal model, Rent an autonomous Linux computer Shubham Saboo, +1 Daily Unwind Daily Unwind

Opensource Gemini CLI Agent with 1M Context

Opensource Gemini CLI Agent with 1M Context PLUS: Opensource Perplexity clone, Run AI-generated code on-demand with Code Sandboxes

PLUS: Opensource Perplexity clone, Run AI-generated code on-demand with Code Sandboxes Shubham Saboo, +1 Daily Unwind Daily Unwind

Build Once and Deploy with Any Agent Framework

Build Once and Deploy with Any Agent Framework PLUS: Multiple parallel agents in your Terminal, ChatGPT Connectors for Pro users

PLUS: Multiple parallel agents in your Terminal, ChatGPT Connectors for Pro users Shubham Saboo, +1 Daily Unwind Daily Unwind

China's Deep Researcher Outperforms OpenAI Deep Research

China's Deep Researcher Outperforms OpenAI Deep Research PLUS: Specialized scalable RAG agents, Eleven Labs voice AI agent that takes actions

PLUS: Specialized scalable RAG agents, Eleven Labs voice AI agent that takes actions Shubham Saboo, +1 Daily Unwind Daily Unwind

Claude Code Now Available in VS Code

Claude Code Now Available in VS Code PLUS: $120M AI “cheating” startup exposed, Opensource agentic browser running locally

PLUS: $120M AI “cheating” startup exposed, Opensource agentic browser running locally Shubham Saboo, +1 Daily Unwind Daily Unwind

Microsoft’s Deep Research Agent for Large Codebases

Microsoft’s Deep Research Agent for Large Codebases PLUS: React application 🔗 MCP server in 3 lines of code, General-purpose AI agent for lomg-running tasks

PLUS: React application 🔗 MCP server in 3 lines of code, General-purpose AI agent for lomg-running tasks Shubham Saboo, +1","**Website Description:**

Unwind AI is a comprehensive online platform dedicated to providing the latest news, tools, and tutorials for AI developers. The website serves as a centralized hub for real-time updates on advancements in artificial intelligence, including open-source alternatives, coding agents, and innovative applications. With sections such as 'Latest AI News', 'Archive', and various tutorials, Unwind AI caters to the needs of developers seeking to enhance their skills and stay informed about the rapidly evolving AI landscape.

Targeting AI developers and enthusiasts, Unwind AI offers a wealth of resources including detailed articles, coding tutorials, and insights into new tools and technologies. Key features include a regularly updated archive of AI blogs, practical guides for building custom AI applications, and summaries of significant industry events like hackathons. This makes it an invaluable resource for anyone looking to deepen their understanding of AI and its applications.",1.0
https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-70B,FreedomIntelligence/HuatuoGPT-o1-70B · Hugging Face,Artificial Intelligence,508,2025-07-08T16:54:45.028253,huggingface.co,Morgann Thain,U05H9P2TGG0,2025-01-03T18:24:18.350639,,website,"A product page for the HuatuoGPT-o1-70B medical language model, offering advanced reasoning capabilities and deployment information.",,"HTML_TAG_START

HuatuoGPT-o1-70B

HuatuoGPT-o1-70B GitHub

Introduction

Introduction HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response. HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response. For more information, visit our GitHub repository:
https://github.com/FreedomIntelligence/HuatuoGPT-o1. For more information, visit our GitHub repository: https://github.com/FreedomIntelligence/HuatuoGPT-o1

Model Info

Model Info Backbone Supported Languages HuatuoGPT-o1-8B LLaMA-3.1-8B English HF Link HuatuoGPT-o1-70B LLaMA-3.1-70B English HF Link HuatuoGPT-o1-7B Qwen2.5-7B English & Chinese HF Link HuatuoGPT-o1-72B Qwen2.5-72B English & Chinese HF Link

Usage

You can use HuatuoGPT-o1 in the same way as Llama-3.1-70B-Instruct. You can deploy it with tools like vllm or Sglang, or perform direct inference:

You can use HuatuoGPT-o1 in the same way as
Llama-3.1-70B-Instruct

Llama-3.1-70B-Instruct. You can deploy it with tools like Sglang, or perform direct inference:
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-70B"",torch_dtype=""auto"",device_map=""auto"")
tokenizer = AutoTokenizer.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-70B"")

input_text = ""How to stop a cough?""
messages = [{""role"": ""user"", ""content"": input_text}]

inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True
), return_tensors=""pt"").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=2048)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-70B"",torch_dtype=""auto"",device_map=""auto"")
tokenizer = AutoTokenizer.from_pretrained(""FreedomIntelligence/HuatuoGPT-o1-70B"")

input_text = ""How to stop a cough?""
messages = [{""role"": ""user"", ""content"": input_text}]

inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True
), return_tensors=""pt"").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=2048)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained( ""FreedomIntelligence/HuatuoGPT-o1-70B"",torch_dtype= ""auto"",device_map= ""auto"" )
tokenizer = AutoTokenizer.from_pretrained( ""FreedomIntelligence/HuatuoGPT-o1-70B"" )

input_text = ""How to stop a cough?"" messages = [{ ""role"" ""user"" ""content"": input_text}]

inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=,add_generation_prompt= ), return_tensors= ).to(model.device)
outputs = model.generate(**inputs, max_new_tokens= (tokenizer.decode(outputs[ ], skip_special_tokens= HuatuoGPT-o1 adopts a thinks-before-it-answers approach, with outputs formatted as:

HuatuoGPT-o1 adopts a thinks-before-it-answers approach, with outputs formatted as:
## Thinking
[Reasoning process]

## Final Response
[Output]

## Thinking
[Reasoning process]

## Final Response
[Output]

## Thinking
[Reasoning process]

## Final Response
[Output]

📖 Citation

📖 Citation
@misc{chen2024huatuogpto1medicalcomplexreasoning,
title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs},
author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
year={2024},
eprint={2412.18925},
archivePrefix={arXiv},
primaryClass={cs. CL},
url={https://arxiv.org/abs/2412.18925},
}

@misc{chen2024huatuogpto1medicalcomplexreasoning,
title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs},
author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
year={2024},
eprint={2412.18925},
archivePrefix={arXiv},
primaryClass={cs. CL},
url={https://arxiv.org/abs/2412.18925},
}

@misc{chen2024huatuogpto1medicalcomplexreasoning,
title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs},
author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
year={2024},
eprint={2412.18925},
archivePrefix={arXiv},
primaryClass={cs. CL},
url={https://arxiv.org/abs/2412.18925},
} HTML_TAG_END Downloads last month HTML_TAG_START HTML_TAG_END Safetensors Model size 70.6B params Tensor type Files info Inference Providers Text Generation This model isn't deployed by any Inference Provider. Ask for provider support

Model tree for FreedomIntelligence/HuatuoGPT-o1-70B

Model tree for FreedomIntelligence/HuatuoGPT-o1-70B Base model meta-llama/Llama-3.1-8B Finetuned meta-llama/Llama-3.1-8B-Instruct Finetuned this model Quantizations 4 models

Datasets used to train
FreedomIntelligence/HuatuoGPT-o1-70B

Datasets used to train FreedomIntelligence/HuatuoGPT-o1-70B

Spaces using
FreedomIntelligence/HuatuoGPT-o1-70B
8

Spaces using FreedomIntelligence/HuatuoGPT-o1-70B

Collection including
FreedomIntelligence/HuatuoGPT-o1-70B

Collection including FreedomIntelligence/HuatuoGPT-o1-70B 4 items Updated Dec 30, 2024","**Website Description:**

The FreedomIntelligence/HuatuoGPT-o1-70B page on Hugging Face presents a sophisticated medical language model designed for advanced medical reasoning. This model, HuatuoGPT-o1, is capable of generating complex thought processes and refining its reasoning before delivering responses, making it a valuable tool for healthcare professionals and researchers. The website provides comprehensive information about the model, including its capabilities, usage instructions, and links to related resources such as its GitHub repository.

Targeting developers, researchers, and medical professionals, the site offers detailed model specifications, deployment options, and practical examples of how to implement the model in various applications. Key features include support for multiple languages, integration with popular tools like vllm and Sglang, and a user-friendly interface for direct inference, ensuring accessibility for users at different technical levels.",1.0
https://www.sigarch.org/the-quarch-experiment-crowdsourcing-a-machine-learning-dataset-for-computer-architecture/,The QuArch Experiment: Crowdsourcing a Machine Learning Dataset for Computer Architecture,Computer Architecture,4404,2025-07-08T16:55:11.211856,www.sigarch.org,Sasha Newman-Oktan,U07DGA6HABV,2025-01-07T17:17:17.336879,,article,,,"The QuArch Experiment: Crowdsourcing a Machine Learning Dataset for Computer Architecture

The QuArch Experiment: Crowdsourcing a Machine Learning Dataset for Computer Architecture by Shvetank Prakash and Vijay Janapa Reddi on Jan 7, 2025 | Tags: AI Agents, Benchmarks, Datasets, Machine Learning

Shvetank Prakash and Vijay Janapa Reddi on Jan 7, 2025 | Tags: AI Agents Benchmarks Datasets Machine Learning.et_post_meta_wrapper

Introduction

Introduction The rise of large language models (LLMs) and generative artificial intelligence (GenAI) presents new opportunities to build innovative tools and is already enabling revolutionary AI-based tools in various domains. However, a significant gap remains in the language model (LM) knowledge required to deliver high-quality, AI-based tools for engineers. Figure 1 illustrates the performance of LMs across various subjects in the MMLU-Pro question-answering benchmark, with engineering consistently ranking among the lowest-performing topics. The rise of large language models (LLMs) and generative artificial intelligence (GenAI) presents new opportunities to build innovative tools and is already enabling revolutionary AI-based tools in various domains. However, a significant gap remains in the language model (LM) knowledge required to deliver high-quality, AI-based tools for engineers. Figure 1 illustrates the performance of LMs across various subjects in the MMLU-Pro question-answering benchmark, with engineering consistently ranking among the lowest-performing topics. Figure 1: Average performance of language models across various subjects in the MMLU-Pro benchmark. Figure 1: Average performance of language models across various subjects in the MMLU-Pro benchmark. Benchmarks play an important role in driving progress within our field (for better or for worse), influencing both advancements and potential setbacks. Similarly, high-quality datasets are essential for developing and enhancing machine learning (ML) methods. Yet computer architecture is notably underserved in this regard. Benchmarks play an important role in driving progress within our field ( for better or for worse ), influencing both advancements and potential setbacks. Similarly, high-quality datasets are essential for developing and enhancing machine learning (ML) methods. Yet computer architecture is notably underserved in this regard. We lack domain-specific datasets that are necessary to benchmark progress and foster AI-driven innovation. This striking gap calls for attention given architectureʼs historical relationship with AI. We lack domain-specific datasets that are necessary to benchmark progress and foster AI-driven innovation. This striking gap calls for attention given architectureʼs historical relationship with AI. This blog post explores what today’s language models understand about computer architecture. Given their relatively modest performance, we introduce QuArch v1.0 as the first step in a broader initiative to create an open-source, community-driven dataset to help usher in an AI-driven computer system design era. At the end of this blog, you’ll find a call to participate, encouraging you to contribute your expertise and shape the future of this initiative. This blog post explores what today’s language models understand about computer architecture. Given their relatively modest performance, we introduce QuArch v1.0 as the first step in a broader initiative to create an open-source, community-driven dataset to help usher in an AI-driven computer system design era. At the end of this blog, you’ll find a call to participate, encouraging you to contribute your expertise and shape the future of this initiative. The State of AI in Computer Architecture

The State of AI in Computer Architecture AI has long been a powerful tool in computer architecture, particularly for optimizing various hardware and software components such as branch predictors, memory controllers, resource allocation, coherence, cache allocation, replacement policies, scheduling, accelerator design, cloud resource sharing, power consumption, and security. ML has enhanced virtually every aspect of computing systems. AI has long been a powerful tool in computer architecture, particularly for optimizing various hardware and software components such as branch predictors memory controllers resource allocation coherence cache allocation replacement policies scheduling accelerator design, cloud resource sharing power consumption security. ML has enhanced virtually every aspect of computing systems. The most recent breakthroughs in LLMs and GenAI, built upon decades of architectural innovation, now stand to revolutionize this relationship further. Consequently, we are starting to see new initiatives like the NSF GenAI4HW & HW4GenAI Workshop at MICRO 2024, the NSF ImageNets4EDA Workshop, and the Architecture 2.0 workshop reflect the growing interest in leveraging these technologies to create a virtuous cycle of innovation. The most recent breakthroughs in LLMs and GenAI, built upon decades of architectural innovation, now stand to revolutionize this relationship further. Consequently, we are starting to see new initiatives like the NSF GenAI4HW & HW4GenAI Workshop at MICRO 2024 NSF ImageNets4EDA Workshop, and the Architecture 2.0 workshop reflect the growing interest in leveraging these technologies to create a virtuous cycle of innovation. This momentum is prominent in electronic design automation (EDA), where researchers develop specialized datasets and algorithms for chip design tasks (Liu et al., Chin et al.). Recent work on Register-Transfer Level (RTL) generation (Liu et al., Blocklove et al., Zhang et al., Tsai et al.) and hardware verification & security (Cosler et al., Ahmad et al., Afsharmazayejani et al.) demonstrate how domain-specific data can enable ML to tackle complex hardware design challenges. This momentum is prominent in electronic design automation (EDA), where researchers develop specialized datasets and algorithms for chip design tasks ( Liu et al Chin et al. ). Recent work on Register-Transfer Level (RTL) generation ( Liu et al. Blocklove et al. Zhang et al. Tsai et al. ) and hardware verification & security ( Cosler et al. Ahmad et al. Afsharmazayejani et al. ) demonstrate how domain-specific data can enable ML to tackle complex hardware design challenges. While EDA focuses on implementation-level tasks, we lack similar resources for system-level reasoning, which is essential for many tasks in computer architecture. EDA machine learning datasets enable automation of RTL generation and verification. We need something that focuses on broader architectural reasoning to inform fundamental design decisions. This creates a complementary pipeline: architects need enhanced AI tools to explore and assess high-level designs that flow into EDA tools for implementation. Ultimately, we need high-quality, large-scale data that captures architectural expertise to enable this vision. While EDA focuses on implementation-level tasks, we lack similar resources for system-level reasoning, which is essential for many tasks in computer architecture. EDA machine learning datasets enable automation of RTL generation and verification. We need something that focuses on broader architectural reasoning to inform fundamental design decisions. This creates a complementary pipeline: architects need enhanced AI tools to explore and assess high-level designs that flow into EDA tools for implementation. Ultimately, we need high-quality, large-scale data that captures architectural expertise to enable this vision. Introducing QuArch

Introducing QuArch To address the dataset gap, we discuss QuArch, a dataset aimed at higher-level architectural decisions—the kind of reasoning that precedes and informs detailed implementation. QuArch (pronounced ‘quark’) is a specialized question-answering (QA) dataset designed to evaluate and enhance the computer architecture knowledge embedded in LMs. Its goal is to assess (and help bridge) the knowledge gap and drive AI-driven innovation in architecture. The QAs follow a multiple-choice format similar to other ML datasets to ensure educational value and feasible assessment. Below, we provide a simple example of QA:

To address the dataset gap, we discuss QuArch, a dataset aimed at higher-level architectural decisions—the kind of reasoning that precedes and informs detailed implementation. QuArch (pronounced ‘quark’) is a specialized question-answering (QA) dataset designed to evaluate and enhance the computer architecture knowledge embedded in LMs. Its goal is to assess (and help bridge) the knowledge gap and drive AI-driven innovation in architecture. The QAs follow a multiple-choice format similar to other ML datasets to ensure educational value and feasible assessment. Below, we provide a simple example of QA: Question 1 Q: Which of the following is a key feature of the Reduced Instruction Set Computer (RISC) architecture? • Complex instructions that can perform multiple operations in a single cycle
Complex instructions that can perform multiple operations in a single cycle • A large number of general-purpose registers to reduce memory access
A large number of general-purpose registers to reduce memory access • A small and fixed instruction format to simplify hardware design
A small and fixed instruction format to simplify hardware design • A memory architecture that prioritizes cache size over latency
A memory architecture that prioritizes cache size over latency When evaluating LLM responses to the above question, subtle yet meaningful differences emerge. Models like GPT-4 and Claude provided diverging answers: some chose option B, others selected option C, and some attempted to rationalize between the two. Others gave outright incorrect answers, highlighting diversity in interpretive approaches. These differences arise from variations in how models interpret the question’s focus, prioritize architectural features, and articulate reasoning. This example shows that even advanced LLMs can struggle with nuanced, domain-specific questions, which emphasizes the need for datasets like QuArch to enhance their understanding of the complex reasoning required in computer architecture. When evaluating LLM responses to the above question, subtle yet meaningful differences emerge. Models like GPT-4 and Claude provided diverging answers: some chose option B, others selected option C, and some attempted to rationalize between the two. Others gave outright incorrect answers, highlighting diversity in interpretive approaches. These differences arise from variations in how models interpret the question’s focus, prioritize architectural features, and articulate reasoning. This example shows that even advanced LLMs can struggle with nuanced, domain-specific questions, which emphasizes the need for datasets like QuArch to enhance their understanding of the complex reasoning required in computer architecture. The 1,500 QAs in QuArch v0.1 have been expertly labeled by industry experts and Ph. D. students in computer architecture. They cover a wide range of areas, as shown in Figure 2. The topics included represent the diverse scope of modern computer architecture. Processor architecture QAs comprise 32% of the dataset, with memory systems at 22% and interconnection networks at 10%. The 1,500 QAs in QuArch v0.1 have been expertly labeled by industry experts and Ph. D. students in computer architecture. They cover a wide range of areas, as shown in Figure 2. The topics included represent the diverse scope of modern computer architecture. Processor architecture QAs comprise 32% of the dataset, with memory systems at 22% and interconnection networks at 10%. Figure 2: Distribution of computer architecture topics in QuArch v0.1. Figure 2: Distribution of computer architecture topics in QuArch v0.1. Why Question-Answering for Architecture? Why Question-Answering for Architecture? Computer architecture encompasses many complex challenges spanning the entire technology stack. Addressing these cross-stack challenges requires synthesizing high-level insights with low-level implementations, demanding a deep understanding of workload behavior, architectural constraints, and system-level interactions. Computer architecture encompasses many complex challenges spanning the entire technology stack. Addressing these cross-stack challenges requires synthesizing high-level insights with low-level implementations, demanding a deep understanding of workload behavior, architectural constraints, and system-level interactions. Question-answering systems are particularly well-suited to support these multifaceted needs. QA tasks inherently require knowledge retrieval, reasoning, complex situational modeling, and input interpretation & manipulation— capabilities essential for solving architectural problems. By training LMs on QA datasets like QuArch, we can enhance their ability to assist in tasks ranging from optimizing cache hierarchies to designing efficient interconnects. Question-answering systems are particularly well-suited to support these multifaceted needs. QA tasks inherently require knowledge retrieval, reasoning, complex situational modeling, and input interpretation & manipulation — capabilities essential for solving architectural problems. By training LMs on QA datasets like QuArch, we can enhance their ability to assist in tasks ranging from optimizing cache hierarchies to designing efficient interconnects. Historically, QA datasets have been instrumental in advancing domain-specific, LLM-based tools across fields like medicine, math, law, finance, and software engineering. These tools enable applications such as diagnostic assistance, equation solving, legal research, financial modeling, and code debugging. Similar opportunities exist to build exciting agentic workflows for computer architecture. Still, the availability of data—or the lack thereof—remains a critical factor in revolutionizing traditional tools and unlocking new paradigms. Historically, QA datasets have been instrumental in advancing domain-specific, LLM-based tools across fields like medicine, finance, and software engineering. These tools enable applications such as diagnostic assistance, equation solving, legal research, financial modeling, and code debugging. Similar opportunities exist to build exciting agentic workflows for computer architecture. Still, the availability of data—or the lack thereof—remains a critical factor in revolutionizing traditional tools and unlocking new paradigms. Evaluating Architectural Knowledge of Language Models

Evaluating Architectural Knowledge of Language Models Using QuArch, we systematically evaluated language models’ computer architecture knowledge across multiple topics. Our evaluation revealed both promising results and concerning gaps. Using QuArch, we systematically evaluated language models’ computer architecture knowledge across multiple topics. Our evaluation revealed both promising results and concerning gaps. The top-performing model, shown in Figure 3, achieved an 84% accuracy rate on basic architectural concepts, demonstrating imperfect but strong foundational knowledge. However, this performance consistently dropped across some topics. Three topics, in particular, caused the biggest drops in accuracy across models: Interconnection Networks, Memory Systems, and Benchmarking & Measurement. See details here. The top-performing model, shown in Figure 3, achieved an 84% accuracy rate on basic architectural concepts, demonstrating imperfect but strong foundational knowledge. However, this performance consistently dropped across some topics. Three topics, in particular, caused the biggest drops in accuracy across models: Interconnection Networks, Memory Systems, and Benchmarking & Measurement. See details Figure 3. Comparison of model accuracies on QuArch v0.1 QAs. See details here. Figure 3. Comparison of model accuracies on QuArch v0.1 QAs. See details More notably, performance dropped significantly by 12% when using smaller, open-source models (under 10B parameters). Furthermore, as discussed in the next section, we empirically observed that this gap becomes particularly pronounced when transitioning from factual questions to complex reasoning tasks. More notably, performance dropped significantly by 12% when using smaller, open-source models (under 10B parameters). Furthermore, as discussed in the next section, we empirically observed that this gap becomes particularly pronounced when transitioning from factual questions to complex reasoning tasks. These results show that while current models show promise in understanding basic architectural concepts, they still have weaknesses that will limit their ability to handle the complex reasoning and deep technical understanding that computer architects apply daily. Moreover, the disparity between small and large model performance suggests that effective, AI-based architecture tools will likely demand substantial resources or require greater specialization. These results show that while current models show promise in understanding basic architectural concepts, they still have weaknesses that will limit their ability to handle the complex reasoning and deep technical understanding that computer architects apply daily. Moreover, the disparity between small and large model performance suggests that effective, AI-based architecture tools will likely demand substantial resources or require greater specialization. Beyond Factual QA: Towards Complex Architectural Reasoning

Beyond Factual QA: Towards Complex Architectural Reasoning Beyond QuArch’s 1,500 QAs, which primarily evaluate the domain knowledge embedded in LMs, we also conducted a case study analyzing a publicly available Ph. D. qualifying exam question that demands advanced reasoning skills. Specifically, we examined optimizations like vectorization. Our findings revealed that even state-of-the-art models, such as GPT-4o, needed help interpreting loop dependencies and conditions essential for proper compiler vectorization. For instance, models misinterpreted simple dependency differences between i+1 and i-1, highlighting significant gaps in their reasoning capabilities. Beyond QuArch’s 1,500 QAs, which primarily evaluate the domain knowledge embedded in LMs, we also conducted a case study analyzing a publicly available Ph. D. qualifying exam question that demands advanced reasoning skills. Specifically, we examined optimizations like vectorization. Our findings revealed that even state-of-the-art models, such as GPT-4o, needed help interpreting loop dependencies and conditions essential for proper compiler vectorization. For instance, models misinterpreted simple dependency differences between i+1 and i-1, highlighting significant gaps in their reasoning capabilities. The above case study demonstrates the need for more sophisticated datasets and targeted training to enhance model performance in complex architectural tasks. To assess and improve reasoning capabilities, we must move beyond factual QAs to develop complex, multi-step problems. These real-world problems must evaluate architectural reasoning across dimensions like performance analysis, design exploration, and hardware-software co-design. Building this comprehensive evaluation framework requires diverse expertise and architectural challenges that only our community can provide, which brings us to the next stage. The above case study demonstrates the need for more sophisticated datasets and targeted training to enhance model performance in complex architectural tasks. To assess and improve reasoning capabilities, we must move beyond factual QAs to develop complex, multi-step problems. These real-world problems must evaluate architectural reasoning across dimensions like performance analysis, design exploration, and hardware-software co-design. Building this comprehensive evaluation framework requires diverse expertise and architectural challenges that only our community can provide, which brings us to the next stage. Join the QuArch Community: Contribute to QuArch v1.0

Join the QuArch Community: Contribute to QuArch v1.0 Our initial QuArch development efforts engaged 50 undergraduate students primarily from electrical engineering and computer science backgrounds, offering valuable insights but revealing significant QA validation challenges due to varying levels of expertise. This experience highlighted the need for a broader, community-driven approach to develop a robust, high-quality dataset— a strategy that has proven successful across numerous domains. Crowdsourced datasets have consistently driven ML innovation, from ImageNet’s transformative impact on computer vision to Mozilla Common Voice’s advancements in speech recognition. Our initial QuArch development efforts engaged 50 undergraduate students primarily from electrical engineering and computer science backgrounds, offering valuable insights but revealing significant QA validation challenges due to varying levels of expertise. This experience highlighted the need for a broader, community-driven approach to develop a robust, high-quality dataset— a strategy that has proven successful across numerous domains. Crowdsourced datasets have consistently driven ML innovation, from ImageNet’s transformative impact on computer vision Mozilla Common Voice’s advancements in speech recognition

How You Can Help

How You Can Help We invite computer architects, educators, industry professionals, and enthusiasts to contribute through our structured three-stage roadmap:

We invite computer architects, educators, industry professionals, and enthusiasts to contribute through our structured three-stage roadmap: • Stage 1: Expanding Fundamental Domain Knowledge: Annotate QAs on concepts like processor execution, memory hierarchy, and parallelism, building the critical foundation for advanced reasoning and design. Stage 1: Expanding Fundamental Domain Knowledge: Annotate QAs on concepts like processor execution, memory hierarchy, and parallelism, building the critical foundation for advanced reasoning and design. • Stage 2: Input Interpretation & Manipulation: Curate QAs based on tasks like analyzing instruction traces and optimizing code for engineering applications. We aim to create diverse practical questions from academia, industry, and public platforms by crowdsourcing questions. Stage 2: Input Interpretation & Manipulation: Curate QAs based on tasks like analyzing instruction traces and optimizing code for engineering applications. We aim to create diverse practical questions from academia, industry, and public platforms by crowdsourcing questions. • Stage 3: Reasoning & Complex Situational Modeling: Craft reasoning-driven QAs that mirror real-world architectural challenges and require retrieval, reasoning, planning, and modeling skills. This stage enables models to reason about trade-offs and system-level design scenarios to push AI’s capabilities. Stage 3: Reasoning & Complex Situational Modeling: Craft reasoning-driven QAs that mirror real-world architectural challenges and require retrieval, reasoning, planning, and modeling skills. This stage enables models to reason about trade-offs and system-level design scenarios to push AI’s capabilities. All contributors will receive appropriate credit and be featured on our community leaderboard. All contributors will receive appropriate credit and be featured on our community leaderboard

Why Get Involved

Why Get Involved The computer architecture community has a strong legacy of collaboration with many of our most impactful tools built and sustained through community efforts, exemplified by projects like the gem5 simulator, RISC-V ISA, McPAT, and SimpleScalar. These projects demonstrate how collective contributions can create resources that advance research and industry practice. The computer architecture community has a strong legacy of collaboration with many of our most impactful tools built and sustained through community efforts, exemplified by projects like the gem5 simulator RISC-V ISA SimpleScalar. These projects demonstrate how collective contributions can create resources that advance research and industry practice. As we enter the era of AI-assisted architecture, QuArch offers a unique opportunity to shape the future of computer architecture through community efforts. All contributors will receive proper attribution, including co-authorship on QuArch publication(s), acknowledgment in documentation, and recognition on our community leaderboard. As we enter the era of AI-assisted architecture, QuArch offers a unique opportunity to shape the future of computer architecture through community efforts. All contributors will receive proper attribution, including co-authorship on QuArch publication(s), acknowledgment in documentation, and recognition on our community leaderboard

Getting Involved

Getting Involved If you’re ready to help shape QuArch, sign up here to join our project using Label Studio for dataset annotation. Once your account is approved, it will take a brief minute for access to the QuArch v1.0 project and labeling interface (Figure 4) to appear. Please review our FAQs for labeling guidelines and best practices. If you’re ready to help shape QuArch, sign up join our project using Label Studio for dataset annotation. Once your account is approved, it will take a brief minute for access to the QuArch v1.0 project and labeling interface (Figure 4) to appear. Please review our for labeling guidelines and best practices. Figure 4: Interface for QuArch dataset labeling. Figure 4: Interface for QuArch dataset labeling. You can also submit your own QAs through this form to contribute additional questions or other input. You can also submit your own QAs through this form to contribute additional questions or other input. If you are also interested in joining the broader Architecture 2.0 community, please fill out this form to be contacted about updates and meet-ups for QuArch and other related activities. If you are also interested in joining the broader Architecture 2.0 community, please fill out this form to be contacted about updates and meet-ups for QuArch and other related activities. Conclusion

Conclusion QuArch is an effort to push the boundaries of computer architecture through crowdsourcing. The goal is to leverage diverse questions from academia and industry while avoiding traditional IP constraints. By contributing to this dataset, we are shaping AI-driven tools and technologies that will impact the future. We hope you join the effort! QuArch is an effort to push the boundaries of computer architecture through crowdsourcing. The goal is to leverage diverse questions from academia and industry while avoiding traditional IP constraints. By contributing to this dataset, we are shaping AI-driven tools and technologies that will impact the future. We hope you join the effort! Acknowledgments

Acknowledgments The development of QuArch was an extraordinary collective effort made possible by the dedication and contributions of a diverse and talented group of individuals across academia and industry. We want to especially acknowledge Amir Yazdanbakhsh (Google DeepMind), who, together with us, developed the vision for Architecture 2.0 that gave birth to QuArch. We are deeply grateful to our collaborators: Fin Amin (NCSU), Arnav Balyan, Yash Choudhary (IIT Bombay), Andy Cheng (Harvard), Sofia Giannuzzi (Harvard), Shreyas Grampurohit (IIT Bombay), Radhika Ghosal (Harvard), Jeffrey Ma (Harvard), Ankita Nayak (Qualcomm AI Research), Aadya Pipersenia (IIT Bombay), Jessica Quaye (Harvard), Arya Tschand (Harvard), Ike Uchendu (Harvard), and Jason Yik (Harvard) for their significant contributions to QuArch v0.1 and the arXiv report, which laid the foundation for many of the ideas presented in this blog. This year-long endeavor was further enriched by the efforts of over 40 students from around the world, whose hard work and creativity were important in shaping the early development of QuArch. The development of QuArch was an extraordinary collective effort made possible by the dedication and contributions of a diverse and talented group of individuals across academia and industry. We want to especially acknowledge Amir Yazdanbakhsh (Google DeepMind), who, together with us, developed the vision for Architecture 2.0 that gave birth to QuArch. We are deeply grateful to our collaborators: Fin Amin (NCSU), Arnav Balyan, Yash Choudhary (IIT Bombay), Andy Cheng (Harvard), Sofia Giannuzzi (Harvard), Shreyas Grampurohit (IIT Bombay), Radhika Ghosal (Harvard), Jeffrey Ma (Harvard), Ankita Nayak (Qualcomm AI Research), Aadya Pipersenia (IIT Bombay), Jessica Quaye (Harvard), Arya Tschand (Harvard), Ike Uchendu (Harvard), and Jason Yik (Harvard) for their significant contributions to QuArch v0.1 and the arXiv report, which laid the foundation for many of the ideas presented in this blog. This year-long endeavor was further enriched by the efforts of over 40 students from around the world, whose hard work and creativity were important in shaping the early development of QuArch. About the Authors

About the Authors Shvetank Prakash is a fourth-year Ph. D. candidate at Harvard University focused on system design automation using machine learning techniques and open-source and emerging technologies for ultra-low power ML hardware architectures. Shvetank Prakash is a fourth-year Ph. D. candidate at Harvard University focused on system design automation using machine learning techniques and open-source and emerging technologies for ultra-low power ML hardware architectures. Vijay Janapa Reddi is an Associate Professor at Harvard University, where his research focuses on computer architecture and machine learning systems for autonomous agents. He serves as Vice President and co-founder of MLCommons, driving community-driven initiatives in machine learning innovation. He is passionate about expanding access to applied machine learning through open-source education, as evidenced by his open-source book “Machine Learning Systems” (mlsysbook.ai) and the widely-adopted TinyML course series on edX. Vijay Janapa Reddi is an Associate Professor at Harvard University, where his research focuses on computer architecture and machine learning systems for autonomous agents. He serves as Vice President and co-founder of MLCommons, driving community-driven initiatives in machine learning innovation. He is passionate about expanding access to applied machine learning through open-source education, as evidenced by his open-source book “Machine Learning Systems” ( mlsysbook.ai ) and the widely-adopted TinyML course series on edX. Disclaimer: These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM. Disclaimer: These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM. Share this:

Share this: • Click to share on Facebook (Opens in new window)
Facebook
Click to share on Facebook (Opens in new window) Facebook • Click to share on X (Opens in new window)
X
Click to share on X (Opens in new window) • Click to share on LinkedIn (Opens in new window)
LinkedIn
Click to share on LinkedIn (Opens in new window) LinkedIn.entry-content.et_post_meta_wrapper","**The QuArch Experiment: Crowdsourcing a Machine Learning Dataset for Computer Architecture**



The QuArch Experiment: Crowdsourcing a Machine Learning Dataset for Computer Architecture

The QuArch Experiment: Crowdsourcing a Machine Learning Dataset for Computer Architecture by Shvetank Prakash and Vijay Janapa Reddi on Jan 7, 2025 | Tags: AI Agents, Benchmarks, Datasets, Machine Learning

Shvetank Prakash and Vijay Janapa Reddi on Jan 7, 2025 | Tags: AI Agents Benchmarks Datasets Machine Learning.et_post_meta_wrapper

Introduction

Key points:

• Introduction The rise of large language models (LLMs) and generative artificial intelligence (GenAI) presents new opportunities to build innovative tools and is already enabling revolutionary AI-based tools in various domains.

• However, a significant gap remains in the language model (LM) knowledge required to deliver high-quality, AI-based tools for engineers.

• Figure 1 illustrates the performance of LMs across various subjects in the MMLU-Pro question-answering benchmark, with engineering consistently ranking among the lowest-performing topics.

• The rise of large language models (LLMs) and generative artificial intelligence (GenAI) presents new opportunities to build innovative tools and is already enabling revolutionary AI-based tools in various domains.

• However, a significant gap remains in the language model (LM) knowledge required to deliver high-quality, AI-based tools for engineers.

• Figure 1 illustrates the performance of LMs across various subjects in the MMLU-Pro question-answering benchmark, with engineering consistently ranking among the lowest-performing topics.

Figure 1: Average performance of language models across various subjects in the MMLU-Pro benchmark. Figure 1: Average performance of language models across various subjects in the MMLU-Pro benchmark. Benchmarks play an important role in driving progress within our field (for better or for worse), influencing both advancements and potential setbacks. Similarly, high-quality datasets are essential for developing and enhancing machine learning (ML) methods. Yet computer architecture is notably underserved in this regard. Benchmarks play an important role in driving progress within our field ( for better or for worse ), influencing both advancements and potential setbacks. Similarly, high-quality datasets are essential for developing and enhancing machine learning (ML) methods. Yet computer architecture is notably underserved in this regard. We lack domain-specific datasets that are necessary to benchmark progress and foster AI-driven innovation. This striking gap calls for attention given architectureʼs historical relationship with AI. We lack domain-specific datasets that are necessary to benchmark progress and foster AI-driven innovation. This striking gap calls for attention given architectureʼs historical relationship with AI. This blog post explores what today’s language models understand about computer architecture. Given their relatively modest performance, we introduce QuArch v1.0 as the first step in a broader initiative to create an open-source, community-driven dataset to help usher in an AI-driven computer system design era. At the end of this blog, you’ll find a call to participate, encouraging you to contribute your expertise and shape the future of this initiative. This blog post explores what today’s language models understand about computer architecture. Given their relatively modest performance, we introduce QuArch v1.0 as the first step in a broader initiative to create an open-source, community-driven dataset to help usher in an AI-driven computer system design era. At the end of this blog, you’ll find a call to participate, encouraging you to contribute your expertise and shape the future of this initiative. The State of AI in Computer Architecture

Key points:

• The State of AI in Computer Architecture AI has long been a powerful tool in computer architecture, particularly for optimizing various hardware and software components such as branch predictors, memory controllers, resource allocation, coherence, cache allocation, replacement policies, scheduling, accelerator design, cloud resource sharing, power consumption, and security.

• ML has enhanced virtually every aspect of computing systems.

• AI has long been a powerful tool in computer architecture, particularly for optimizing various hardware and software components such as branch predictors memory controllers resource allocation coherence cache allocation replacement policies scheduling accelerator design, cloud resource sharing power consumption security.

• ML has enhanced virtually every aspect of computing systems.

• The most recent breakthroughs in LLMs and GenAI, built upon decades of architectural innovation, now stand to revolutionize this relationship further.

• Consequently, we are starting to see new initiatives like the NSF GenAI4HW & HW4GenAI Workshop at MICRO 2024, the NSF ImageNets4EDA Workshop, and the Architecture 2.0 workshop reflect the growing interest in leveraging these technologies to create a virtuous cycle of innovation.

The most recent breakthroughs in LLMs and GenAI, built upon decades of architectural innovation, now stand to revolutionize this relationship further. Consequently, we are starting to see new initiatives like the NSF GenAI4HW & HW4GenAI Workshop at MICRO 2024 NSF ImageNets4EDA Workshop, and the Architecture 2.0 workshop reflect the growing interest in leveraging these technologies to create a virtuous cycle of innovation. This momentum is prominent in electronic design automation (EDA), where researchers develop specialized datasets and algorithms for chip design tasks (Liu et al., Chin et al.). Recent work on Register-Transfer Level (RTL) generation (Liu et al., Blocklove et al., Zhang et al., Tsai et al.) and hardware verification & security (Cosler et al., Ahmad et al., Afsharmazayejani et al.) demonstrate how domain-specific data can enable ML to tackle complex hardware design challenges. This momentum is prominent in electronic design automation (EDA), where researchers develop specialized datasets and algorithms for chip design tasks ( Liu et al Chin et al. ). Recent work on Register-Transfer Level (RTL) generation ( Liu et al. Blocklove et al. Zhang et al. Tsai et al. ) and hardware verification & security ( Cosler et al. Ahmad et al. Afsharmazayejani et al. ) demonstrate how domain-specific data can enable ML to tackle complex hardware design challenges. While EDA focuses on implementation-level tasks, we lack similar resources for system-level reasoning, which is essential for many tasks in computer architecture. EDA machine learning datasets enable automation of RTL generation and verification. We need something that focuses on broader architectural reasoning to inform fundamental design decisions. This creates a complementary pipeline: architects need enhanced AI tools to explore and assess high-level designs that flow into EDA tools for implementation. Ultimately, we need high-quality, large-scale data that captures architectural expertise to enable this vision. While EDA focuses on implementation-level tasks, we lack similar resources for system-level reasoning, which is essential for many tasks in computer architecture. EDA machine learning datasets enable automation of RTL generation and verification. We need something that focuses on broader architectural reasoning to inform fundamental design decisions. This creates a complementary pipeline: architects need enhanced AI tools to explore and assess high-level designs that flow into EDA tools for implementation. Ultimately, we need high-quality, large-scale data that captures architectural expertise to enable this vision. Introducing QuArch

Key points:

• Introducing QuArch To address the dataset gap, we discuss QuArch, a dataset aimed at higher-level architectural decisions—the kind of reasoning that precedes and informs detailed implementation.

• QuArch (pronounced ‘quark’) is a specialized question-answering (QA) dataset designed to evaluate and enhance the computer architecture knowledge embedded in LMs.

• Its goal is to assess (and help bridge) the knowledge gap and drive AI-driven innovation in architecture.

• The QAs follow a multiple-choice format similar to other ML datasets to ensure educational value and feasible assessment.

• Below, we provide a simple example of QA:.

Key points:

• To address the dataset gap, we discuss QuArch, a dataset aimed at higher-level architectural decisions—the kind of reasoning that precedes and informs detailed implementation.

• QuArch (pronounced ‘quark’) is a specialized question-answering (QA) dataset designed to evaluate and enhance the computer architecture knowledge embedded in LMs.

• Its goal is to assess (and help bridge) the knowledge gap and drive AI-driven innovation in architecture.

• The QAs follow a multiple-choice format similar to other ML datasets to ensure educational value and feasible assessment.

• Below, we provide a simple example of QA: Question 1 Q: Which of the following is a key feature of the Reduced Instruction Set Computer (RISC) architecture? • Complex instructions that can perform multiple operations in a single cycle
Complex instructions that can perform multiple operations in a single cycle • A large number of general-purpose registers to reduce memory access
A large number of general-purpose registers to reduce memory access • A small and fixed instruction format to simplify hardware design
A small and fixed instruction format to simplify hardware design • A memory architecture that prioritizes cache size over latency
A memory architecture that prioritizes cache size over latency When evaluating LLM responses to the above question, subtle yet meaningful differences emerge.

• Models like GPT-4 and Claude provided diverging answers: some chose option B, others selected option C, and some attempted to rationalize between the two.

Others gave outright incorrect answers, highlighting diversity in interpretive approaches. These differences arise from variations in how models interpret the question’s focus, prioritize architectural features, and articulate reasoning. This example shows that even advanced LLMs can struggle with nuanced, domain-specific questions, which emphasizes the need for datasets like QuArch to enhance their understanding of the complex reasoning required in computer architecture. When evaluating LLM responses to the above question, subtle yet meaningful differences emerge. Models like GPT-4 and Claude provided diverging answers: some chose option B, others selected option C, and some attempted to rationalize between the two. Others gave outright incorrect answers, highlighting diversity in interpretive approaches. These differences arise from variations in how models interpret the question’s focus, prioritize architectural features, and articulate reasoning. This example shows that even advanced LLMs can struggle with nuanced, domain-specific questions, which emphasizes the need for datasets like QuArch to enhance their understanding of the complex reasoning required in computer architecture. The 1,500 QAs in QuArch v0.1 have been expertly labeled by industry experts and Ph. D. students in computer architecture. They cover a wide range of areas, as shown in Figure 2. The topics included represent the diverse scope of modern computer architecture. Processor architecture QAs comprise 32% of the dataset, with memory systems at 22% and interconnection networks at 10%. The 1,500 QAs in QuArch v0.1 have been expertly labeled by industry experts and Ph. D. students in computer architecture. They cover a wide range of areas, as shown in Figure 2. The topics included represent the diverse scope of modern computer architecture. Processor architecture QAs comprise 32% of the dataset, with memory systems at 22% and interconnection networks at 10%. Figure 2: Distribution of computer architecture topics in QuArch v0.1. Figure 2: Distribution of computer architecture topics in QuArch v0.1. Why Question-Answering for Architecture? Why Question-Answering for Architecture? Computer architecture encompasses many complex challenges spanning the entire technology stack. Addressing these cross-stack challenges requires synthesizing high-level insights with low-level implementations, demanding a deep understanding of workload behavior, architectural constraints, and system-level interactions. Computer architecture encompasses many complex challenges spanning the entire technology stack. Addressing these cross-stack challenges requires synthesizing high-level insights with low-level implementations, demanding a deep understanding of workload behavior, architectural constraints, and system-level interactions. Question-answering systems are particularly well-suited to support these multifaceted needs. QA tasks inherently require knowledge retrieval, reasoning, complex situational modeling, and input interpretation & manipulation— capabilities essential for solving architectural problems. By training LMs on QA datasets like QuArch, we can enhance their ability to assist in tasks ranging from optimizing cache hierarchies to designing efficient interconnects. Question-answering systems are particularly well-suited to support these multifaceted needs. QA tasks inherently require knowledge retrieval, reasoning, complex situational modeling, and input interpretation & manipulation — capabilities essential for solving architectural problems. By training LMs on QA datasets like QuArch, we can enhance their ability to assist in tasks ranging from optimizing cache hierarchies to designing efficient interconnects. Historically, QA datasets have been instrumental in advancing domain-specific, LLM-based tools across fields like medicine, math, law, finance, and software engineering. These tools enable applications such as diagnostic assistance, equation solving, legal research, financial modeling, and code debugging. Similar opportunities exist to build exciting agentic workflows for computer architecture. Still, the availability of data—or the lack thereof—remains a critical factor in revolutionizing traditional tools and unlocking new paradigms. Historically, QA datasets have been instrumental in advancing domain-specific, LLM-based tools across fields like medicine, finance, and software engineering. These tools enable applications such as diagnostic assistance, equation solving, legal research, financial modeling, and code debugging. Similar opportunities exist to build exciting agentic workflows for computer architecture. Still, the availability of data—or the lack thereof—remains a critical factor in revolutionizing traditional tools and unlocking new paradigms. Evaluating Architectural Knowledge of Language Models

Key points:

• Evaluating Architectural Knowledge of Language Models Using QuArch, we systematically evaluated language models’ computer architecture knowledge across multiple topics.

• Our evaluation revealed both promising results and concerning gaps.

• Using QuArch, we systematically evaluated language models’ computer architecture knowledge across multiple topics.

• Our evaluation revealed both promising results and concerning gaps.

• The top-performing model, shown in Figure 3, achieved an 84% accuracy rate on basic architectural concepts, demonstrating imperfect but strong foundational knowledge.

• However, this performance consistently dropped across some topics.

Three topics, in particular, caused the biggest drops in accuracy across models: Interconnection Networks, Memory Systems, and Benchmarking & Measurement. See details here. The top-performing model, shown in Figure 3, achieved an 84% accuracy rate on basic architectural concepts, demonstrating imperfect but strong foundational knowledge. However, this performance consistently dropped across some topics. Three topics, in particular, caused the biggest drops in accuracy across models: Interconnection Networks, Memory Systems, and Benchmarking & Measurement. See details Figure 3. Comparison of model accuracies on QuArch v0.1 QAs. See details here. Figure 3. Comparison of model accuracies on QuArch v0.1 QAs. See details More notably, performance dropped significantly by 12% when using smaller, open-source models (under 10B parameters). Furthermore, as discussed in the next section, we empirically observed that this gap becomes particularly pronounced when transitioning from factual questions to complex reasoning tasks. More notably, performance dropped significantly by 12% when using smaller, open-source models (under 10B parameters). Furthermore, as discussed in the next section, we empirically observed that this gap becomes particularly pronounced when transitioning from factual questions to complex reasoning tasks. These results show that while current models show promise in understanding basic architectural concepts, they still have weaknesses that will limit their ability to handle the complex reasoning and deep technical understanding that computer architects apply daily. Moreover, the disparity between small and large model performance suggests that effective, AI-based architecture tools will likely demand substantial resources or require greater specialization. These results show that while current models show promise in understanding basic architectural concepts, they still have weaknesses that will limit their ability to handle the complex reasoning and deep technical understanding that computer architects apply daily. Moreover, the disparity between small and large model performance suggests that effective, AI-based architecture tools will likely demand substantial resources or require greater specialization. Beyond Factual QA: Towards Complex Architectural Reasoning

Key points:

• Beyond Factual QA: Towards Complex Architectural Reasoning Beyond QuArch’s 1,500 QAs, which primarily evaluate the domain knowledge embedded in LMs, we also conducted a case study analyzing a publicly available Ph.

• qualifying exam question that demands advanced reasoning skills.

• Specifically, we examined optimizations like vectorization.

• Our findings revealed that even state-of-the-art models, such as GPT-4o, needed help interpreting loop dependencies and conditions essential for proper compiler vectorization.

• For instance, models misinterpreted simple dependency differences between i+1 and i-1, highlighting significant gaps in their reasoning capabilities.

Beyond QuArch’s 1,500 QAs, which primarily evaluate the domain knowledge embedded in LMs, we also conducted a case study analyzing a publicly available Ph. D. qualifying exam question that demands advanced reasoning skills. Specifically, we examined optimizations like vectorization. Our findings revealed that even state-of-the-art models, such as GPT-4o, needed help interpreting loop dependencies and conditions essential for proper compiler vectorization. For instance, models misinterpreted simple dependency differences between i+1 and i-1, highlighting significant gaps in their reasoning capabilities. The above case study demonstrates the need for more sophisticated datasets and targeted training to enhance model performance in complex architectural tasks. To assess and improve reasoning capabilities, we must move beyond factual QAs to develop complex, multi-step problems. These real-world problems must evaluate architectural reasoning across dimensions like performance analysis, design exploration, and hardware-software co-design. Building this comprehensive evaluation framework requires diverse expertise and architectural challenges that only our community can provide, which brings us to the next stage. The above case study demonstrates the need for more sophisticated datasets and targeted training to enhance model performance in complex architectural tasks. To assess and improve reasoning capabilities, we must move beyond factual QAs to develop complex, multi-step problems. These real-world problems must evaluate architectural reasoning across dimensions like performance analysis, design exploration, and hardware-software co-design. Building this comprehensive evaluation framework requires diverse expertise and architectural challenges that only our community can provide, which brings us to the next stage. Join the QuArch Community: Contribute to QuArch v1.0

Key points:

• Join the QuArch Community: Contribute to QuArch v1.0 Our initial QuArch development efforts engaged 50 undergraduate students primarily from electrical engineering and computer science backgrounds, offering valuable insights but revealing significant QA validation challenges due to varying levels of expertise.

• This experience highlighted the need for a broader, community-driven approach to develop a robust, high-quality dataset— a strategy that has proven successful across numerous domains.

• Crowdsourced datasets have consistently driven ML innovation, from ImageNet’s transformative impact on computer vision to Mozilla Common Voice’s advancements in speech recognition.

• Our initial QuArch development efforts engaged 50 undergraduate students primarily from electrical engineering and computer science backgrounds, offering valuable insights but revealing significant QA validation challenges due to varying levels of expertise.

• This experience highlighted the need for a broader, community-driven approach to develop a robust, high-quality dataset— a strategy that has proven successful across numerous domains.

• Crowdsourced datasets have consistently driven ML innovation, from ImageNet’s transformative impact on computer vision Mozilla Common Voice’s advancements in speech recognition.

How You Can Help

How You Can Help We invite computer architects, educators, industry professionals, and enthusiasts to contribute through our structured three-stage roadmap:

Key points:

• We invite computer architects, educators, industry professionals, and enthusiasts to contribute through our structured three-stage roadmap: • Stage 1: Expanding Fundamental Domain Knowledge: Annotate QAs on concepts like processor execution, memory hierarchy, and parallelism, building the critical foundation for advanced reasoning and design.

• Stage 1: Expanding Fundamental Domain Knowledge: Annotate QAs on concepts like processor execution, memory hierarchy, and parallelism, building the critical foundation for advanced reasoning and design.

• • Stage 2: Input Interpretation & Manipulation: Curate QAs based on tasks like analyzing instruction traces and optimizing code for engineering applications.

• We aim to create diverse practical questions from academia, industry, and public platforms by crowdsourcing questions.

• Stage 2: Input Interpretation & Manipulation: Curate QAs based on tasks like analyzing instruction traces and optimizing code for engineering applications.

• We aim to create diverse practical questions from academia, industry, and public platforms by crowdsourcing questions.

• Stage 3: Reasoning & Complex Situational Modeling: Craft reasoning-driven QAs that mirror real-world architectural challenges and require retrieval, reasoning, planning, and modeling skills. This stage enables models to reason about trade-offs and system-level design scenarios to push AI’s capabilities. Stage 3: Reasoning & Complex Situational Modeling: Craft reasoning-driven QAs that mirror real-world architectural challenges and require retrieval, reasoning, planning, and modeling skills. This stage enables models to reason about trade-offs and system-level design scenarios to push AI’s capabilities. All contributors will receive appropriate credit and be featured on our community leaderboard. All contributors will receive appropriate credit and be featured on our community leaderboard

Why Get Involved

Key points:

• Why Get Involved The computer architecture community has a strong legacy of collaboration with many of our most impactful tools built and sustained through community efforts, exemplified by projects like the gem5 simulator, RISC-V ISA, McPAT, and SimpleScalar.

• These projects demonstrate how collective contributions can create resources that advance research and industry practice.

• The computer architecture community has a strong legacy of collaboration with many of our most impactful tools built and sustained through community efforts, exemplified by projects like the gem5 simulator RISC-V ISA SimpleScalar.

• These projects demonstrate how collective contributions can create resources that advance research and industry practice.

• As we enter the era of AI-assisted architecture, QuArch offers a unique opportunity to shape the future of computer architecture through community efforts.

• All contributors will receive proper attribution, including co-authorship on QuArch publication(s), acknowledgment in documentation, and recognition on our community leaderboard.

As we enter the era of AI-assisted architecture, QuArch offers a unique opportunity to shape the future of computer architecture through community efforts. All contributors will receive proper attribution, including co-authorship on QuArch publication(s), acknowledgment in documentation, and recognition on our community leaderboard

Getting Involved

Key points:

• Getting Involved If you’re ready to help shape QuArch, sign up here to join our project using Label Studio for dataset annotation.

• Once your account is approved, it will take a brief minute for access to the QuArch v1.0 project and labeling interface (Figure 4) to appear.

• Please review our FAQs for labeling guidelines and best practices.

• If you’re ready to help shape QuArch, sign up join our project using Label Studio for dataset annotation.

• Once your account is approved, it will take a brief minute for access to the QuArch v1.0 project and labeling interface (Figure 4) to appear.

• Please review our for labeling guidelines and best practices.

Figure 4: Interface for QuArch dataset labeling. Figure 4: Interface for QuArch dataset labeling. You can also submit your own QAs through this form to contribute additional questions or other input. You can also submit your own QAs through this form to contribute additional questions or other input. If you are also interested in joining the broader Architecture 2.0 community, please fill out this form to be contacted about updates and meet-ups for QuArch and other related activities. If you are also interested in joining the broader Architecture 2.0 community, please fill out this form to be contacted about updates and meet-ups for QuArch and other related activities. Conclusion

Key points:

• Conclusion QuArch is an effort to push the boundaries of computer architecture through crowdsourcing.

• The goal is to leverage diverse questions from academia and industry while avoiding traditional IP constraints.

• By contributing to this dataset, we are shaping AI-driven tools and technologies that will impact the future.

• We hope you join the effort! QuArch is an effort to push the boundaries of computer architecture through crowdsourcing.

• The goal is to leverage diverse questions from academia and industry while avoiding traditional IP constraints.

• By contributing to this dataset, we are shaping AI-driven tools and technologies that will impact the future.

We hope you join the effort! Acknowledgments

Key points:

• Acknowledgments The development of QuArch was an extraordinary collective effort made possible by the dedication and contributions of a diverse and talented group of individuals across academia and industry.

• We want to especially acknowledge Amir Yazdanbakhsh (Google DeepMind), who, together with us, developed the vision for Architecture 2.0 that gave birth to QuArch.

• We are deeply grateful to our collaborators: Fin Amin (NCSU), Arnav Balyan, Yash Choudhary (IIT Bombay), Andy Cheng (Harvard), Sofia Giannuzzi (Harvard), Shreyas Grampurohit (IIT Bombay), Radhika Ghosal (Harvard), Jeffrey Ma (Harvard), Ankita Nayak (Qualcomm AI Research), Aadya Pipersenia (IIT Bombay), Jessica Quaye (Harvard), Arya Tschand (Harvard), Ike Uchendu (Harvard), and Jason Yik (Harvard) for their significant contributions to QuArch v0.1 and the arXiv report, which laid the foundation for many of the ideas presented in this blog.

• This year-long endeavor was further enriched by the efforts of over 40 students from around the world, whose hard work and creativity were important in shaping the early development of QuArch.

• The development of QuArch was an extraordinary collective effort made possible by the dedication and contributions of a diverse and talented group of individuals across academia and industry.

• We want to especially acknowledge Amir Yazdanbakhsh (Google DeepMind), who, together with us, developed the vision for Architecture 2.0 that gave birth to QuArch.

We are deeply grateful to our collaborators: Fin Amin (NCSU), Arnav Balyan, Yash Choudhary (IIT Bombay), Andy Cheng (Harvard), Sofia Giannuzzi (Harvard), Shreyas Grampurohit (IIT Bombay), Radhika Ghosal (Harvard), Jeffrey Ma (Harvard), Ankita Nayak (Qualcomm AI Research), Aadya Pipersenia (IIT Bombay), Jessica Quaye (Harvard), Arya Tschand (Harvard), Ike Uchendu (Harvard), and Jason Yik (Harvard) for their significant contributions to QuArch v0.1 and the arXiv report, which laid the foundation for many of the ideas presented in this blog. This year-long endeavor was further enriched by the efforts of over 40 students from around the world, whose hard work and creativity were important in shaping the early development of QuArch. About the Authors

Key points:

• About the Authors Shvetank Prakash is a fourth-year Ph.

• candidate at Harvard University focused on system design automation using machine learning techniques and open-source and emerging technologies for ultra-low power ML hardware architectures.

• Shvetank Prakash is a fourth-year Ph.

• candidate at Harvard University focused on system design automation using machine learning techniques and open-source and emerging technologies for ultra-low power ML hardware architectures.

Vijay Janapa Reddi is an Associate Professor at Harvard University, where his research focuses on computer architecture and machine learning systems for autonomous agents. He serves as Vice President and co-founder of MLCommons, driving community-driven initiatives in machine learning innovation. He is passionate about expanding access to applied machine learning through open-source education, as evidenced by his open-source book “Machine Learning Systems” (mlsysbook.ai) and the widely-adopted TinyML course series on edX. Vijay Janapa Reddi is an Associate Professor at Harvard University, where his research focuses on computer architecture and machine learning systems for autonomous agents. He serves as Vice President and co-founder of MLCommons, driving community-driven initiatives in machine learning innovation. He is passionate about expanding access to applied machine learning through open-source education, as evidenced by his open-source book “Machine Learning Systems” ( mlsysbook.ai ) and the widely-adopted TinyML course series on edX. Disclaimer: These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM. Disclaimer: These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM. Share this:

Share this: • Click to share on Facebook (Opens in new window)
Facebook
Click to share on Facebook (Opens in new window) Facebook • Click to share on X (Opens in new window)
## X
Click to share on X (Opens in new window) • Click to share on LinkedIn (Opens in new window)
LinkedIn
Click to share on LinkedIn (Opens in new window) LinkedIn.entry-content.et_post_meta_wrapper",1.0
https://arxiv.org/abs/2412.14135,Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective,Computer Science,1072,2025-07-08T16:55:25.306101,arxiv.org,Joshua Shou,U072Q1M48B0,2025-01-02T10:27:37.849029,,article,,,"rdf:RDF xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#""
xmlns:dc=""http://purl.org/dc/elements/1.1/""
xmlns:trackback=""http://madskills.com/public/xml/rss/module/trackback/"">
<rdf:Description
rdf:about=""/abs/2412.14135""
dc:identifier=""/abs/2412.14135""
dc:title=""Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective""
trackback:ping=""/trackback/2412.14135"" />
</rdf:RDF>

Computer Science > Artificial Intelligence

Computer Science > Artificial Intelligence arXiv:2412.14135 [Submitted on 18 Dec 2024]

Title:Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective

Title: Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective Authors: Zhiyuan Zeng Qinyuan Cheng Zhangyue Yin Bo Wang Shimin Li Yunhua Zhou Qipeng Guo Xuanjing Huang Xipeng Qiu View a PDF of the paper titled Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective, by Zhiyuan Zeng and 8 other authors View PDF HTML (experimental)
Abstract:OpenAI o1 represents a significant milestone in Artificial Inteiligence, which achieves expert-level performances on many challanging tasks that require strong reasoning this http URL has claimed that the main techinique behinds o1 is the reinforcement learining. Recent works use alternative approaches like knowledge distillation to imitate o1's reasoning style, but their effectiveness is limited by the capability ceiling of the teacher model. Therefore, this paper analyzes the roadmap to achieving o1 from the perspective of reinforcement learning, focusing on four key components: policy initialization, reward design, search, and learning. Policy initialization enables models to develop human-like reasoning behaviors, equipping them with the ability to effectively explore solution spaces for complex problems. Reward design provides dense and effective signals via reward shaping or reward modeling, which is the guidance for both search and learning. Search plays a crucial role in generating high-quality solutions during both training and testing phases, which can produce better solutions with more computation. Learning utilizes the data generated by search for improving policy, which can achieve the better performance with more parameters and more searched data. Existing open-source projects that attempt to reproduce o1 can be seem as a part or a variant of our roadmap. Collectively, these components underscore how learning and search drive o1's advancement, making meaningful contributions to the development of LLM. Abstract: OpenAI o1 represents a significant milestone in Artificial Inteiligence, which achieves expert-level performances on many challanging tasks that require strong reasoning this http URL has claimed that the main techinique behinds o1 is the reinforcement learining. Recent works use alternative approaches like knowledge distillation to imitate o1's reasoning style, but their effectiveness is limited by the capability ceiling of the teacher model. Therefore, this paper analyzes the roadmap to achieving o1 from the perspective of reinforcement learning, focusing on four key components: policy initialization, reward design, search, and learning. Policy initialization enables models to develop human-like reasoning behaviors, equipping them with the ability to effectively explore solution spaces for complex problems. Reward design provides dense and effective signals via reward shaping or reward modeling, which is the guidance for both search and learning. Search plays a crucial role in generating high-quality solutions during both training and testing phases, which can produce better solutions with more computation. Learning utilizes the data generated by search for improving policy, which can achieve the better performance with more parameters and more searched data. Existing open-source projects that attempt to reproduce o1 can be seem as a part or a variant of our roadmap. Collectively, these components underscore how learning and search drive o1's advancement, making meaningful contributions to the development of LLM. CONTEXT Subjects: Artificial Intelligence (cs. AI); Machine Learning (cs. LG) Cite as: arXiv:2412.14135 [cs. AI] arXiv:2412.14135v1 [cs. AI] for this version) https://doi.org/10.48550/arXiv.2412.14135 Focus to learn more tooltip description arXiv-issued DOI via DataCite

Submission history

Submission history From: Zhiyuan Zeng [ view email
Wed, 18 Dec 2024 18:24:47 UTC (1,122 KB)
end leftcolumn Full-text links:

Access Paper:

Access Paper: View a PDF of the paper titled Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective, by Zhiyuan Zeng and 8 other authors • View PDF
View PDF • HTML (experimental)
HTML (experimental) • TeX Source
TeX Source • Other Formats
Other Formats view license end full-text Current browse context: < prev next >
recent 2024-12 Change to browse by:

References & Citations

References & Citations • NASA ADS
NASA ADS • Google Scholar
Google Scholar • Semantic Scholar
Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation

BibTeX formatted citation loading... Data provided by:

Bookmark

Bookmark end extra-services LABS AREA Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer What is the Explorer? Connected Papers Toggle Connected Papers What is Connected Papers? Litmaps Toggle Litmaps What is Litmaps? scite.ai Toggle scite Smart Citations What are Smart Citations? Code, Data, Media

Code, Data and Media Associated with this Article

Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv What is alphaXiv? Links to Code Toggle CatalyzeX Code Finder for Papers What is CatalyzeX? DagsHub Toggle DagsHub What is DagsHub? GotitPub Toggle Gotit.pub What is GotitPub? Huggingface Toggle Hugging Face What is Huggingface? Links to Code Toggle Papers with Code What is Papers with Code? ScienceCast Toggle ScienceCast What is ScienceCast? Demos

Replicate Toggle Replicate What is Replicate? Spaces Toggle Hugging Face Spaces What is Spaces? Spaces Toggle TXYZ. AI What is TXYZ. AI? Related Papers

Recommenders and Search Tools

Recommenders and Search Tools Link to Influence Flower Influence Flower What are Influence Flowers? Core recommender toggle CORE Recommender What is CORE? • Author
Author • Venue
• Institution

Institution • Topic
About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs END LABS AREA Which authors of this paper are endorsers? Disable MathJax What is MathJax?","**Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective**



rdf:RDF xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#""
xmlns:dc=""http://purl.org/dc/elements/1.1/""
xmlns:trackback=""http://madskills.com/public/xml/rss/module/trackback/"">
<rdf:Description
rdf:about=""/abs/2412.14135""
dc:identifier=""/abs/2412.14135""
dc:title=""Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective""
trackback:ping=""/trackback/2412.14135"" />
</rdf:RDF>

Computer Science > Artificial Intelligence

Computer Science > Artificial Intelligence arXiv:2412.14135 [Submitted on 18 Dec 2024]

Title:Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective

Key points:

• Title: Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective Authors: Zhiyuan Zeng Qinyuan Cheng Zhangyue Yin Bo Wang Shimin Li Yunhua Zhou Qipeng Guo Xuanjing Huang Xipeng Qiu View a PDF of the paper titled Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective, by Zhiyuan Zeng and 8 other authors View PDF HTML (experimental)
Abstract:OpenAI o1 represents a significant milestone in Artificial Inteiligence, which achieves expert-level performances on many challanging tasks that require strong reasoning this http URL has claimed that the main techinique behinds o1 is the reinforcement learining.

• Recent works use alternative approaches like knowledge distillation to imitate o1's reasoning style, but their effectiveness is limited by the capability ceiling of the teacher model.

• Therefore, this paper analyzes the roadmap to achieving o1 from the perspective of reinforcement learning, focusing on four key components: policy initialization, reward design, search, and learning.

• Policy initialization enables models to develop human-like reasoning behaviors, equipping them with the ability to effectively explore solution spaces for complex problems.

• Reward design provides dense and effective signals via reward shaping or reward modeling, which is the guidance for both search and learning.

• Search plays a crucial role in generating high-quality solutions during both training and testing phases, which can produce better solutions with more computation.

Learning utilizes the data generated by search for improving policy, which can achieve the better performance with more parameters and more searched data. Existing open-source projects that attempt to reproduce o1 can be seem as a part or a variant of our roadmap. Collectively, these components underscore how learning and search drive o1's advancement, making meaningful contributions to the development of LLM. Abstract: OpenAI o1 represents a significant milestone in Artificial Inteiligence, which achieves expert-level performances on many challanging tasks that require strong reasoning this http URL has claimed that the main techinique behinds o1 is the reinforcement learining. Recent works use alternative approaches like knowledge distillation to imitate o1's reasoning style, but their effectiveness is limited by the capability ceiling of the teacher model. Therefore, this paper analyzes the roadmap to achieving o1 from the perspective of reinforcement learning, focusing on four key components: policy initialization, reward design, search, and learning. Policy initialization enables models to develop human-like reasoning behaviors, equipping them with the ability to effectively explore solution spaces for complex problems. Reward design provides dense and effective signals via reward shaping or reward modeling, which is the guidance for both search and learning. Search plays a crucial role in generating high-quality solutions during both training and testing phases, which can produce better solutions with more computation. Learning utilizes the data generated by search for improving policy, which can achieve the better performance with more parameters and more searched data. Existing open-source projects that attempt to reproduce o1 can be seem as a part or a variant of our roadmap. Collectively, these components underscore how learning and search drive o1's advancement, making meaningful contributions to the development of LLM. CONTEXT Subjects: Artificial Intelligence (cs. AI); Machine Learning (cs. LG) Cite as: arXiv:2412.14135 [cs. AI] arXiv:2412.14135v1 [cs. AI] for this version) https://doi.org/10.48550/arXiv.2412.14135 Focus to learn more tooltip description arXiv-issued DOI via DataCite

Submission history

Submission history From: Zhiyuan Zeng [ view email
Wed, 18 Dec 2024 18:24:47 UTC (1,122 KB)
end leftcolumn Full-text links:

Access Paper:

Access Paper: View a PDF of the paper titled Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective, by Zhiyuan Zeng and 8 other authors • View PDF
View PDF • HTML (experimental)
HTML (experimental) • TeX Source
TeX Source • Other Formats
Other Formats view license end full-text Current browse context: < prev next >
recent 2024-12 Change to browse by:

References & Citations

References & Citations • NASA ADS
## NASA ADS • Google Scholar
Google Scholar • Semantic Scholar
Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation

BibTeX formatted citation loading... Data provided by:

Bookmark

Bookmark end extra-services LABS AREA Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer What is the Explorer? Connected Papers Toggle Connected Papers What is Connected Papers? Litmaps Toggle Litmaps What is Litmaps? scite.ai Toggle scite Smart Citations What are Smart Citations? Code, Data, Media

Code, Data and Media Associated with this Article

Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv What is alphaXiv? Links to Code Toggle CatalyzeX Code Finder for Papers What is CatalyzeX? DagsHub Toggle DagsHub What is DagsHub? GotitPub Toggle Gotit.pub What is GotitPub? Huggingface Toggle Hugging Face What is Huggingface? Links to Code Toggle Papers with Code What is Papers with Code? ScienceCast Toggle ScienceCast What is ScienceCast? Demos

Replicate Toggle Replicate What is Replicate? Spaces Toggle Hugging Face Spaces What is Spaces? Spaces Toggle TXYZ. AI What is TXYZ. AI? Related Papers

Recommenders and Search Tools

Recommenders and Search Tools Link to Influence Flower Influence Flower What are Influence Flowers? Core recommender toggle CORE Recommender What is CORE? • Author
Author • Venue
• Institution

Institution • Topic
About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Key points:

• arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

• Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy.

• arXiv is committed to these values and only works with partners that adhere to them.

• Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy.

• arXiv is committed to these values and only works with partners that adhere to them.

• Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs END LABS AREA Which authors of this paper are endorsers? Disable MathJax What is MathJax?",1.0
https://github.com/Shubhamsaboo/awesome-llm-apps,"GitHub - Shubhamsaboo/awesome-llm-apps: Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.",Software Development,1781,2025-07-08T16:55:39.426531,github.com,Cristin Connerney,U059KFGJUSU,2025-01-07T10:29:13.976839,,website,A curated collection of LLM applications and resources utilizing AI agents and RAG.,,"Shubhamsaboo awesome-llm-apps Public • Notifications
You must be signed in to change notification settings
Notifications You must be signed in to change notification settings • Fork
5.6k
• Star

49k
Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models. Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models. www.theunwindai.com

License

License Apache-2.0 license Branches Activity Notifications You must be signed in to change notification settings

Shubhamsaboo/awesome-llm-apps

Shubhamsaboo/awesome-llm-apps Branches Go to file Open more actions menu

Folders and files

Folders and files Last commit message Last commit date

Latest commit

Latest commit

History

History 674 Commits.github/ workflows.github/ workflows advanced_ai_agents advanced_ai_agents advanced_llm_apps advanced_llm_apps banner banner google_adk_tutorials/ structured_output_agent google_adk_tutorials/ structured_output_agent mcp_ai_agents mcp_ai_agents rag_tutorials rag_tutorials starter_ai_agents starter_ai_agents voice_ai_agents voice_ai_agents.gitignore.gitignore LICENSE LICENSE README.md README.md View all files

Repository files navigation

Repository files navigation Deutsch |
Español |
français |
日本語 |
한국어 |
Português |
Русский |
中文

Deutsch Español français Português Русский

🌟 Awesome LLM Apps

🌟 Awesome LLM Apps A curated collection of Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer. A curated collection of Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

🤔 Why Awesome LLM Apps?

🤔 Why Awesome LLM Apps? • 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more. • 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP & RAG.
🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP & RAG. • 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.
🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

📂 Featured AI Projects

📂 Featured AI Projects

AI Agents

AI Agents

🌱 Starter AI Agents

🌱 Starter AI Agents • 🎙️ AI Blog to Podcast Agent
🎙️ AI Blog to Podcast Agent • ️🩹 AI Breakup Recovery Agent
️🩹 AI Breakup Recovery Agent • 📊 AI Data Analysis Agent
📊 AI Data Analysis Agent • 🩻 AI Medical Imaging Agent
🩻 AI Medical Imaging Agent • 😂 AI Meme Generator Agent (Browser)
😂 AI Meme Generator Agent (Browser) • 🎵 AI Music Generator Agent
🎵 AI Music Generator Agent • 🛫 AI Travel Agent (Local & Cloud)
🛫 AI Travel Agent (Local & Cloud) • Gemini Multimodal Agent
Gemini Multimodal Agent • 🌐 Local News Agent (OpenAI Swarm)
🌐 Local News Agent (OpenAI Swarm) • 🔄 Mixture of Agents
🔄 Mixture of Agents • 📊 xAI Finance Agent
📊 xAI Finance Agent • 🔍 OpenAI Research Agent
🔍 OpenAI Research Agent • 🕸️ Web Scrapping AI Agent (Local & Cloud)
🕸️ Web Scrapping AI Agent (Local & Cloud)

🚀 Advanced AI Agents

🚀 Advanced AI Agents • 🔍 AI Deep Research Agent
🔍 AI Deep Research Agent • 🤝 AI Consultant Agent
🤝 AI Consultant Agent • 🏗️ AI System Architect Agent
🏗️ AI System Architect Agent • 🎯 AI Lead Generation Agent
🎯 AI Lead Generation Agent • 💰 AI Financial Coach Agent
💰 AI Financial Coach Agent • 🎬 AI Movie Production Agent
🎬 AI Movie Production Agent • 📈 AI Investment Agent
📈 AI Investment Agent • 🏋️️ AI Health & Fitness Agent
🏋️️ AI Health & Fitness Agent • 🚀 AI Product Launch Intelligence Agent
🚀 AI Product Launch Intelligence Agent • 🗞️ AI Journalist Agent
🗞️ AI Journalist Agent • 🧠 AI Mental Wellbeing Agent
🧠 AI Mental Wellbeing Agent • 📑 AI Meeting Agent
📑 AI Meeting Agent • 🧬 AI Self-Evolving Agent
🧬 AI Self-Evolving Agent • 🎧 AI Social Media News and Podcast Agent
🎧 AI Social Media News and Podcast Agent

🎮 Autonomous Game Playing Agents

🎮 Autonomous Game Playing Agents • 🎮 AI 3D Pygame Agent
🎮 AI 3D Pygame Agent • AI Chess Agent
AI Chess Agent • 🎲 AI Tic-Tac-Toe Agent
🎲 AI Tic-Tac-Toe Agent

🤝 Multi-agent Teams

🤝 Multi-agent Teams • 🧲 AI Competitor Intelligence Agent Team
🧲 AI Competitor Intelligence Agent Team • 💲 AI Finance Agent Team
💲 AI Finance Agent Team • 🎨 AI Game Design Agent Team
🎨 AI Game Design Agent Team • 👨️ AI Legal Agent Team (Cloud & Local)
👨️ AI Legal Agent Team (Cloud & Local) • 💼 AI Recruitment Agent Team
💼 AI Recruitment Agent Team • 👨💼 AI Services Agency (CrewAI)
👨💼 AI Services Agency (CrewAI) • 👨🏫 AI Teaching Agent Team
👨🏫 AI Teaching Agent Team • 💻 Multimodal Coding Agent Team
💻 Multimodal Coding Agent Team • Multimodal Design Agent Team
Multimodal Design Agent Team • 🌏 AI Travel Planner Agent Team
🌏 AI Travel Planner Agent Team

🗣️ Voice AI Agents

🗣️ Voice AI Agents • 🗣️ AI Audio Tour Agent
🗣️ AI Audio Tour Agent • 📞 Customer Support Voice Agent
📞 Customer Support Voice Agent • 🔊 Voice RAG Agent (OpenAI SDK)
🔊 Voice RAG Agent (OpenAI SDK)

🌐 MCP AI Agents

🌐 MCP AI Agents • ️ Browser MCP Agent
️ Browser MCP Agent • 🐙 GitHub MCP Agent
🐙 GitHub MCP Agent • 📑 Notion MCP Agent
📑 Notion MCP Agent • 🌍 AI Travel Planner MCP Agent
🌍 AI Travel Planner MCP Agent

📀 RAG (Retrieval Augmented Generation)

📀 RAG (Retrieval Augmented Generation) • 🔗 Agentic RAG
🔗 Agentic RAG • 🧐 Agentic RAG with Reasoning
🧐 Agentic RAG with Reasoning • 📰 AI Blog Search (RAG)
📰 AI Blog Search (RAG) • 🔍 Autonomous RAG
🔍 Autonomous RAG • 🔄 Corrective RAG (CRAG)
🔄 Corrective RAG (CRAG) • 🐋 Deepseek Local RAG Agent
🐋 Deepseek Local RAG Agent • 🤔 Gemini Agentic RAG
🤔 Gemini Agentic RAG • 👀 Hybrid Search RAG (Cloud)
👀 Hybrid Search RAG (Cloud) • 🔄 Llama 3.1 Local RAG
🔄 Llama 3.1 Local RAG • 🖥️ Local Hybrid Search RAG
🖥️ Local Hybrid Search RAG • 🦙 Local RAG Agent
🦙 Local RAG Agent • 🧩 RAG-as-a-Service
🧩 RAG-as-a-Service • RAG Agent with Cohere
RAG Agent with Cohere • ️ Basic RAG Chain
️ Basic RAG Chain • 📠 RAG with Database Routing
📠 RAG with Database Routing • 🖼️ Vision RAG
🖼️ Vision RAG

💾 LLM Apps with Memory Tutorials

💾 LLM Apps with Memory Tutorials • 💾 AI ArXiv Agent with Memory
💾 AI ArXiv Agent with Memory • 🛩️ AI Travel Agent with Memory
🛩️ AI Travel Agent with Memory • 💬 Llama3 Stateful Chat
💬 Llama3 Stateful Chat • 📝 LLM App with Personalized Memory
📝 LLM App with Personalized Memory • 🗄️ Local ChatGPT Clone with Memory
🗄️ Local ChatGPT Clone with Memory • 🧠 Multi-LLM Application with Shared Memory
🧠 Multi-LLM Application with Shared Memory

💬 Chat with X Tutorials

💬 Chat with X Tutorials • 💬 Chat with GitHub (GPT & Llama3)
💬 Chat with GitHub (GPT & Llama3) • 📨 Chat with Gmail
📨 Chat with Gmail • 📄 Chat with PDF (GPT & Llama3)
📄 Chat with PDF (GPT & Llama3) • 📚 Chat with Research Papers (ArXiv) (GPT & Llama3)
📚 Chat with Research Papers (ArXiv) (GPT & Llama3) • 📝 Chat with Substack
📝 Chat with Substack • 📽️ Chat with YouTube Videos
📽️ Chat with YouTube Videos

🔧 LLM Fine-tuning Tutorials

🔧 LLM Fine-tuning Tutorials • 🔧 Llama 3.2 Fine-tuning
🔧 Llama 3.2 Fine-tuning

🚀 Getting Started

🚀 Getting Started • Clone the repository
git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git
Clone the repository

Clone the repository
git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git

git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git • Navigate to the desired project directory
cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
Navigate to the desired project directory

Navigate to the desired project directory
cd awesome-llm-apps/starter_ai_agents/ai_travel_agent

awesome-llm-apps/starter_ai_agents/ai_travel_agent • Install the required dependencies
pip install -r requirements.txt
Install the required dependencies

Install the required dependencies
pip install -r requirements.txt

pip install -r requirements.txt • Follow the project-specific instructions in each project's README.md file to set up and run the app. Follow the project-specific instructions in each project's README.md file to set up and run the app. Follow the project-specific instructions in each project's
README.md

README.md file to set up and run the app.

🤝 Contributing to Open Source

🤝 Contributing to Open Source Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new GitHub Issue or submit a pull request. Make sure to follow the existing project structure and include a detailed README.md for each new app. Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new GitHub Issue or submit a pull request. Make sure to follow the existing project structure and include a detailed
README.md

README.md for each new app. Thank You, Community, for the Support! 🙏

Thank You, Community, for the Support! 🙏 🌟 Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents. Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents. About

Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models. Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models. www.theunwindai.com

Topics

Topics python

Resources

Resources Readme

License

License Apache-2.0 license

Uh oh! Uh oh! There was an error while loading. Please reload this page. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page Activity

Stars

Watchers

Watchers watching

Forks

Report repository

Releases

Releases No releases published

Packages
0

Packages No packages published

Uh oh! Uh oh! There was an error while loading. Please reload this page. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page

Contributors
44

Contributors + 30 contributors

Languages

Languages • Python
60.0%
Python • JavaScript
31.6%
JavaScript • TypeScript
7.8%
TypeScript • CSS
0.3%
• PLpgSQL

0.1%
PLpgSQL • HTML
0.1%
• Other

0.1%","**Website Description:**

The GitHub repository 'awesome-llm-apps' curated by Shubhamsaboo serves as a comprehensive collection of innovative applications utilizing Large Language Models (LLMs) and AI agents. This repository showcases a variety of tools and resources that leverage advanced models from OpenAI, Anthropic, Gemini, and various open-source alternatives, enabling users to explore and implement AI-driven solutions in their projects. 

Targeted towards developers, researchers, and AI enthusiasts, the repository provides valuable insights and practical examples of LLM applications, including those that utilize Retrieval-Augmented Generation (RAG), multi-agent systems, and voice agents. Key features include a well-organized structure of resources, detailed tutorials, and the ability to run applications locally, making it an essential hub for anyone interested in the latest advancements in AI technology.",1.0
