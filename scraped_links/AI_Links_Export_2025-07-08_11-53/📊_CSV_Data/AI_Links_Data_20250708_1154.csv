url,title,category,word_count,scraped_at,domain,slack_user,slack_user_id,slack_timestamp,slack_channel,content_type,brief_description,article_summary,full_content,formatted_content,completeness_ratio
https://yidingjiang.github.io/blog/post/exploration/,The Era of Exploration,Artificial Intelligence,6325,2025-07-08T11:54:45.981192,yidingjiang.github.io,Sai,U07TKPGTKDF,2025-07-07T11:21:22.591239,,article,,,"Large language models are the unintended byproduct of about three decades worth of freely accessible human text online. Ilya Sutskever compared this reservoir of information to fossil fuel, abundant but ultimately finite. Some studies suggest that, at current token‑consumption rates, frontier labs could exhaust the highest‑quality English web text well before the decade ends. Even if those projections prove overly pessimistic, one fact is clear: today’s models consume data far faster than humans can produce it. Large language models are the unintended byproduct of about three decades worth of freely accessible human text online. Ilya Sutskever compared this reservoir of information to fossil fuel, abundant but ultimately finite. Some studies suggest that, at current token‑consumption rates, frontier labs could exhaust the highest‑quality English web text well before the decade ends. Even if those projections prove overly pessimistic, one fact is clear: today’s models consume data far faster than humans can produce it. David Silver and Richard Sutton call this coming phase the “Era of Experience,”  where meaningful progress will depend on data that learning agents generate for themselves. In this post, I want to build on their statement further: the bottleneck is not having just any experience but collecting the right kind of experience that benefits learning. The next wave of AI progress will hinge less on stacking parameters and more on exploration, the process of acquiring new and informative experience. David Silver and Richard Sutton call this coming phase the “ Era of Experience,”  where meaningful progress will depend on data that learning agents generate for themselves. In this post, I want to build on their statement further: the bottleneck is not having just experience but collecting the kind of experience that benefits learning. The next wave of AI progress will hinge less on stacking parameters and more on exploration, the process of acquiring new and informative experience. To talk about experience collection, we must also ask what it costs to collect them. Scaling is, in the end, a question of resources – compute cycles, synthetic‑data generation, data curation pipelines, human oversight, any expenditure that creates learning signal. For simplicity, I’ll fold all of these costs into a single bookkeeping unit I call flops. Strictly speaking, a flop is one floating‑point operation, but the term has become a lingua franca for “how much effort did this system consume?” I’m co‑opting it here not for its engineering precision but because it gives us a common abstract currency. My discussion depends only on relative spend, not on the particular mix of silicon, data, or human time. Treat flops as shorthand for “whatever scarce resource constrains scale.”

To talk about experience collection, we must also ask what it costs to collect them. Scaling is, in the end, a question of resources – compute cycles, synthetic‑data generation, data curation pipelines, human oversight, any expenditure that creates learning signal. For simplicity, I’ll fold all of these costs into a single bookkeeping unit I call. Strictly speaking, a flop is one floating‑point operation, but the term has become a lingua franca for “how much effort did this system consume?” I’m co‑opting it here not for its engineering precision but because it gives us a common abstract currency. My discussion depends only on relative spend, not on the particular mix of silicon, data, or human time. Treat flops as shorthand for “whatever scarce resource constrains scale.” In the sections that follow, I’ll lay out a handful of observations and connect ideas that usually appear in different contexts. Exploration is most often used in the context of reinforcement learning (RL), but I will also use “exploration” in a broader sense – much wider than its usual role in RL – because every data-driven system has to decide which experiences to collect before it can learn from them. This usage of exploration is also inspired by my friend Minqi’s excellent article “General intelligence requires rethinking exploration.”

In the sections that follow, I’ll lay out a handful of observations and connect ideas that usually appear in different contexts. Exploration is most often used in the context of reinforcement learning (RL), but I will also use “exploration” in a broader sense – much wider than its usual role in RL – because every data-driven system has to decide which experiences to collect before it can learn from them. This usage of exploration is also inspired by my friend ’s excellent article “ General intelligence requires rethinking exploration The rest of the post is organized as the following: first, how pre‑training inadvertently solved a part of the exploration problem, second, why better exploration translates into better generalization, and finally, where we should spend the next hundred thousand GPU‑years. The rest of the post is organized as the following: first, how pre‑training inadvertently solved a part of the exploration problem, second, why better exploration translates into better generalization, and finally, where we should spend the next hundred thousand GPU‑years. Pretraining is exploration

Pretraining is exploration The standard LLM pipeline is to first pretrain a large model on next-token prediction with a large amount of text and then finetune the model with RL to achieve some desired objectives. Without large-scale pretraining, the RL step would struggle to make any progress. This contrast suggests that pretraining has accomplished something that is difficult for tabula rasa RL (i.e., from scratch). The standard LLM pipeline is to first pretrain a large model on next-token prediction with a large amount of text and then finetune the model with RL to achieve some desired objectives. Without large-scale pretraining, the RL step would struggle to make any progress. This contrast suggests that pretraining has accomplished something that is difficult for tabula rasa RL (i.e., from scratch). A seemingly contradictory and widely observed trend in recent research is that smaller models can demonstrate significantly improved reasoning abilities once distilled using the chain-of-thought generated by larger, more capable models. Some interpret this as evidence that large scale is not a prerequisite for effective reasoning. In my opinion, this conclusion is misguided. The question we should ask is: if model capacity is not the bottleneck for reasoning, why do small models need to distill from a larger model at all? A seemingly contradictory and widely observed trend in recent research is that smaller models can demonstrate significantly improved reasoning abilities once distilled using the chain-of-thought generated by larger, more capable models. Some interpret this as evidence that large scale is not a prerequisite for effective reasoning. In my opinion, this conclusion is misguided. The question we should ask is: if model capacity is not the bottleneck for reasoning, why do small models need to distill from a larger model at all? A compelling explanation for both observations is that the immense cost of pretraining is effectively paying a massive, upfront “exploration tax.” By themselves, models with no pretraining or smaller pretrained models have a much harder time reliably exploring the solution space and discovering good solutions on their own1. The pretraining stage pays this tax by spending vast amounts of compute on diverse data to learn a rich sampling distribution under which the correct continuations are likely. Distillation, in turn, is a mechanism for letting a smaller model inherit that payment, bootstrapping its exploration capabilities from the massive investment made in the larger model. A compelling explanation for both observations is that the immense cost of pretraining is effectively paying a massive, upfront “ exploration tax.” By themselves, models with no pretraining or smaller pretrained models have a much harder time reliably exploring the solution space and discovering good solutions on their own. The pretraining stage pays this tax by spending vast amounts of compute on diverse data to learn a rich sampling distribution under which the correct continuations are likely. Distillation, in turn, is a mechanism for letting a smaller model inherit that payment, bootstrapping its exploration capabilities from the massive investment made in the larger model. Why is this pre-paid exploration so important? In its most general form, the RL loop looks something like this:

Why is this pre-paid exploration so important? In its most general form, the RL loop looks something like this: • Exploration. The agent generates some randomized exploration trajectories. Exploration. The agent generates some randomized exploration trajectories. • Reinforce. The good trajectories are up-weighted and the bad ones are down-weighted. Reinforce. The good trajectories are up-weighted and the bad ones are down-weighted. For this learning loop to be effective, the agent must be capable of generating at least a minimal number of “good” trajectories during its exploration phase. This concept is sometimes called coverage in RL. In LLMs, this exploration is typically achieved through sampling from the model’s autoregressive output distribution. Under this exploration scheme, the correct solutions need to already be likely in the naive sampling distribution. If a lower‑capacity model rarely stumbles on a valid solution by random sampling, it would have nothing useful to reinforce. For this learning loop to be effective, the agent must be capable of generating at least a minimal number of “good” trajectories during its exploration phase. This concept is sometimes called coverage in RL. In LLMs, this exploration is typically achieved through sampling from the model’s autoregressive output distribution. Under this exploration scheme, the correct solutions need to already be likely in the naive sampling distribution. If a lower‑capacity model rarely stumbles on a valid solution by random sampling, it would have nothing useful to reinforce. Exploration without any prior information is a very difficult process. Even in the simplest tabular RL setting, where every situation (a state) and every action can be listed out in a table, theory says learning needs a lot of trials. A well known lower-bound on the sample complexity on the number of episodes for tabular RL is \(\Omega(\frac{SAH^2}{\epsilon^2})\) (Dann & Brunskill, 2015), where \(S\) is the size of the state space, \(A\) is the size of the action space, \(H\) is the horizon, and \(\epsilon\) is the “distance” to the best possible solution. This means that the minimum number of episodes grows linearly with the number of state-action pairs, and quadratically with the horizon. For LLMs, the state space now includes every possible text prefix, and the action space is any next token, both of which are very large. Without any prior information, RL in this setting would be practically impossible. Exploration without any prior information is a very difficult process. Even in the simplest tabular RL setting, where every situation (a state) and every action can be listed out in a table, theory says learning needs a lot of trials. A well known lower-bound on the sample complexity on the number of episodes for tabular RL is \(\Omega(\frac{SAH^2}{\epsilon^2})\) (Dann & Brunskill, 2015), where \(S\) is the size of the state space, \(A\) is the size of the action space, \(H\) is the horizon, and \(\epsilon\) is the “distance” to the best possible solution. This means that the minimum number of episodes grows linearly with the number of state-action pairs, and quadratically with the horizon. For LLMs, the state space now includes every possible text prefix, and the action space is any next token, both of which are very large. Without any prior information, RL in this setting would be practically impossible. Up to now, the hard work of exploration has been largely done by pretraining and learning a better prior from which to sample trajectories. However, this also means that the types of trajectories the model can sample naively are heavily constrained by the prior. To progress further, we must figure out how to move beyond the prior. Up to now, the hard work of exploration has been largely done by pretraining and learning a better prior from which to sample trajectories. However, this also means that the types of trajectories the model can sample naively are heavily constrained by the prior. To progress further, we must figure out how to move beyond the prior. Exploration helps generalization

Exploration helps generalization Historically, RL research has focused on solving a single environment at a time, like Atari or MuJoCo. This is equivalent to training and testing on the same datapoint. However, how well a given model does in the single environment does not say much about how well the model can handle truly novel situations. Machine learning is ultimately about generalization: for many hard problems, if we know them in advance, we can engineer a bespoke solution. What matters is succeeding on problems we haven’t seen or even anticipated. Historically, RL research has focused on solving a single environment at a time, like Atari or MuJoCo. This is equivalent to training and testing on the same datapoint. However, how well a given model does in the single environment does not say much about how well the model can handle truly novel situations. Machine learning is ultimately about generalization: for many hard problems, if we know them in advance, we can engineer a bespoke solution. What matters is succeeding on problems we haven’t seen or even anticipated. The generalization performance of RL is critical for language models. During training, an LLM sees only a finite set of prompts, yet at deployment it must handle arbitrary user queries that could be very different from the ones seen during training. Notably, current LLMs excel at tasks with a verifiable reward (e.g., coding puzzles or formal proofs) because the correctness can be easily checked. The tougher question is to generalize these capabilities to fuzzier domains (e.g., generate a research report or write a novel) where feedback is sparse or ambiguous, and large-scale training and data collection are difficult. The generalization performance of RL is critical for language models. During training, an LLM sees only a finite set of prompts, yet at deployment it must handle arbitrary user queries that could be very different from the ones seen during training. Notably, current LLMs excel at tasks with a verifiable reward (e.g., coding puzzles or formal proofs) because the correctness can be easily checked. The tougher question is to generalize these capabilities to fuzzier domains (e.g., generate a research report or write a novel) where feedback is sparse or ambiguous, and large-scale training and data collection are difficult. What are some options we have for training a generalizable model? A recurring theme of deep learning is that data diversity drives robust generalization. Exploration directly controls the diversity of the data. In supervised learning, a labeled example reveals all its details in a single forward pass2 so the only way to increase data diversity is to collect more data. In RL, by contrast, each interaction only exposes a narrow slice of the environment. As a result, the agent must gather a sufficiently varied set of trajectories to build a representative picture. If the trajectories collected lack diversity (e.g., naive random sampling), the policy can overfit to that narrow slice and stumble even within the very same environment. What are some options we have for training a generalizable model? A recurring theme of deep learning is that data diversity drives robust generalization. Exploration directly controls the diversity of the data. In supervised learning, a labeled example reveals all its details in a single forward pass so the only way to increase data diversity is to collect more data. In RL, by contrast, each interaction only exposes a narrow slice of the environment. As a result, the agent must gather a sufficiently varied set of trajectories to build a representative picture. If the trajectories collected lack diversity (e.g., naive random sampling), the policy can overfit to that narrow slice and stumble even within the very same environment. This problem compounds when there are multiple environments. A popular RL generalization benchmark is Procgen, which is a collection of Atari-like games that have procedurally generated environments, so each game in principle contains “infinitely” many environments. The objective is to train on a fixed number of environments for a fixed number of steps and generalize to completely unseen environments3. This problem compounds when there are multiple environments. A popular RL generalization benchmark is Procgen, which is a collection of Atari-like games that have procedurally generated environments, so each game in principle contains “infinitely” many environments. The objective is to train on a fixed number of environments for a fixed number of steps and generalize to completely unseen environments Many existing approaches for this benchmark treat the problem as a representation learning problem and apply regularization techniques adapted from supervised learning (e.g., dropout or data augmentation). These help, but they overlook exploration, one of the most important structural components of RL. Since the agents collect their own data, they can improve generalization by changing exploration. In a previous work, my coauthors and I showed that pairing an existing RL algorithm with a stronger exploration strategy can double its generalization performance on Procgen without explicit regularization. In a more recent work, we found that better exploration also lets the model leverage more expressive model architectures and computational resources, and generalize better on Procgen as the result4. Many existing approaches for this benchmark treat the problem as a representation learning problem and apply regularization techniques adapted from supervised learning (e.g., dropout or data augmentation). These help, but they overlook exploration, one of the most important structural components of RL. Since the agents collect their own data, they can improve generalization by changing exploration. In a previous work, my coauthors and I showed that pairing an existing RL algorithm with a stronger exploration strategy can double its generalization performance on Procgen without explicit regularization. In a more recent work, we found that better exploration also lets the model leverage more expressive model architectures and computational resources, and generalize better on Procgen as the result While Procgen is certaintly not as difficult and complex as the problems LLMs are trained to solve today, the overall problem structure is essentially the same – the RL agent is trained on a finite set of problems and tested on new problems at test time without further training. The way we do exploration with LLMs today is fairly simple, typically limited to sampling from the model’s autoregressive distribution with tweaks to temperature or entropy bonus, so there is a large design space for potentially better exploration approaches. Admittedly, there have not been many successful examples in this direction. This could be because it is a very hard problem, it is not flop-efficient enough to be practical, or we just haven’t tried hard enough. However, if Procgen-style exploration gains do translate, we’re leaving efficiency – and perhaps entirely new capabilities – on the table. The next section discusses where we might look first. While Procgen is certaintly not as difficult and complex as the problems LLMs are trained to solve today, the overall problem structure is essentially the same – the RL agent is trained on a finite set of problems and tested on new problems at test time without further training. The way we do exploration with LLMs today is fairly simple, typically limited to sampling from the model’s autoregressive distribution with tweaks to temperature or entropy bonus, so there is a large design space for potentially better exploration approaches. Admittedly, there have not been many successful examples in this direction. This could be because it is a very hard problem, it is not flop-efficient enough to be practical, or we just haven’t tried hard enough. However, if Procgen-style exploration gains do translate, we’re leaving efficiency – and perhaps entirely new capabilities – on the table. The next section discusses where we might look first. Two axes of scaling exploration

Two axes of scaling exploration Exploration, in the broad sense I’m using here, is deciding what data the learner will see. That decision happens on two distinct axes:

Exploration, in the broad sense I’m using here, is deciding what data the learner will see. That decision happens on two distinct axes: • World sampling – deciding where to learn. World here refers to a particular problem that needs to be solved. In supervised learning (or unsupervised pretraining), this axis covers data collection, synthetic generation and curation: gathering and filtering raw documents, images, or code, each of which corresponds to a “world”. In RL, this corresponds to designing or generating environments, such as a single math puzzle or a coding problem. We can even arrange the worlds into curricula. In both cases, world sampling is fundamentally about what “data points” the learner is allowed to see. This also decides the limit on all the information any agent can possibly learn. World sampling – deciding where to learn. World here refers to a particular problem that needs to be solved. In supervised learning (or unsupervised pretraining), this axis covers data collection, synthetic generation and curation: gathering and filtering raw documents, images, or code, each of which corresponds to a “world”. In RL, this corresponds to designing or generating environments, such as a single math puzzle or a coding problem. We can even arrange the worlds into curricula. In both cases, world sampling is fundamentally about what “data points” the learner is allowed to see. This also decides the limit on all the information any agent can possibly learn. • Path sampling – deciding how to gather data inside a world. This step is unique to RL. Once a world is chosen, the agent still has to pick which trajectories to collect: random walks, curiosity‑driven policies, tree search, tool-use, etc. Different path‑sampling strategies can have different computation cost and produce very different training distributions even when the underlying world is identical. In short, path sampling is about what the learner “wants” to see. Path sampling – deciding how to gather data inside a world. This step is unique to RL. Once a world is chosen, the agent still has to pick which trajectories to collect: random walks, curiosity‑driven policies, tree search, tool-use, etc. Different path‑sampling strategies can have different computation cost and produce very different training distributions even when the underlying world is identical. In short, path sampling is about what the learner “wants” to see. In supervised learning or unsupervised pretraining, the second axis incurs a constant cost because a single forward (and backward) pass has access to all the information each data point contains (e.g., cross-entropy loss). Because there is no obvious way to “dig deeper” inside a single example (other than make the models larger), the exploration cost lives almost entirely on the first axis – world sampling. The flops can either go into acquire new worlds (e.g., new data points) or processing existing worlds (e.g., curation and synthetic data). In supervised learning or unsupervised pretraining, the second axis incurs a constant cost because a single forward (and backward) pass has access to all the information each data point contains (e.g., cross-entropy loss). Because there is no obvious way to “dig deeper” inside a single example (other than make the models larger), the exploration cost lives almost entirely on the first axis – world sampling. The flops can either go into acquire new worlds (e.g., new data points) or processing existing worlds (e.g., curation and synthetic data). In contrast, RL has much more flexibility in the second axis (in addition to the first axis). Because most random trajectories reveal little information about the ideal behavior, the information density (useful bits per flop) in RL is far lower than in supervised learning or pretraining. If we naïvely sample trajectory, we risk wasting flops on noise. So it’s imporant to be judicious about how we spend our flops5. There are also more options for spending flops to explore within each world. For example, we can either sample more trajectories from a single environment or we can spend more flops thinking about how to sample the next trajectory to discover high-value states and actions. In contrast, RL has much more flexibility in the second axis (in addition to the first axis). Because most random trajectories reveal little information about the ideal behavior, the information density (useful bits per flop) in RL is far lower than in supervised learning or pretraining. If we naïvely sample trajectory, we risk wasting flops on noise. So it’s imporant to be judicious about how we spend our flops. There are also more options for spending flops to explore within each world. For example, we can either sample more trajectories from a single environment or we can spend more flops thinking about how to sample the next trajectory to discover high-value states and actions. For most, if not all, machine learning problems, the high-level goal can be understood as maximizing information per flop. For that purpose, these two levers form a trade-off curve. If one spends too much resources on world sampling and not enough on path sampling, the agent may not extract any meaningful experience from the sampled worlds. Vice versa, if one spends too much resources on a small set of worlds, the agent could overfit to the training worlds and would not learn generalizable behavior that transfer across worlds. The ideal scenario happens somewhere in between where the resources are divided between sampling new worlds and running algorithms (i.e., better than random sampling) that can extract more information from a single world. For most, if not all, machine learning problems, the high-level goal can be understood as maximizing information per flop. For that purpose, these two levers form a trade-off curve. If one spends too much resources on world sampling and not enough on path sampling, the agent may not extract any meaningful experience from the sampled worlds. Vice versa, if one spends too much resources on a small set of worlds, the agent could overfit to the training worlds and would not learn generalizable behavior that transfer across worlds. The ideal scenario happens somewhere in between where the resources are divided between sampling new worlds and running algorithms (i.e., better than random sampling) that can extract more information from a single world. If you are familiar with scaling laws, what I just described sounds a lot like the Chinchilla scaling laws but the two axes correspond to the compute used for different types of sampling rather than parameters and data. At each performance level, one should to be able to trace out an isoperformance curve where the x-axis and y-axis are the compute put into interacting with any given environment and the compute given to the environments, whether it is for generating the environment or for running the environment (e.g., a generative verifier with CoT). If you are familiar with scaling laws, what I just described sounds a lot like the Chinchilla scaling laws but the two axes correspond to the compute used for different types of sampling rather than parameters and data. At each performance level, one should to be able to trace out an isoperformance curve where the x-axis and y-axis are the compute put into interacting with any given environment and the compute given to the environments, whether it is for generating the environment or for running the environment (e.g., a generative verifier with CoT). Of the two axes, path sampling is a relatively well-defined problem. A principled approach for doing exploration within an environment is to reduce the model’s uncertainty6. Many existing approaches for exploration have very strong sample complexity but they tend to be prohibitively expensive. Nonetheless, there is arguably a well-defined objective for path sampling and the main obstacle is to figure out a computationally efficient approximation. On the other hand, it is much less clear what the objective is for world sampling. One appealing idea is open-ended learning but even open-ended learning requires defining the universe of all environments (i.e., environment specs) or a subjective observer that judges whether an outcome is “interesting”. Of the two axes, path sampling is a relatively well-defined problem. A principled approach for doing exploration within an environment is to reduce the model’s uncertainty. Many existing approaches for exploration have very strong sample complexity but they tend to be prohibitively expensive. Nonetheless, there is arguably a well-defined objective for path sampling and the main obstacle is to figure out a computationally efficient approximation. On the other hand, it is much less clear what the objective is for world sampling. One appealing idea is open-ended learning but even open-ended learning requires defining the universe of all environments (i.e., environment specs) or a subjective observer that judges whether an outcome is “interesting”. What objective should world sampling optimize? The unfortunate reality (or fortunate, depending on your perspective) is that the space of environments is infinite, but our resources are finite. If we want to do something useful, then we must express some preference over the environments. I suspect that the problem of designing environments will eventually become similar to selecting pretraining data. It will be hard to say exactly why one environment will help another environment and we will need a lot of them. In other words, there may not be a single clean and nice objective for designing environment specs. What objective should world sampling optimize? The unfortunate reality (or fortunate, depending on your perspective) is that the space of environments is infinite, but our resources are finite. If we want to do something useful, then we must express some preference over the environments. I suspect that the problem of designing environments will eventually become similar to selecting pretraining data. It will be hard to say exactly why one environment will help another environment and we will need a lot of them. In other words, there may not be a single clean and nice objective for designing environment specs. The more likely scenario (probably already happening) is that everyone will start designing specs within their own expertise or domain of interest. When we have enough “human-approved” and “useful” specs, maybe we can try to learn some common principles and eventually automate the process by learning from them, much like how pretraining data is selected today. It would be inconvenient if we need the same amount of environments as pretraining data to achieve the same level of generality for decision making, but there are some preliminary evidence that this may not be the case. In a recent work, we found that a fairly small number of environments is enough to train an agent capable of general exploration and decision making in entirely out-of-distribution environments. Furthermore, using existing LLMs could also greatly accelerate the design process. The more likely scenario (probably already happening) is that everyone will start designing specs within their own expertise or domain of interest. When we have enough “human-approved” and “useful” specs, maybe we can try to learn some common principles and eventually automate the process by learning from them, much like how pretraining data is selected today. It would be inconvenient if we need the same amount of environments as pretraining data to achieve the same level of generality for decision making, but there are some preliminary evidence that this may not be the case. In a recent work, we found that a fairly small number of environments is enough to train an agent capable of general exploration and decision making in entirely out-of-distribution environments. Furthermore, using existing LLMs could also greatly accelerate the design process. Of course, these are all very high-level musings and how one exactly scales these two axes is much less obvious than scaling pretraining. However, if we were able to figure out a reliable way to introduce scale into world sampling, and a more intelligent way of path sampling, we should be able to see isoperformance curves that bends inwards towards the origin (maybe they won’t be as smooth). This style of scaling laws would teach us the best way to allocate computation resource between the environments and the agent. Of course, these are all very high-level musings and how one exactly scales these two axes is much less obvious than scaling pretraining. However, if we were able to figure out a reliable way to introduce scale into world sampling, and a more intelligent way of path sampling, we should be able to see isoperformance curves that bends inwards towards the origin (maybe they won’t be as smooth). This style of scaling laws would teach us the best way to allocate computation resource between the environments and the agent. Final thoughts

Final thoughts I could keep unfolding more tangents – better curiosity objectives, open-endedness, meta‑exploration that learns how to explore – but I think it is more important to make the high-level point clear. I could keep unfolding more tangents – better curiosity objectives open-endedness meta‑exploration that learns how to explore – but I think it is more important to make the high-level point clear. Existing paradigms of scaling have been incredibly effective but all paradigms will eventually saturate. The question is where to pour the next orders of magnitude of compute. I’ve argued that exploration – both world sampling and path sampling – offers a promising direction. We don’t yet know the right scaling laws, the right environment generators, or the right exploration objectives, but intuitively they should be possible. The coming years will decide whether exploration can stretch our flops further on top of the existing paradigms. I think the bet is worth making. Existing paradigms of scaling have been incredibly effective but all paradigms will eventually saturate. The question is where to pour the next orders of magnitude of compute. I’ve argued that exploration – both world sampling and path sampling – offers a promising direction. We don’t yet know the right scaling laws, the right environment generators, or the right exploration objectives, but intuitively they should be possible. The coming years will decide whether exploration can stretch our flops further on top of the existing paradigms. I think the bet is worth making. Acknowledgement

Acknowledgement Many thanks to Allan Zhou, Sam Sokota, Minqi Jiang, Ellie Haber, Alex Robey, Swaminathan Gurumurthy, Kevin Li, Calvin Luo, Abitha Thankaraj, and Zico Kolter for their feedback and discussion on the draft. Many thanks to Allan Zhou, Sam Sokota, Minqi Jiang, Ellie Haber, Alex Robey, Swaminathan Gurumurthy, Kevin Li, Calvin Luo, Abitha Thankaraj, and Zico Kolter for their feedback and discussion on the draft. • A valid alternative possibility is that the RL optimization objective does not work well with smaller models, but this is likely not the case because before LLMs most successful applications of RL involved very small models. ︎
A valid alternative possibility is that the RL optimization objective does not work well with smaller models, but this is likely not the case because before LLMs most successful applications of RL involved very small models. ︎

A valid alternative possibility is that the RL optimization objective does not work well with smaller models, but this is likely not the case because before LLMs most successful applications of RL involved very small models. • This doesn’t mean the model can full exploit this information because the model can be limited in its computation power. It just means that if it wants to, that information is fully available. ︎
This doesn’t mean the model can full exploit this information because the model can be limited in its computation power. It just means that if it wants to, that information is fully available. ︎

This doesn’t mean the model can full exploit this information because the model can be limited in its computation power. It just means that if it wants to, that information is fully available. • For generalization to be tractable, we must assume that a “good enough” policy exists for all environments. This is analogous to assuming there is small or no label noise in supervised learning. ︎
For generalization to be tractable, we must assume that a “good enough” policy exists for all environments. This is analogous to assuming there is small or no label noise in supervised learning. ︎

For generalization to be tractable, we must assume that a “good enough” policy exists for all environments. This is analogous to assuming there is small or no label noise in supervised learning. • At the time of writing, I believe this sets a new state-of-the-art performance on the “25M easy” benchmark of ProcGen. ︎
At the time of writing, I believe this sets a new state-of-the-art performance on the “25M easy” benchmark of ProcGen. ︎

At the time of writing, I believe this sets a new state-of-the-art performance on the “25M easy” benchmark of ProcGen. • Interestingly, for many problems such as Atari, random sampling works reasonably well. I think that says much more about the environments than the exploration method itself. ︎
Interestingly, for many problems such as Atari, random sampling works reasonably well. I think that says much more about the environments than the exploration method itself. ︎

Interestingly, for many problems such as Atari, random sampling works reasonably well. I think that says much more about the environments than the exploration method itself. • There is a wide family of RL algorithms under the names of posterior sampling or information-directed sampling that try to direct the exploration to reduce the model’s uncertainty, but they are generally too expensive to be done exactly at the scale of LLMs. Various approximations exist but they have not been widely used for LLMs to the best of my knowledge. ︎
There is a wide family of RL algorithms under the names of posterior sampling or information-directed sampling that try to direct the exploration to reduce the model’s uncertainty, but they are generally too expensive to be done exactly at the scale of LLMs. Various approximations exist but they have not been widely used for LLMs to the best of my knowledge. ︎

There is a wide family of RL algorithms under the names of posterior sampling information-directed sampling that try to direct the exploration to reduce the model’s uncertainty, but they are generally too expensive to be done exactly at the scale of LLMs. Various approximations exist but they have not been widely used for LLMs to the best of my knowledge.","## The Era of Exploration

- **Large Language Models (LLMs)**
  - Unintended byproduct of three decades of freely accessible human text online.
  - Ilya Sutskever compares this information reservoir to **fossil fuel**: abundant but finite.
  - Studies suggest:
    - At current token-consumption rates, frontier labs could exhaust high-quality English web text before the decade ends.
    - Today's models consume data faster than humans can produce it.

- **Era of Experience**
  - Coined by David Silver and Richard Sutton.
  - Meaningful progress will depend on data generated by learning agents themselves.
  - The bottleneck is not just having experience but collecting the **right kind of experience** that benefits learning.
  - Future AI progress will focus on:
    - **Exploration**: acquiring new and informative experiences.

- **Cost of Experience Collection**
  - Scaling is fundamentally a question of resources:
    - Compute cycles
    - Synthetic-data generation
    - Data curation pipelines
    - Human oversight
    - Any expenditure that creates learning signals.
  - Introduced a bookkeeping unit called **flops**:
    - Represents one floating-point operation.
    - Used as a common currency for measuring effort consumed by systems.
    - Discussion focuses on relative spend, not specific resources.

- **Exploration in Data-Driven Systems**
  - Exploration is often associated with **Reinforcement Learning (RL)** but is broader:
    - Every data-driven system must decide which experiences to collect before learning.
  - Inspired by Minqi’s article: **General intelligence requires rethinking exploration**.

- **Post Organization**
  - The following sections will cover:
    1. How pre-training inadvertently solved part of the exploration problem.
    2. Why better exploration translates into better generalization.
    3. Where to allocate the next hundred thousand GPU-years.

- **Pretraining as Exploration**
  - Standard LLM pipeline:
    1. Pretrain a large model on next-token prediction using extensive text.
    2. Fine-tune the model with RL for desired objectives.
  - Without large-scale pretraining, RL struggles to progress.
  - Observations:
    - Smaller models show improved reasoning when distilled using chain-of-thought from larger models.
    - Some interpret this as evidence that large scale isn't necessary for effective reasoning, but this view is misguided.
  - Key question:
    - If model capacity isn't the bottleneck, why do smaller models need to distill from larger ones?
  - Explanation:
    - The cost of pretraining is an **upfront exploration tax**.
    - Models without pretraining or smaller pretrained models find it harder to explore the solution space.
    - Pretraining pays this tax by investing compute in diverse data to learn a rich sampling distribution.
    - Distillation allows smaller models to inherit this exploration capability from larger models.

## The Era of Exploration

### Importance of Pre-Paid Exploration
- **Pretrained models** vs. smaller models:
  - Smaller models struggle to explore the solution space effectively.
  - Pretraining involves significant compute resources to learn a rich sampling distribution.
- **Distillation**:
  - Allows smaller models to inherit exploration capabilities from larger models.
  - Bootstraps exploration from the investment made in larger models.

### Reinforcement Learning (RL) Loop
- General form of the RL loop:
  - **Exploration**: Agent generates randomized exploration trajectories.
  - **Reinforce**: Good trajectories are up-weighted; bad ones are down-weighted.
- **Coverage in RL**:
  - Essential for the agent to generate a minimal number of ""good"" trajectories during exploration.
  - In LLMs, exploration is achieved through sampling from the model’s autoregressive output distribution.
  - Correct solutions must be likely in the naive sampling distribution.
  - Lower-capacity models may struggle to find valid solutions through random sampling.

### Challenges of Exploration
- Exploration without prior information is difficult:
  - Even in simple tabular RL, extensive trials are needed.
  - Sample complexity lower-bound: \(\Omega(\frac{SAH^2}{\epsilon^2})\) (Dann & Brunskill, 2015)
    - **Variables**:
      - \(S\): Size of the state space
      - \(A\): Size of the action space
      - \(H\): Horizon
      - \(\epsilon\): Distance to the best possible solution
  - Minimum episodes grow linearly with state-action pairs and quadratically with the horizon.
- For LLMs:
  - State space includes every possible text prefix.
  - Action space is any next token, both very large.
  - Without prior information, RL becomes practically impossible.

### Role of Pretraining
- Pretraining has done the hard work of exploration:
  - Learns a better prior for sampling trajectories.
  - Types of trajectories sampled are constrained by the prior.
- Need to move beyond the prior for further progress.

### Exploration and Generalization
- Historical focus in RL:
  - Solving a single environment (e.g., Atari, MuJoCo).
  - Equivalent to training and testing on the same datapoint.
- Importance of generalization:
  - Success in unseen or unanticipated problems is crucial.
  - Generalization performance is critical for language models (LLMs).
- LLM training vs. deployment:
  - Trained on a finite set of prompts.
  - Must handle arbitrary user queries at deployment.
- Current LLMs excel in tasks with verifiable rewards (e.g., coding puzzles, formal proofs).
- Challenges in fuzzier domains (e.g., generating research reports, writing novels):
  - Feedback is sparse or ambiguous.
  - Large-scale training and data collection are difficult.

### Options for Training Generalizable Models
- Data diversity drives robust generalization:
  - Exploration directly controls data diversity.
- In supervised learning:
  - A labeled example reveals all details in a single forward pass.
  - Increasing data diversity requires collecting more data.
- In RL:
  - Each interaction exposes a narrow slice of the environment.
  - Agents must gather varied trajectories to build a representative picture.
  - Lack of diversity in collected trajectories can lead to overfitting.

## The Era of Exploration

### Supervised Learning vs. Reinforcement Learning (RL)
- **Supervised Learning**:
  - Labeled examples reveal all details in a single forward pass.
  - Data diversity can only be increased by collecting more data.

- **Reinforcement Learning (RL)**:
  - Each interaction exposes a narrow slice of the environment.
  - Agents must gather varied trajectories to build a representative picture.
  - Lack of diversity in trajectories (e.g., naive random sampling) can lead to overfitting.

### Challenges in Multiple Environments
- **Procgen Benchmark**:
  - A collection of Atari-like games with procedurally generated environments.
  - Each game theoretically contains “infinitely” many environments.
  - Objective: Train on a fixed number of environments and generalize to unseen ones.

### Current Approaches and Limitations
- Many existing methods treat the problem as a **representation learning** issue:
  - Apply regularization techniques from supervised learning (e.g., dropout, data augmentation).
  - These methods overlook **exploration**, a crucial component of RL.

- **Exploration Strategies**:
  - Agents can improve generalization by changing exploration methods.
  - Previous work showed that pairing RL algorithms with stronger exploration strategies can:
    - Double generalization performance on Procgen without explicit regularization.
    - Allow models to leverage more expressive architectures and resources.

### Comparison with LLMs
- While Procgen is less complex than problems faced by **Large Language Models (LLMs)**:
  - The problem structure is similar: trained on finite problems, tested on new ones without further training.
  - Current exploration in LLMs is simple, often limited to sampling from autoregressive distributions.
  - There is significant potential for better exploration approaches.

- **Challenges in Exploration**:
  - Few successful examples of improved exploration strategies.
  - Possible reasons for limited success:
    - Difficulty of the problem.
    - Inefficiency in terms of computational resources.
    - Lack of rigorous attempts to explore this area.

- If gains from Procgen-style exploration translate to LLMs:
  - We may be missing out on efficiency and new capabilities.

### Two Axes of Scaling Exploration
- **Exploration** involves deciding what data the learner will see, occurring on two axes:
  1. **World Sampling**:
     - Deciding where to learn (specific problems to solve).
     - In supervised learning, this includes data collection, synthetic generation, and curation.
     - In RL, it involves designing or generating environments (e.g., math puzzles, coding problems).
     - World sampling limits the information any agent can learn.
  2. **Path Sampling**:
     - Deciding how to gather data within a chosen world (unique to RL).
     - Agents choose trajectories to collect (e.g., random walks, curiosity-driven policies, tree search).
     - Different path-sampling strategies can vary in computational cost and training distributions.

- In supervised learning or unsupervised pretraining:
  - The second axis incurs a constant cost due to access to all information in a data point.
  - Exploration cost primarily resides on the first axis (world sampling).
  - Computational resources can be allocated to acquiring new worlds or processing existing ones.

## The Era of Exploration

### Key Concepts
- **Supervised Learning** and **Unsupervised Pretraining**:
  - Constant cost on the second axis due to access to all information in data points (e.g., cross-entropy loss).
  - Exploration cost primarily on the first axis – **world sampling**.

- **Reinforcement Learning (RL)**:
  - Greater flexibility in both axes (world sampling and path sampling).
  - Random trajectories often reveal little about ideal behavior, leading to lower information density (useful bits per flop).
  - Naïve trajectory sampling risks wasting flops on noise.

### Spending Flops Wisely
- Options for exploring within each world:
  - Sample more trajectories from a single environment.
  - Spend flops on strategizing the next trajectory to discover high-value states and actions.

### Maximizing Information per Flop
- High-level goal in machine learning:
  - Maximize information per flop using a trade-off curve between world sampling and path sampling.
  - Risks:
    - Too much focus on world sampling may lead to meaningless experiences.
    - Overfitting to a small set of worlds may hinder generalizable behavior.
  - Ideal scenario:
    - Balanced resource allocation between sampling new worlds and extracting information from existing ones.

### Scaling Laws and Performance Curves
- Similarity to **Chinchilla Scaling Laws**:
  - Two axes correspond to compute used for different types of sampling rather than parameters and data.
  - Isoperformance curve:
    - X-axis: Compute for interacting with environments.
    - Y-axis: Compute for generating or running environments (e.g., generative verifier with CoT).

### Path Sampling vs. World Sampling
- **Path Sampling**:
  - Well-defined problem with a clear objective: reduce model uncertainty.
  - Existing approaches have strong sample complexity but can be expensive.

- **World Sampling**:
  - Less clear objective; open-ended learning requires defining the universe of environments or subjective judgment of interesting outcomes.
  - Infinite space of environments vs. finite resources necessitates expressing preferences over environments.

### Designing Environment Specs
- Challenges in optimizing world sampling objectives:
  - Designing environments may resemble selecting pretraining data.
  - Difficulty in determining why one environment aids another.
  - Likely scenario:
    - Designing specs within individual expertise or domain.
    - Learning common principles from sufficient “human-approved” and “useful” specs.

### Conclusion
- Preliminary evidence suggests that fewer environments may suffice for achieving generality in decision-making, contrasting with the need for extensive pretraining data.

## The Era of Exploration

- **Objective**: Train an agent for general exploration and decision-making in out-of-distribution environments.

- **Design Process Acceleration**: 
  - Utilizing existing **Large Language Models (LLMs)** can significantly speed up the design process.
  - Anticipated trend: Individuals will design specifications within their own expertise or domain.

- **Learning from Specifications**: 
  - Accumulating enough ""human-approved"" and ""useful"" specifications may allow for the identification of common principles.
  - Potential to automate the process, similar to current pretraining data selection.

- **Generalization Concerns**: 
  - It would be inconvenient if the same number of environments as pretraining data is needed for decision-making generality.
  - Preliminary evidence suggests that a small number of environments can suffice for training an agent in out-of-distribution scenarios.

- **Scaling Challenges**: 
  - Scaling exploration and decision-making is less straightforward than scaling pretraining.
  - Reliable methods for world sampling and intelligent path sampling are needed.
  - Expected outcome: Isoperformance curves that bend inwards towards the origin, indicating efficient resource allocation between environments and agents.

## Final Thoughts

- **Exploration Focus**: 
  - Exploration (world sampling and path sampling) is a promising direction for future research.
  - Current scaling paradigms are effective but may eventually reach saturation.
  - The challenge lies in determining where to allocate additional computational resources.

- **Future Considerations**: 
  - Unknowns include the right scaling laws, environment generators, and exploration objectives.
  - The next few years will reveal if exploration can enhance computational efficiency beyond existing paradigms.

## Acknowledgements

- Special thanks to: 
  - Allan Zhou 
  - Sam Sokota 
  - Minqi Jiang 
  - Ellie Haber 
  - Alex Robey 
  - Swaminathan Gurumurthy 
  - Kevin Li 
  - Calvin Luo 
  - Abitha Thankaraj 
  - Zico Kolter 
  - For their feedback and discussions on the draft.

## Additional Insights

- **RL Optimization Objective**: 
  - A valid alternative possibility is that the RL optimization objective may not perform well with smaller models, but this is likely not the case as previous successful RL applications involved small models.

- **Information Exploitation**: 
  - Models may not fully exploit available information due to computational limitations, but the information remains accessible if desired.

- **Generalization Assumption**: 
  - For generalization to be feasible, it is assumed that a ""good enough"" policy exists for all environments, similar to the assumption of minimal label noise in supervised learning.

- **Performance Benchmark**: 
  - At the time of writing, a new state-of-the-art performance was achieved on the ""25M easy"" benchmark of ProcGen.

- **Random Sampling Effectiveness**: 
  - Random sampling works reasonably well for many problems, such as Atari, indicating more about the environments than the exploration method itself.

- **Exploration Algorithms**: 
  - A variety of RL algorithms, such as posterior sampling or information-directed sampling, aim to reduce model uncertainty during exploration.
  - These methods are generally too costly to implement at the scale of LLMs, and existing approximations have not been widely adopted.",0.34877470355731227
