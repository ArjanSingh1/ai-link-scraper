url,title,category,word_count,scraped_at,domain,slack_user,slack_user_id,slack_timestamp,slack_channel,content_type,brief_description,article_summary,full_content,formatted_content,completeness_ratio
https://www.linkedin.com/company/mercor-ai/,Mercor | LinkedIn,Technology,5419,2025-07-08T16:21:55.729312,www.linkedin.com,Vir Kashyap,U0910396AQ3,2025-07-01T15:37:38.762619,,website,"Mercor utilizes AI to match talent with optimal job opportunities, enhancing the recruitment process.",,"Mercor

Mercor

Software Development

Software Development

San Francisco, California 305,431 followers

San Francisco, California 305,431 followers

We use AI to understand human ability and match talent with the opportunities they're best suited for. We use AI to understand human ability and match talent with the opportunities they're best suited for. See jobs Follow • Discover all 633 employees
Discover all 633 employees

Discover all 633 employees • Report this company
Report this company

About us

About us We use AI to understand human ability and match talent with the opportunities they're best suited for. We use AI to understand human ability and match talent with the opportunities they're best suited for. Website mercor.com/?utm_source=linkedin&utm_medium=bio External link for Mercor

External link for Mercor Industry Software Development Company size 51-200 employees Headquarters San Francisco, California Privately Held Founded

Locations

Locations • Primary

San Francisco, California 94105, US

Get directions
Primary San Francisco, California 94105, US

San Francisco, California 94105, US Get directions

Employees at Mercor

Employees at Mercor • Marcos de Almeida Fugulin

AI Business Consultant | Strategic Advisor | Sales & Marketing Operations

Marcos de Almeida Fugulin

Marcos de Almeida Fugulin

AI Business Consultant | Strategic Advisor | Sales & Marketing Operations

AI Business Consultant | Strategic Advisor | Sales & Marketing Operations • Sundeep Peechu

Sundeep Peechu

Sundeep Peechu • Kofi Anim-Appiah

Communications & Sensing Signal Processing Research

Kofi Anim-Appiah

Kofi Anim-Appiah

Communications & Sensing Signal Processing Research

Communications & Sensing Signal Processing Research • Niko Bonatsos

Obsessed with technical founders building products for themselves

Niko Bonatsos

Niko Bonatsos

Obsessed with technical founders building products for themselves

Obsessed with technical founders building products for themselves See all employees

Updates

Updates • Mercor

305,431 followers

1w

Report this post

“The whole process happened in a day—I heard back within a week.”

From Northwestern to Amazon to building the future of AI, David’s journey shows what’s possible when hiring is based on merit and expertise. Through Mercor, he went from application to offer in under a week. If you’re ready to skip the slow hiring loops and find work that matches your skills, apply at work.mercor.com

…more

36

7 Comments

Like

Comment

Share
Mercor 305,431 followers

305,431 followers • Report this post
Report this post “The whole process happened in a day—I heard back within a week.”

From Northwestern to Amazon to building the future of AI, David’s journey shows what’s possible when hiring is based on merit and expertise. Through Mercor, he went from application to offer in under a week. If you’re ready to skip the slow hiring loops and find work that matches your skills, apply at work.mercor.com

“The whole process happened in a day—I heard back within a week.”

From Northwestern to Amazon to building the future of AI, David’s journey shows what’s possible when hiring is based on merit and expertise. Through Mercor, he went from application to offer in under a week. If you’re ready to skip the slow hiring loops and find work that matches your skills, apply at work.mercor.com ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 7 Comments Comment • Mercor

305,431 followers

1w

Edited

Report this post

After 25 years in finance and crypto, Kushal found a way to apply his domain expertise as a Software Engineering expert to improve the next generation of AI models - an opportunity he matched with through Mercor. He appreciated Mercor’s meritocratic AI interview, which focused solely on his skills and experience. “Mercor didn’t care about what I look like or my age, only what I could build.”

The world’s leading AI labs need academic and industry experts to provide critical human judgment to shape how AI models learn and evolve. Find an opportunity that values your expertise at work.mercor.com.

…more

52

1 Comment

Like

Comment

Share
Mercor 305,431 followers

305,431 followers Edited • Report this post
Report this post After 25 years in finance and crypto, Kushal found a way to apply his domain expertise as a Software Engineering expert to improve the next generation of AI models - an opportunity he matched with through Mercor. He appreciated Mercor’s meritocratic AI interview, which focused solely on his skills and experience. “Mercor didn’t care about what I look like or my age, only what I could build.”

The world’s leading AI labs need academic and industry experts to provide critical human judgment to shape how AI models learn and evolve. Find an opportunity that values your expertise at work.mercor.com. After 25 years in finance and crypto, Kushal found a way to apply his domain expertise as a Software Engineering expert to improve the next generation of AI models - an opportunity he matched with through Mercor. He appreciated Mercor’s meritocratic AI interview, which focused solely on his skills and experience. “Mercor didn’t care about what I look like or my age, only what I could build.”

The world’s leading AI labs need academic and industry experts to provide critical human judgment to shape how AI models learn and evolve. Find an opportunity that values your expertise at work.mercor.com ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 1 Comment Comment • Mercor

305,431 followers

2w

Edited

Report this post

At Mercor, we value academic expertise and help PhDs monetize their domain knowledge by contributing to frontier AI. Cassidy, who holds a PhD in linguistics and spent 8 years at a national lab, finally found a role that recognizes her expertise. In the past, interviewers lacked the domain knowledge to understand the depth of her skills. Through Mercor, she had the opportunity to showcase her linguistics expertise and demonstrate the value she brings. Today, she works on a project for a leading AI lab as a Linguistic Intelligence Analyst. Hear her story and find your next role at work.mercor.com.

…more

67

4 Comments

Like

Comment

Share
Mercor 305,431 followers

305,431 followers Edited • Report this post
Report this post At Mercor, we value academic expertise and help PhDs monetize their domain knowledge by contributing to frontier AI. Cassidy, who holds a PhD in linguistics and spent 8 years at a national lab, finally found a role that recognizes her expertise. In the past, interviewers lacked the domain knowledge to understand the depth of her skills. Through Mercor, she had the opportunity to showcase her linguistics expertise and demonstrate the value she brings. Today, she works on a project for a leading AI lab as a Linguistic Intelligence Analyst. Hear her story and find your next role at work.mercor.com. At Mercor, we value academic expertise and help PhDs monetize their domain knowledge by contributing to frontier AI. Cassidy, who holds a PhD in linguistics and spent 8 years at a national lab, finally found a role that recognizes her expertise. In the past, interviewers lacked the domain knowledge to understand the depth of her skills. Through Mercor, she had the opportunity to showcase her linguistics expertise and demonstrate the value she brings. Today, she works on a project for a leading AI lab as a Linguistic Intelligence Analyst. Hear her story and find your next role at work.mercor.com ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 4 Comments Comment • Mercor

305,431 followers

3w

Edited

Report this post

Meet Ersin Akinici, a software engineer with over 10 years of experience now using his expertise to shape the future of AI through critical human insight. From the start, Ersin was impressed by our interview process. The interview began with broad questions, then dove deep into his technical skills and background. Ersin particularly enjoyed receiving transparent feedback about his interview and areas for growth. After applying, Ersin quickly became part of an exciting team of reviewers who utilize their knowledge to test, challenge, and provide critical human judgment to improve AI models. “There’s really this sense of, wow, I’m building something that is going to impact generations in the future,” Ersin said. Whether you’re a software engineer, PhD, consultant, or doctor, you can earn up to $200/hr while working on your own terms through Mercor. Apply today at work.mercor.com.

…more

53

1 Comment

Like

Comment

Share
Mercor 305,431 followers

305,431 followers Edited • Report this post
Report this post Meet Ersin Akinici, a software engineer with over 10 years of experience now using his expertise to shape the future of AI through critical human insight. From the start, Ersin was impressed by our interview process. The interview began with broad questions, then dove deep into his technical skills and background. Ersin particularly enjoyed receiving transparent feedback about his interview and areas for growth. After applying, Ersin quickly became part of an exciting team of reviewers who utilize their knowledge to test, challenge, and provide critical human judgment to improve AI models. “There’s really this sense of, wow, I’m building something that is going to impact generations in the future,” Ersin said. Whether you’re a software engineer, PhD, consultant, or doctor, you can earn up to $200/hr while working on your own terms through Mercor. Apply today at work.mercor.com. Meet Ersin Akinici, a software engineer with over 10 years of experience now using his expertise to shape the future of AI through critical human insight. From the start, Ersin was impressed by our interview process. The interview began with broad questions, then dove deep into his technical skills and background. Ersin particularly enjoyed receiving transparent feedback about his interview and areas for growth. After applying, Ersin quickly became part of an exciting team of reviewers who utilize their knowledge to test, challenge, and provide critical human judgment to improve AI models. “There’s really this sense of, wow, I’m building something that is going to impact generations in the future,” Ersin said. Whether you’re a software engineer, PhD, consultant, or doctor, you can earn up to $200/hr while working on your own terms through Mercor. Apply today at work.mercor.com ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 1 Comment Comment • Mercor reposted this

Brendan Foody

CEO @ Mercor | Thiel Fellow

3w

Report this post

Over 60% of hires on the Mercor platform come from referrals. This is one of the highest of any marketplace in the world. People love Mercor

287

22 Comments

Like

Comment

Share
Mercor reposted this

Mercor reposted this Brendan Foody CEO @ Mercor | Thiel Fellow

CEO @ Mercor | Thiel Fellow • Report this post
Report this post Over 60% of hires on the Mercor platform come from referrals. This is one of the highest of any marketplace in the world. People love Mercor

Over 60% of hires on the Mercor platform come from referrals. This is one of the highest of any marketplace in the world. People love Mercor ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 22 Comments Comment • Mercor reposted this

Mercor

305,431 followers

3w

Report this post

Meet the engineers shaping the future of AI. We recently brought together three software engineers to share their firsthand experiences finding work through Mercor, where we connect experts with the world’s most innovative AI labs. From software engineers to PhDs, we offer a platform for skilled professionals to monetize their expertise by testing, challenging, and advancing frontier AI models. Our AI-powered interview process is fast, transparent, and designed with experts in mind. Work remotely, on your terms, and earn up to $200/hour. Ready to match with your next opportunity? Apply today at work.mercor.com

…more

91

Like

Comment

Share
Mercor reposted this

Mercor reposted this Mercor 305,431 followers

305,431 followers • Report this post
Report this post Meet the engineers shaping the future of AI. We recently brought together three software engineers to share their firsthand experiences finding work through Mercor, where we connect experts with the world’s most innovative AI labs. From software engineers to PhDs, we offer a platform for skilled professionals to monetize their expertise by testing, challenging, and advancing frontier AI models. Our AI-powered interview process is fast, transparent, and designed with experts in mind. Work remotely, on your terms, and earn up to $200/hour. Ready to match with your next opportunity? Apply today at work.mercor.com

Meet the engineers shaping the future of AI. We recently brought together three software engineers to share their firsthand experiences finding work through Mercor, where we connect experts with the world’s most innovative AI labs. From software engineers to PhDs, we offer a platform for skilled professionals to monetize their expertise by testing, challenging, and advancing frontier AI models. Our AI-powered interview process is fast, transparent, and designed with experts in mind. Work remotely, on your terms, and earn up to $200/hour. Ready to match with your next opportunity? Apply today at work.mercor.com ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" Comment • Mercor

305,431 followers

3w

Report this post

Meet the engineers shaping the future of AI. We recently brought together three software engineers to share their firsthand experiences finding work through Mercor, where we connect experts with the world’s most innovative AI labs. From software engineers to PhDs, we offer a platform for skilled professionals to monetize their expertise by testing, challenging, and advancing frontier AI models. Our AI-powered interview process is fast, transparent, and designed with experts in mind. Work remotely, on your terms, and earn up to $200/hour. Ready to match with your next opportunity? Apply today at work.mercor.com

…more

91

Like

Comment

Share
Mercor 305,431 followers

305,431 followers • Report this post
Report this post Meet the engineers shaping the future of AI. We recently brought together three software engineers to share their firsthand experiences finding work through Mercor, where we connect experts with the world’s most innovative AI labs. From software engineers to PhDs, we offer a platform for skilled professionals to monetize their expertise by testing, challenging, and advancing frontier AI models. Our AI-powered interview process is fast, transparent, and designed with experts in mind. Work remotely, on your terms, and earn up to $200/hour. Ready to match with your next opportunity? Apply today at work.mercor.com

Meet the engineers shaping the future of AI. We recently brought together three software engineers to share their firsthand experiences finding work through Mercor, where we connect experts with the world’s most innovative AI labs. From software engineers to PhDs, we offer a platform for skilled professionals to monetize their expertise by testing, challenging, and advancing frontier AI models. Our AI-powered interview process is fast, transparent, and designed with experts in mind. Work remotely, on your terms, and earn up to $200/hour. Ready to match with your next opportunity? Apply today at work.mercor.com ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" Comment • Mercor reposted this

Jacob Effron

Managing Director at Redpoint Ventures

1mo

Report this post

This week on Unsupervised Learning, Patrick Chase and I sat down with Brendan Foody, the co-founder and CEO of Mercor, a company building AI-native labor markets. They’re operating at the intersection of AI x recruiting, data labeling, evals and more and recently raised $100M at a $2B valuation. What you’ll take away:

- Why most humans will soon be doing evals as their job
- How AI is already outperforming humans in hiring assessments
- Why legacy data labeling pipelines are breaking down
- How Mercor uses AI to make its own hiring more predictive
- How a meeting with xAI revealed a massive market shift for Mercor

Listen here👇

YouTube: https://lnkd.in/gFBfd_6f 
Spotify: https://bit.ly/4jSyG2R
Apple: https://bit.ly/4mHAiz3

Some other key take-aways:

📈 Most Knowledge Work Will Become Evaluation Work

Brendan believes a massive shift is underway: instead of doing tasks, many knowledge workers will soon focus on creating evaluations that teach models how to do those tasks. As foundation models automate more work, it becomes economically and structurally more efficient for humans to build evals than to execute the work themselves. Brendan sees this as not just a niche market, but one of the core directions the labor economy is heading toward.

🧠 AI Already Outperforms Humans at Candidate Assessment

Mercor has found that models are now better than most humans at interpreting resumes, assessing interviews, and predicting job performance. They’ve even replaced parts of their internal hiring funnel with AI interviewers—and seen conversion rates improve. The most valuable part of human input in hiring is now shifting toward selling the opportunity, not evaluating the candidate.

️ Crowdsourced Labeling Pipelines Are Breaking Down

As model quality improves, the old approach of using lightly trained gig workers to produce training data no longer works. Today’s models require high-context, edge-case data—created in tight collaboration with researchers. Brendan explains how Mercor capitalized on this shift by focusing on rapidly sourcing highly skilled contributors, not just volume.

157

7 Comments

Like

Comment

Share
Mercor reposted this

Mercor reposted this Jacob Effron Managing Director at Redpoint Ventures

Managing Director at Redpoint Ventures • Report this post
Report this post This week on Unsupervised Learning, Patrick Chase and I sat down with Brendan Foody, the co-founder and CEO of Mercor, a company building AI-native labor markets. They’re operating at the intersection of AI x recruiting, data labeling, evals and more and recently raised $100M at a $2B valuation. What you’ll take away:

- Why most humans will soon be doing evals as their job
- How AI is already outperforming humans in hiring assessments
- Why legacy data labeling pipelines are breaking down
- How Mercor uses AI to make its own hiring more predictive
- How a meeting with xAI revealed a massive market shift for Mercor

Listen here👇

YouTube: https://lnkd.in/gFBfd_6f 
Spotify: https://bit.ly/4jSyG2R
Apple: https://bit.ly/4mHAiz3

Some other key take-aways:

📈 Most Knowledge Work Will Become Evaluation Work

Brendan believes a massive shift is underway: instead of doing tasks, many knowledge workers will soon focus on creating evaluations that teach models how to do those tasks. As foundation models automate more work, it becomes economically and structurally more efficient for humans to build evals than to execute the work themselves. Brendan sees this as not just a niche market, but one of the core directions the labor economy is heading toward.

🧠 AI Already Outperforms Humans at Candidate Assessment

Mercor has found that models are now better than most humans at interpreting resumes, assessing interviews, and predicting job performance. They’ve even replaced parts of their internal hiring funnel with AI interviewers—and seen conversion rates improve. The most valuable part of human input in hiring is now shifting toward selling the opportunity, not evaluating the candidate.

️ Crowdsourced Labeling Pipelines Are Breaking Down

As model quality improves, the old approach of using lightly trained gig workers to produce training data no longer works. Today’s models require high-context, edge-case data—created in tight collaboration with researchers. Brendan explains how Mercor capitalized on this shift by focusing on rapidly sourcing highly skilled contributors, not just volume. This week on Unsupervised Learning, Patrick Chase and I sat down with Brendan Foody, the co-founder and CEO of Mercor, a company building AI-native labor markets. They’re operating at the intersection of AI x recruiting, data labeling, evals and more and recently raised $100M at a $2B valuation. What you’ll take away:

- Why most humans will soon be doing evals as their job
- How AI is already outperforming humans in hiring assessments
- Why legacy data labeling pipelines are breaking down
- How Mercor uses AI to make its own hiring more predictive
- How a meeting with xAI revealed a massive market shift for Mercor

Listen here👇

YouTube: https://lnkd.in/gFBfd_6f Spotify: https://bit.ly/4jSyG2R Apple: https://bit.ly/4mHAiz3 Some other key take-aways:

📈 Most Knowledge Work Will Become Evaluation Work

Brendan believes a massive shift is underway: instead of doing tasks, many knowledge workers will soon focus on creating evaluations that teach models how to do those tasks. As foundation models automate more work, it becomes economically and structurally more efficient for humans to build evals than to execute the work themselves. Brendan sees this as not just a niche market, but one of the core directions the labor economy is heading toward.

🧠 AI Already Outperforms Humans at Candidate Assessment

Mercor has found that models are now better than most humans at interpreting resumes, assessing interviews, and predicting job performance. They’ve even replaced parts of their internal hiring funnel with AI interviewers—and seen conversion rates improve. The most valuable part of human input in hiring is now shifting toward selling the opportunity, not evaluating the candidate.

️ Crowdsourced Labeling Pipelines Are Breaking Down

As model quality improves, the old approach of using lightly trained gig workers to produce training data no longer works. Today’s models require high-context, edge-case data—created in tight collaboration with researchers. Brendan explains how Mercor capitalized on this shift by focusing on rapidly sourcing highly skilled contributors, not just volume. ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 7 Comments Comment • Mercor

305,431 followers

1mo

Report this post

Applications for the Mercor Graduate Fellowship close at the end of the day this Thursday, May 15. Apply before the deadline—and refer your friends! Mercor works with thousands of PhDs and postdocs across fields—from math to philosophy. We see firsthand that academia is full of people whose talents are wildly under-funded and under-valued. We’d like to change that—beginning with this fellowship. MERITOCRACY

We want to identify exceptional talent regardless of academic pedigree, institutional affiliation, or personal connections. Traditional fellowship programs rely heavily on recommendation letters, publication records, and prestigious affiliations—metrics that frequently favor those with established networks and privileged backgrounds. All we want to evaluate is to understand (i) what problems you are trying to solve and (ii) how you want to solve them. This allows us to discover brilliant talent who might be overlooked by conventional selection processes—whether you're from a top-ranked university or a less-known institution, whether you have impressive publications or are still developing your research. The Mercor Graduate Fellowship is designed to support talented individuals who demonstrate exceptional promise but may lack resources or recognition. We want to increase access to opportunity and funding in academia. THE FELLOWSHIP

- $50,000, no strings attached.
- Open to current PhD students and postdocs in STEM fields.
- To apply, just upload a resume and take a 10-minute virtual interview with Mercor’s AI
- Apply by May 15th (but we review applications as they come in)

THE NETWORK

We’ll host a retreat for selected fellows to connect with other people building cool things and thinking deep thoughts. THE BONUS

Searching for talent is hard—refer someone who wins, and we’ll pay you $10,000. Brendan Foody

CEO @ Mercor | Thiel Fellow

2mo

We’re awarding a $50,000 fellowship to STEM PhDs. Comment your email and I’ll send you a link to interview. Every transformative idea starts with someone on their way—before the breakthroughs or the recognition. Mercor works with thousands of PhDs and postdocs across fields—from math to philosophy. We see firsthand that academia is full of people whose talents are wildly under-funded and under-valued. We’d like to change that, beginning with this fellowship. The fellowship
- $50,000, no strings attached.
- Open to current, incoming, and graduating PhD students and postdocs in STEM.
- To apply, just upload a resume and take a 10-minute virtual interview.
- Apply by May 15th (but we review applications as they come in). The network
We’ll host a retreat for selected fellows to connect with other people building cool things and thinking deep thoughts. This is optional, but who would want to miss it? The bonus
Searching for talent is hard. Refer someone who wins, and we’ll pay you $10,000.

30

9 Comments

Like

Comment

Share
Mercor 305,431 followers

305,431 followers • Report this post
Report this post Applications for the Mercor Graduate Fellowship close at the end of the day this Thursday, May 15. Apply before the deadline—and refer your friends! Mercor works with thousands of PhDs and postdocs across fields—from math to philosophy. We see firsthand that academia is full of people whose talents are wildly under-funded and under-valued. We’d like to change that—beginning with this fellowship. MERITOCRACY

We want to identify exceptional talent regardless of academic pedigree, institutional affiliation, or personal connections. Traditional fellowship programs rely heavily on recommendation letters, publication records, and prestigious affiliations—metrics that frequently favor those with established networks and privileged backgrounds. All we want to evaluate is to understand (i) what problems you are trying to solve and (ii) how you want to solve them. This allows us to discover brilliant talent who might be overlooked by conventional selection processes—whether you're from a top-ranked university or a less-known institution, whether you have impressive publications or are still developing your research. The Mercor Graduate Fellowship is designed to support talented individuals who demonstrate exceptional promise but may lack resources or recognition. We want to increase access to opportunity and funding in academia. THE FELLOWSHIP

- $50,000, no strings attached.
- Open to current PhD students and postdocs in STEM fields.
- To apply, just upload a resume and take a 10-minute virtual interview with Mercor’s AI
- Apply by May 15th (but we review applications as they come in)

THE NETWORK

We’ll host a retreat for selected fellows to connect with other people building cool things and thinking deep thoughts. THE BONUS

Searching for talent is hard—refer someone who wins, and we’ll pay you $10,000. Applications for the Mercor Graduate Fellowship close at the end of the day this Thursday, May 15. Apply before the deadline—and refer your friends! Mercor works with thousands of PhDs and postdocs across fields—from math to philosophy. We see firsthand that academia is full of people whose talents are wildly under-funded and under-valued. We’d like to change that—beginning with this fellowship. MERITOCRACY

We want to identify exceptional talent regardless of academic pedigree, institutional affiliation, or personal connections. Traditional fellowship programs rely heavily on recommendation letters, publication records, and prestigious affiliations—metrics that frequently favor those with established networks and privileged backgrounds. All we want to evaluate is to understand (i) what problems you are trying to solve and (ii) how you want to solve them. This allows us to discover brilliant talent who might be overlooked by conventional selection processes—whether you're from a top-ranked university or a less-known institution, whether you have impressive publications or are still developing your research. The Mercor Graduate Fellowship is designed to support talented individuals who demonstrate exceptional promise but may lack resources or recognition. We want to increase access to opportunity and funding in academia. THE FELLOWSHIP

- $50,000, no strings attached.
- Open to current PhD students and postdocs in STEM fields.
- To apply, just upload a resume and take a 10-minute virtual interview with Mercor’s AI
- Apply by May 15th (but we review applications as they come in)

THE NETWORK

We’ll host a retreat for selected fellows to connect with other people building cool things and thinking deep thoughts. THE BONUS

Searching for talent is hard—refer someone who wins, and we’ll pay you $10,000. Brendan Foody CEO @ Mercor | Thiel Fellow

CEO @ Mercor | Thiel Fellow We’re awarding a $50,000 fellowship to STEM PhDs. Comment your email and I’ll send you a link to interview. Every transformative idea starts with someone on their way—before the breakthroughs or the recognition. Mercor works with thousands of PhDs and postdocs across fields—from math to philosophy. We see firsthand that academia is full of people whose talents are wildly under-funded and under-valued. We’d like to change that, beginning with this fellowship. The fellowship
- $50,000, no strings attached.
- Open to current, incoming, and graduating PhD students and postdocs in STEM.
- To apply, just upload a resume and take a 10-minute virtual interview.
- Apply by May 15th (but we review applications as they come in). The network
We’ll host a retreat for selected fellows to connect with other people building cool things and thinking deep thoughts. This is optional, but who would want to miss it? The bonus
Searching for talent is hard. Refer someone who wins, and we’ll pay you $10,000. We’re awarding a $50,000 fellowship to STEM PhDs. Comment your email and I’ll send you a link to interview. Every transformative idea starts with someone on their way—before the breakthroughs or the recognition. Mercor works with thousands of PhDs and postdocs across fields—from math to philosophy. We see firsthand that academia is full of people whose talents are wildly under-funded and under-valued. We’d like to change that, beginning with this fellowship. The fellowship
- $50,000, no strings attached.
- Open to current, incoming, and graduating PhD students and postdocs in STEM.
- To apply, just upload a resume and take a 10-minute virtual interview.
- Apply by May 15th (but we review applications as they come in). The network
We’ll host a retreat for selected fellows to connect with other people building cool things and thinking deep thoughts. This is optional, but who would want to miss it? The bonus
Searching for talent is hard. Refer someone who wins, and we’ll pay you $10,000. ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 9 Comments Comment ""/organization-guest/api/feedUpdates/71301545?paginationToken=0-1752006088102-1beb6b9034af7eee1e91b6ca1b718fd1"" ""0-1752006088102-1beb6b9034af7eee1e91b6ca1b718fd1""

Join now to see what you are missing

Join now to see what you are missing • Find people you know at Mercor
Find people you know at Mercor • Browse recommended jobs for you
Browse recommended jobs for you • View all updates, news, and articles
View all updates, news, and articles Join now

Similar pages

Similar pages • Profile.fyi (Acquired by Mercor)

Software Development

Profile.fyi (Acquired by Mercor)

Profile.fyi (Acquired by Mercor) Software Development

Software Development • Mercor

Outsourcing and Offshoring Consulting

Berchem, Vlaanderen

Mercor

Mercor Outsourcing and Offshoring Consulting

Outsourcing and Offshoring Consulting Berchem, Vlaanderen

Berchem, Vlaanderen • Seros

Software Development

Washington, D. C. Seros

Software Development

Software Development Washington, D. C. Washington, D. C. • Thiel Capital

Investment Management

West Hollywood, California

Thiel Capital

Thiel Capital Investment Management

Investment Management West Hollywood, California

West Hollywood, California • Mercor Intelligence

Software Development

Mercor Intelligence

Mercor Intelligence Software Development

Software Development • Lovable

Software Development

Lovable

Lovable Software Development

Software Development • ElevenLabs

Research Services

ElevenLabs

ElevenLabs Research Services

Research Services • Stealth

Software Development

Los Angeles, CALIFORNIA

Stealth

Stealth Software Development

Software Development Los Angeles, CALIFORNIA

Los Angeles, CALIFORNIA • Anysphere

Software Development

Anysphere

Anysphere Software Development

Software Development • Scale AI

Software Development

San Francisco, California

Scale AI

Scale AI Software Development

Software Development San Francisco, California

San Francisco, California Show more similar pages Show fewer similar pages

Browse jobs

Browse jobs • Engineer jobs

555,845 open jobs

Engineer jobs

Engineer jobs 555,845 open jobs • Voice Over Artist jobs

1,765 open jobs

Voice Over Artist jobs

Voice Over Artist jobs 1,765 open jobs • Writer jobs

26,384 open jobs

Writer jobs

Writer jobs 26,384 open jobs • Senior Software Engineer jobs

78,145 open jobs

Senior Software Engineer jobs

Senior Software Engineer jobs 78,145 open jobs • Analyst jobs

694,057 open jobs

Analyst jobs

Analyst jobs 694,057 open jobs • Developer jobs

258,935 open jobs

Developer jobs

Developer jobs 258,935 open jobs • Software Engineer jobs

300,699 open jobs

Software Engineer jobs

Software Engineer jobs 300,699 open jobs • Intern jobs

71,196 open jobs

Intern jobs

Intern jobs 71,196 open jobs • Copywriter jobs

17,206 open jobs

Copywriter jobs

Copywriter jobs 17,206 open jobs • Scientist jobs

48,969 open jobs

Scientist jobs

Scientist jobs 48,969 open jobs • Associate jobs

1,091,945 open jobs

Associate jobs

Associate jobs 1,091,945 open jobs • Manager jobs

1,880,925 open jobs

Manager jobs

Manager jobs 1,880,925 open jobs • Staff Software Engineer jobs

64,945 open jobs

Staff Software Engineer jobs

Staff Software Engineer jobs 64,945 open jobs • Full Stack Engineer jobs

38,546 open jobs

Full Stack Engineer jobs

Full Stack Engineer jobs 38,546 open jobs • Machine Learning Engineer jobs

148,937 open jobs

Machine Learning Engineer jobs

Machine Learning Engineer jobs 148,937 open jobs • Specialist jobs

768,666 open jobs

Specialist jobs

Specialist jobs 768,666 open jobs • Project Manager jobs

253,048 open jobs

Project Manager jobs

Project Manager jobs 253,048 open jobs • Editor jobs

19,020 open jobs

Editor jobs

Editor jobs 19,020 open jobs • Designer jobs

65,273 open jobs

Designer jobs

Designer jobs 65,273 open jobs • Python Developer jobs

46,642 open jobs

Python Developer jobs

Python Developer jobs 46,642 open jobs Show more jobs like this Show fewer jobs like this

Funding

Funding Mercor 3 total rounds Last Round Series B Mar 20, 2025 External Crunchbase Link for last round of funding US$ 100.0M Investors Felicis Other investors See more info on crunchbase","**Website Description:**

Mercor is a software development company based in San Francisco, California, specializing in leveraging artificial intelligence to assess human capabilities and effectively match talent with suitable opportunities. With a focus on innovation, Mercor aims to enhance the recruitment process by providing insights that connect individuals with roles that align with their skills and potential.

The website offers detailed information about the company, including its services, employee profiles, and job opportunities. It serves as a platform for potential clients and job seekers to understand Mercor's mission and explore career options within the organization. The company is privately held and employs between 51 to 200 individuals, showcasing a diverse team of professionals dedicated to advancing AI in the talent acquisition space.",1.0
https://github.com/apple/ml-diffucoder,GitHub - apple/ml-diffucoder: DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation,Machine Learning,2901,2025-07-08T16:22:06.333981,github.com,Sai,U07TKPGTKDF,2025-07-02T09:58:33.027069,,website,A GitHub repository for the DiffuCoder project focused on improving masked diffusion models for code generation.,,"ml-diffucoder Public • Notifications
You must be signed in to change notification settings
Notifications You must be signed in to change notification settings • Fork
33
• Star

524
DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

License

License View license Branches Activity Notifications You must be signed in to change notification settings

apple/ml-diffucoder

apple/ml-diffucoder Branches Go to file Open more actions menu

Folders and files

Folders and files Last commit message Last commit date

Latest commit

Latest commit

History

History 10 Commits recipes recipes open_r1 open_r1.gitignore.gitignore ACKNOWLEDGEMENTS ACKNOWLEDGEMENTS CODE_OF_CONDUCT.md CODE_OF_CONDUCT.md CONTRIBUTING.md CONTRIBUTING.md LICENSE LICENSE README.md README.md inference_demo.py inference_demo.py run.sh run.sh setup.py setup.py View all files

Repository files navigation

Repository files navigation

Masked Diffusion Models for Code Generation

Masked Diffusion Models for Code Generation This software project accompanies the research paper, DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation. This software project accompanies the research paper, DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

Updates

Updates MLX support on Apple Silicon is in progress. We will make necessary updates to the repository once it is available. MLX support on Apple Silicon is in progress. We will make necessary updates to the repository once it is available. • June 8, 2025. We are still training for 14B models, to be updated. June 8, 2025. We are still training for 14B models, to be updated. • June 7, 2025. MLX community implementation of DiffuCoder 8bit model
June 7, 2025. MLX community implementation of DiffuCoder 8bit model • June 4, 2025. MLX support in progress. To preview or contribute, please check out this PR started by @Goekdeniz-Guelmez: this PR
June 4, 2025. support in progress. To preview or contribute, please check out this PR started by @Goekdeniz-Guelmez: this PR • June 4, 2025. Update inference usage/examples/demo. June 4, 2025. Update inference usage/examples/demo. • June 2, 2025. Models are available on Huggingface. June 2, 2025. Models are available on Huggingface. • June 1, 2025. Code is available. June 1, 2025. Code is available. Motivation

Motivation Scaling upon Masked Denoising Models (MDMs), diffusion LLMs (dLLMs) such as LLaDA and Dream have achieved performance on par with similarly sized autoregressive (AR) LLMs across many benchmarks. Recent commercial-scale dLLMs like Mercury and Gemini further demonstrate that diffusion-based code generators can rival top AR code models on programming tasks while offering faster text generation. Scaling upon Masked Denoising Models (MDMs), diffusion LLMs (dLLMs) such as have achieved performance on par with similarly sized autoregressive (AR) LLMs across many benchmarks. Recent commercial-scale dLLMs like Mercury Gemini further demonstrate that diffusion-based code generators can rival top AR code models on programming tasks while offering faster text generation. However, the generation pattern and post-training strategies of dLLMs remain under-explored. In this work, we investigate the following questions:

However, the generation pattern and post-training strategies of dLLMs remain under-explored. In this work, we investigate the following questions: • How does the generation pattern of dLLMs differ from AR models? How does the generation pattern of dLLMs differ from AR models? • What is the difference in modeling different data modalities, such as code vs. math? What is the difference in modeling different data modalities, such as code vs. math? • How diverse can dLLMs be, and how should post-training be designed? How diverse can dLLMs be, and how should post-training be designed? We train DiffuCoder using the adaptation approach in DiffuLLaMA and introduce a new metric — autoregressiveness score — to quantify the causal pattern during dLLM generation. The key findings are listed below. We train DiffuCoder using the adaptation approach in DiffuLLaMA and introduce a new metric — autoregressiveness score — to quantify the causal pattern during dLLM generation. The key findings are listed below. Findings

Findings • dLLMs still exhibit a left-to-right bias due to the nature of text, but they can also break this strict order in AR models.
dLLMs still exhibit a left-to-right bias due to the nature of text, but they can also break this strict order in AR models.

dLLMs still exhibit a left-to-right bias due to the nature of text, but they can also break this strict order in AR models. • After pre-training, we show that code tasks induce less global AR-ness than math. After pre-training, we show that code tasks induce less global AR-ness than math. After pre-training, we show that code tasks induce less global AR-ness than math. • In dLLMs, changing the sampling temperature not only affects sampled tokens (as in AR models), but also alters the generation order itself. In dLLMs, changing the sampling temperature not only affects sampled tokens (as in AR models), but also alters the generation order itself. In dLLMs, changing the sampling temperature not only affects sampled tokens (as in AR models), but also alters the generation order itself. For more interesting findings, please refer to our original paper! For more interesting findings, please refer to our original paper! We propose Coupled-GRPO, a post-training method to improve DiffuCoder's performance. We propose Coupled-GRPO, a post-training method to improve DiffuCoder's performance. Coupled-GRPO

Coupled-GRPO In diffusion LLMs, the per-timestep loss $\mathcal{L}_{t}$ typically computes log-probabilities only at masked token positions, which leads to inefficiency and high variance when sampling is limited. To address this, Coupled-GRPO introduces a coupled-sampling scheme:

In diffusion LLMs, the per-timestep loss $\mathcal{L}_{t}$ typically computes log-probabilities only at masked token positions, which leads to inefficiency and high variance when sampling is limited. To address this, Coupled-GRPO introduces a coupled-sampling scheme: • For each training example, we select $\lambda$ pairs of timesteps $(t, \hat{t})$ such that $t + \hat{t} = T$. For each training example, we select $\lambda$ pairs of timesteps $(t, \hat{t})$ such that $t + \hat{t} = T$ • We apply two complementary token masks — each mask hides part of the tokens, and together they cover the entire set of target tokens. We apply two complementary token masks — each mask hides part of the tokens, and together they cover the entire set of target tokens. • As a result, every token is unmasked in exactly one of the two forward passes. As a result, every token is unmasked in exactly one of the two forward passes. This ensures that:

This ensures that: • Every token's log-probability is computed at least once, providing a non-zero learning signal for all tokens. Every token's log-probability is computed at least once, providing a non-zero learning signal for all tokens. • The probability estimates are more accurate, since each token is evaluated in a realistic partially-masked context (rather than always being fully masked). The probability estimates are more accurate, since each token is evaluated in a realistic partially-masked context (rather than always being fully masked). • The scheme effectively uses $2\lambda$ times more sampling passes than the baseline (we choose $\lambda=1$), improving estimation with modest computational overhead. The scheme effectively uses $2\lambda$ times more sampling passes than the baseline (we choose $\lambda=1$ ), improving estimation with modest computational overhead. In this repository, we release our implementation of Coupled-GRPO, built upon open-r1. In this repository, we release our implementation of Coupled-GRPO, built upon open-r1

Getting Started

Getting Started
run.sh # start training
setup.py # modified open-r1/setup.py
src/open_r1/ # our code based on open-r1
configs.py # with diffusion related params
coupled_grpo.py # inherits trl GRPOTrainer
grpo.py # main training script
rewards.py # rewrite code reward and code_format reward
utils/code_providers.py # rewrite pass rate extraction for E2B
recipes/process_data.py # prepare grpo training data
recipes/config_coupled_code.yaml # training config
tests/test_code_reward.py # test sandbox execution for code

run.sh # start training
setup.py # modified open-r1/setup.py
src/open_r1/ # our code based on open-r1
configs.py # with diffusion related params
coupled_grpo.py # inherits trl GRPOTrainer
grpo.py # main training script
rewards.py # rewrite code reward and code_format reward
utils/code_providers.py # rewrite pass rate extraction for E2B
recipes/process_data.py # prepare grpo training data
recipes/config_coupled_code.yaml # training config
tests/test_code_reward.py # test sandbox execution for code

run.sh # start training
setup.py # modified open-r1/setup.py
src/open_r1/ # our code based on open-r1
configs.py # with diffusion related params
coupled_grpo.py # inherits trl GRPOTrainer
grpo.py # main training script
rewards.py # rewrite code reward and code_format reward
utils/code_providers.py # rewrite pass rate extraction for E2B
recipes/process_data.py # prepare grpo training data
recipes/config_coupled_code.yaml # training config
tests/test_code_reward.py # test sandbox execution for code

1. Prepare code and environment

1. Prepare code and environment Clone the source code of Open-R1 from git clone https://github.com/huggingface/open-r1. Merge and replace files between ours and Open-R1's (including setup.py). Clone the source code of Open-R1 from
git clone https://github.com/huggingface/open-r1

git clone https://github.com/huggingface/open-r1. Merge and replace files between ours and Open-R1's (including
setup.py

setup.py Set up the environment and dependencies following Open-R1:

Set up the environment and dependencies following Open-R1:
env=openr1
conda create -n $env python=3.11 -y -c anaconda
conda activate $env

pip install vllm==0.8.4
pip install setuptools
pip install flash-attn==2.8.0.post1 --no-build-isolation
pip install -e "".[code]""

env=openr1
conda create -n python=3.11 -y -c anaconda
conda activate pip install vllm==0.8.4
pip install setuptools
pip install flash-attn==2.8.0.post1 --no-build-isolation
pip install -e.[code] Prepare a code sandbox at E2B. Export your E2B token to E2B_API_KEY environment variable. Log in to wandb and export your WANDB_ENTITY. Prepare a code sandbox at. Export your E2B token to
E2B_API_KEY

E2B_API_KEY environment variable. Log in to wandb and export your
WANDB_ENTITY

WANDB_ENTITY

2. Data preparation

2. Data preparation We prepare a hard split of GRPO training data based on AceCode-89k. We prepare a hard split of GRPO training data based on AceCode-89k
cd recipes
python process_data.py --dataset_path ""TIGER-Lab/AceCode-89K"" --output_path ""./acecode_hard.jsonl"" --difficulty ""hard""

recipes
python process_data.py --dataset_path TIGER-Lab/AceCode-89K --output_path./acecode_hard.jsonl --difficulty

3. Start GRPO training

3. Start GRPO training
cd..
bash run.sh
# in `run.sh`, we start e2b server locally, but you can also run it on CPU clusters...
bash run.sh in `run.sh`, we start e2b server locally, but you can also run it on CPU clusters. Inference

Inference The DiffuCoder models (Base, Instruct, and cpGRPO) are now available on HuggingFace. Change TOKEN_PER_STEP to trade off between performance and speed. The DiffuCoder models ( Instruct cpGRPO ) are now available on HuggingFace. Change
TOKEN_PER_STEP

TOKEN_PER_STEP to trade off between performance and speed. Usage for Base model (click to expand)
import torch
from transformers import AutoModel, AutoTokenizer

model_path = ""apple/DiffuCoder-7B-Base""
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to(""cuda"").eval()

prompt = """"""
from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
\""\""\""
Check if in given list of numbers, are any two numbers closer to each other than given threshold.
>>> has_close_elements([1.0, 2.0, 3.0], 0.5)
False
>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
True
\""\""\""
""""""

TOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens

inputs = tokenizer(prompt, return_tensors=""pt"")
input_ids = inputs.input_ids.to(device=""cuda"")
attention_mask = inputs.attention_mask.to(device=""cuda"")

output = model.diffusion_generate(
input_ids,
attention_mask=attention_mask,
max_new_tokens=256,
output_history=True,
return_dict_in_generate=True,
steps=256//TOKEN_PER_STEP,
temperature=0.2,
top_p=0.95,
alg=""entropy"",
alg_temp=0.,
)
generations = [
tokenizer.decode(g[len(p):].tolist())
for p, g in zip(input_ids, output.sequences)
]

print(generations[0].split(tokenizer.eos_token)[0])

import transformers import AutoModel AutoTokenizer model_path ""apple/DiffuCoder-7B-Base"" AutoModel from_pretrained model_path torch_dtype bfloat16 trust_remote_code tokenizer AutoTokenizer from_pretrained model_path trust_remote_code ""cuda"" prompt from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool: Check if in given list of numbers, are any two numbers closer to each other than given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) TOKEN_PER_STEP # diffusion timesteps * TOKEN_PER_STEP = total new tokens inputs tokenizer prompt return_tensors input_ids inputs input_ids device ""cuda"" attention_mask inputs attention_mask device ""cuda"" output diffusion_generate input_ids attention_mask attention_mask max_new_tokens output_history return_dict_in_generate TOKEN_PER_STEP temperature ""entropy"" alg_temp generations tokenizer decode tolist input_ids output sequences generations tokenizer eos_token Output (click to expand)
# Sort the list to make it easier to find close elements
numbers.sort()

# Iterate through the list, checking each adjacent pair
for i in range(len(numbers) - 1):
# If the difference between the current and next element is less than the threshold, return True
if numbers[i + 1] - numbers[i] < threshold:
return True

# If no such pair is found, return False
return False

# Sort the list to make it easier to find close elements
numbers.sort()

# Iterate through the list, checking each adjacent pair
for i in range(len(numbers) - 1):
# If the difference between the current and next element is less than the threshold, return True
if numbers[i + 1] - numbers[i] < threshold:
return True

# If no such pair is found, return False
return False

# Sort the list to make it easier to find close elements
numbers.sort()

# Iterate through the list, checking each adjacent pair
for i in range(len(numbers) - 1):
# If the difference between the current and next element is less than the threshold, return True
if numbers[i + 1] - numbers[i] < threshold:
return True

# If no such pair is found, return False
return False
Given an example input from the HumanEval test, the output of DiffuCoder-Base is a direct completion of the code snippet. Given an example input from the HumanEval test, the output of DiffuCoder-Base is a direct completion of the code snippet. Given an example input from the HumanEval test, the output of DiffuCoder-Base is a direct completion of the code snippet. Usage for Instruct model (click to expand)
import torch
from transformers import AutoModel, AutoTokenizer

model_path = ""apple/DiffuCoder-7B-cpGRPO""
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to(""cuda"").eval()

query = ""Write a function to find the shared elements from the given two lists.""
prompt = f""""""<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{query.strip()}
<|im_end|>
<|im_start|>assistant
"""""" ## following the template of qwen; you can also use apply_chat_template function

TOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens

inputs = tokenizer(prompt, return_tensors=""pt"")
input_ids = inputs.input_ids.to(device=""cuda"")
attention_mask = inputs.attention_mask.to(device=""cuda"")

output = model.diffusion_generate(
input_ids,
attention_mask=attention_mask,
max_new_tokens=256,
output_history=True,
return_dict_in_generate=True,
steps=256//TOKEN_PER_STEP,
temperature=0.4,
top_p=0.95,
alg=""entropy"",
alg_temp=0.,
)
generations = [
tokenizer.decode(g[len(p):].tolist())
for p, g in zip(input_ids, output.sequences)
]

print(generations[0].split('<|dlm_pad|>')[0])

import transformers import AutoModel AutoTokenizer model_path ""apple/DiffuCoder-7B-cpGRPO"" AutoModel from_pretrained model_path torch_dtype bfloat16 trust_remote_code tokenizer AutoTokenizer from_pretrained model_path trust_remote_code ""cuda"" ""Write a function to find the shared elements from the given two lists."" prompt f""""""<|im_start|>system You are a helpful assistant.<|im_end|> <|im_start|>user <|im_end|> <|im_start|>assistant ## following the template of qwen; you can also use apply_chat_template function TOKEN_PER_STEP # diffusion timesteps * TOKEN_PER_STEP = total new tokens inputs tokenizer prompt return_tensors input_ids inputs input_ids device ""cuda"" attention_mask inputs attention_mask device ""cuda"" output diffusion_generate input_ids attention_mask attention_mask max_new_tokens output_history return_dict_in_generate TOKEN_PER_STEP temperature ""entropy"" alg_temp generations tokenizer decode tolist input_ids output sequences generations '<|dlm_pad|>' Output (click to expand)
Here is the code to solve this problem:
```python
def shared_elements(list1, list2):
return [value for value in list1 if value in list2]
```<|im_end|>

Here is the code to solve this problem:
```python
def shared_elements(list1, list2):
return [value for value in list1 if value in list2]
```<|im_end|>

Here is the code to solve this problem:
```python
def shared_elements(list1, list2):
return [value for value in list1 if value in list2]
```<|im_end|>
Given an example input from the MBPP test, the output of DiffuCoder-cpGRPO is a chat-based response. Given an example input from the MBPP test, the output of DiffuCoder-cpGRPO is a chat-based response. Given an example input from the MBPP test, the output of DiffuCoder-cpGRPO is a chat-based response. Demo

🚀 Start the demo and enter any prompt you want!

🚀 Start the demo and enter any prompt you want!
python inference_demo.py

python inference_demo.py

Evaluation

Evaluation The diffusion inference algorithm is based on Dream-7B. The code evaluation is based on Qwen2.5-Coder. The diffusion inference algorithm is based on Dream-7B. The code evaluation is based on Qwen2.5-Coder

Acknowledgments

Acknowledgments We sincerely appreciate the following works for DiffuCoder:

We sincerely appreciate the following works for DiffuCoder: • Our data used in pre-training/mid-training/instruction tuning are from OpenCoder. Our data used in pre-training/mid-training/instruction tuning are from OpenCoder • Our instruction tuning code is based on LLaMA-Factory. Our instruction tuning code is based on LLaMA-Factory • Our coupled-GRPO is built upon Open-R1 and d1. Our coupled-GRPO is built upon Open-R1 • Our evaluation is built upon Dream and Qwen2.5-Coder. Our evaluation is built upon Qwen2.5-Coder

Citation

Citation
@article{gong2025diffucoder,
title={DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation},
author={Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang},
year={2025},
eprint={2506.20639},
archivePrefix={arXiv},
primaryClass={cs. CL},
url={https://arxiv.org/abs/2506.20639},
}

@article gong2025diffucoder DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation author Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang eprint 2506.20639 archivePrefix primaryClass https://arxiv.org/abs/2506.20639

About

DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

Resources

Resources Readme

License

License View license

Code of conduct

Code of conduct Code of conduct

Uh oh! Uh oh! There was an error while loading. Please reload this page. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page Activity Custom properties

Stars

Watchers

Watchers watching

Forks

Report repository

Releases

Releases No releases published

Packages
0

Packages No packages published

Uh oh! Uh oh! There was an error while loading. Please reload this page. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page

Contributors
3

Contributors

Uh oh! Uh oh! There was an error while loading. Please reload this page. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page

Languages

Languages • Python
99.7%
Python • Shell
0.3%","**Website Description:**

The GitHub repository for DiffuCoder serves as a comprehensive resource for understanding and enhancing masked diffusion models specifically tailored for code generation. This project is closely associated with the research paper titled 'DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation,' providing essential documentation, code files, and collaborative tools for developers and researchers interested in machine learning and code synthesis.

The repository includes various resources such as setup instructions, demo scripts, and contribution guidelines, making it an invaluable platform for both academic and practical applications in the field of artificial intelligence. Users can explore the latest updates, access the codebase, and participate in the ongoing development of the project, fostering a community of innovation and knowledge sharing in the realm of code generation technologies.",1.0
https://www.linkedin.com/pulse/welcome-era-evals-brendan-foody-ezykc/,Welcome to The Era of Evals,Artificial Intelligence,2041,2025-07-08T16:22:24.150652,www.linkedin.com,Vir Kashyap,U0910396AQ3,2025-07-01T15:37:38.762619,,article,,,"""urn:li:member:0"" ""true"" ""true"" Reinforcement Learning (RL) is driving the most exciting advancements in AI. RL is becoming so effective that models will be able to saturate any evaluation. This means that the primary barrier to applying agents to the entire economy is building evals for everything. However, AI labs are facing a dire shortage of relevant evaluations. Academic evaluations that labs goal on don’t reflect what consumers and enterprises demand in the economy. Reinforcement Learning (RL) is driving the most exciting advancements in AI. RL is becoming so effective that models will be able to saturate any evaluation. This means that the primary barrier to applying agents to the entire economy is building evals for everything. However, AI labs are facing a dire shortage of relevant evaluations. Academic evaluations that labs goal on don’t reflect what consumers and enterprises demand in the economy. Evals are the new PRD. Progress in accelerating knowledge work will converge on building environments and evaluations that map real workspaces and deliverables. This new RL-centric paradigm of human data is vastly more data efficient than pretraining, SFT, or RLHF. Most knowledge work includes recurring workflows as variable costs, but creating an environment or evaluation can transform that into a one-time fixed cost. Evals are the new PRD. Progress in accelerating knowledge work will converge on building environments and evaluations that map real workspaces and deliverables. This new RL-centric paradigm of human data is vastly more data efficient than pretraining, SFT, or RLHF. Most knowledge work includes recurring workflows as variable costs, but creating an environment or evaluation can transform that into a one-time fixed cost. Training on Verifiable Rewards

Training on Verifiable Rewards RL environments allow for rewarding outcomes and intermediate steps in an evaluation. Models take many attempts at a problem, using test-time compute to ""think"" before it answers. Human created autograders reward the attempts which were ""good"". Reinforcing on those ""good"" trajectories upweights the chains of thought that were used to get to the answer. This teaches models to think correctly about different types of problems as researchers iteratively hill climb evals. These environments can be thought of as existing on a spectrum of rigidity between two categories:

RL environments allow for rewarding outcomes and intermediate steps in an evaluation. Models take many attempts at a problem, using test-time compute to ""think"" before it answers. Human created autograders reward the attempts which were ""good"". Reinforcing on those ""good"" trajectories upweights the chains of thought that were used to get to the answer. This teaches models to think correctly about different types of problems as researchers iteratively hill climb evals. These environments can be thought of as existing on a spectrum of rigidity between two categories: Objective domains: Games, like pac-man, chess, and Go, have clear states spaces, action spaces, and desired outcomes. Math, code, and even some tasks in biology, can often be formulated with near game-like verifiability. This is where RL has achieved early massive success already, notably, AlphaProof, AlphaFold, and DeepSeek R1 and the many code generation models on the market today. Subjective domains: It’s more difficult to measure accuracy in many real world tasks such as generating investment memos, making legal briefs, providing therapy. This makes it difficult to verify that a model achieved desired outcomes. Additionally, experts often support multiple valid opinions about desired processes and outcomes. Rubric-based rewards serve as a way to learn from the messiness of expert human opinions. How to evaluate and train with rubrics as environments is an exciting area of research with roots laid as early as constitutional AI and RLAIF work from Anthropic.

• Objective domains: Games, like pac-man, chess, and Go, have clear states spaces, action spaces, and desired outcomes. Math, code, and even some tasks in biology, can often be formulated with near game-like verifiability. This is where RL has achieved early massive success already, notably, AlphaProof, AlphaFold, and DeepSeek R1 and the many code generation models on the market today. Objective domains: Games, like pac-man, chess, and Go, have clear states spaces, action spaces, and desired outcomes. Math, code, and even some tasks in biology, can often be formulated with near game-like verifiability. This is where RL has achieved early massive success already, notably, AlphaProof, AlphaFold, and DeepSeek R1 and the many code generation models on the market today. • Subjective domains: It’s more difficult to measure accuracy in many real world tasks such as generating investment memos, making legal briefs, providing therapy. This makes it difficult to verify that a model achieved desired outcomes. Additionally, experts often support multiple valid opinions about desired processes and outcomes. Rubric-based rewards serve as a way to learn from the messiness of expert human opinions. How to evaluate and train with rubrics as environments is an exciting area of research with roots laid as early as constitutional AI and RLAIF work from Anthropic. Subjective domains: It’s more difficult to measure accuracy in many real world tasks such as generating investment memos, making legal briefs, providing therapy. This makes it difficult to verify that a model achieved desired outcomes. Additionally, experts often support multiple valid opinions about desired processes and outcomes. Rubric-based rewards serve as a way to learn from the messiness of expert human opinions. How to evaluate and train with rubrics as environments is an exciting area of research with roots laid as early as constitutional AI and RLAIF work from Anthropic. Computer-use agents sit somewhere in the middle. For most of the tasks humans do on computers, goals start to become ambiguous and multi-faceted. Once defined, the actions and outcomes are programmatic and verifiable. These could include planning trips, responding to emails, shopping, or posting on social media. In all of these cases, containerized environments allow for horizontal scaling to learn online from thousands of interactions in parallel. Computer-use agents sit somewhere in the middle. For most of the tasks humans do on computers, goals start to become ambiguous and multi-faceted. Once defined, the actions and outcomes are programmatic and verifiable. These could include planning trips, responding to emails, shopping, or posting on social media. In all of these cases, containerized environments allow for horizontal scaling to learn online from thousands of interactions in parallel. Environments Create Experience

Environments Create Experience Eventually, our AI systems will learn automatically from signals in the real world like pupils’ test scores increasing, sales closing, maybe even bridges being built. However, intermediate rewards will always remain critical. Similar to how humans learn from other people, models will need guidance on which styles of teaching and sales techniques are most effective. Humans will remain an integral part of the environments models learn from. Eventually, our AI systems will learn automatically from signals in the real world like pupils’ test scores increasing, sales closing, maybe even bridges being built. However, intermediate rewards will always remain critical. Similar to how humans learn from other people, models will need guidance on which styles of teaching and sales techniques are most effective. Humans will remain an integral part of the environments models learn from. We will never escape the era of data; it must follow us to the frontier. That frontier is human created environments that provide durable sources of experiential data. These environments can serve to train and evaluate models. We will never escape the era of data; it must follow us to the frontier. That frontier is human created environments that provide durable sources of experiential data. These environments can serve to train and evaluate models. The Path Forward

The Path Forward Meeting today’s data demand requires rethinking the way we generate signal from human efforts. Creating evals and RL environments is the highest leverage and most durable use of people’s time. Mercor has helped pioneer environment generation using autograders and continues to push the boundaries of RL data with simulated workspaces, multi-turn support, and multi-modality. Meeting today’s data demand requires rethinking the way we generate signal from human efforts. Creating evals and RL environments is the highest leverage and most durable use of people’s time. Mercor has helped pioneer environment generation using autograders and continues to push the boundaries of RL data with simulated workspaces, multi-turn support, and multi-modality. Knowledge work will quickly converge on building RL environments and evaluations for agents to learn from. As AI enters the workforce and operates over proprietary information and under unique professional contexts, these environments codify knowledge and goals for agents. Once individual steps of agentic workflows reach sufficient reliability, all that will be left will be RL training on the goals laid out by humankind. Knowledge work will quickly converge on building RL environments and evaluations for agents to learn from. As AI enters the workforce and operates over proprietary information and under unique professional contexts, these environments codify knowledge and goals for agents. Once individual steps of agentic workflows reach sufficient reliability, all that will be left will be RL training on the goals laid out by humankind. ""https://www.linkedin.com/feed/update/urn:li:ugcPost:7345546932519411712"" ""urn:li:ugcPost:7345546932519411712"" ""urn:li:ugcPost:7345546932519411712"" ""https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Fwelcome-era-evals-brendan-foody-ezykc%2F"" ""%reactionType% %selectionState%"" Celebrate Support Insightful Comment ""Link copied to clipboard."" ""Something went wrong while copying to clipboard."" • Copy
• LinkedIn

LinkedIn • Facebook
Facebook • Twitter
Twitter ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 81 Comments Daniel Fiott Chief Executive Officer @ Cloud Continuous

Chief Executive Officer @ Cloud Continuous • Report this comment
Report this comment Era of Evals has a nice ring to it. Love seeing this actually happening at enterprise scale. Era of Evals has a nice ring to it. Love seeing this actually happening at enterprise scale. 1 Reaction Kavyasree Kuruva Intern @GQT | Java Developer

Intern @GQT | Java Developer • Report this comment
Report this comment Thank you for the opportunity. I am a fresher. This is my email id kavyasreekoli@gmail.com

Thank you for the opportunity. I am a fresher. This is my email id kavyasreekoli@gmail.com 1 Reaction • Report this comment
Report this comment Huge milestone, Brendan! Working with 6 out of the Magnificent 7 is no small feat. At agenQ, we’re big believers in how AI talent is reshaping the enterprise,Mercor’s momentum is proof. Excited to see what’s next! Huge milestone, Brendan! Working with 6 out of the Magnificent 7 is no small feat. At agenQ, we’re big believers in how AI talent is reshaping the enterprise,Mercor’s momentum is proof. Excited to see what’s next! 1 Reaction 2 Reactions Satwik Behera Machine Learning Engineer | Data Scientist

Machine Learning Engineer | Data Scientist • Report this comment
Report this comment satwikbehera12@gmail.com

satwikbehera12@gmail.com 1 Reaction Archana B Research Scholar @ VNIT, Nagpur

|| Information Retrieval || Image Processing || Computer Vision || NLP

Research Scholar @ VNIT, Nagpur

|| Information Retrieval || Image Processing || Computer Vision || NLP • Report this comment
Report this comment archana.narayandas07@gmail.com

archana.narayandas07@gmail.com 1 Reaction See more comments To view or add a comment, sign in

To view or add a comment, sign in

More articles by Brendan Foody

More articles by Brendan Foody • The Secret Mercor Master Plan

Mar 4, 2025

The Secret Mercor Master Plan

Imagine a world where Jeff Bezos is a hedge fund investor, Howard Shultz is a salesman, and Reed Hastings is a teacher.…

292

24 Comments
The Secret Mercor Master Plan Mar 4, 2025

The Secret Mercor Master Plan

The Secret Mercor Master Plan Imagine a world where Jeff Bezos is a hedge fund investor, Howard Shultz is a salesman, and Reed Hastings is a teacher.…

Imagine a world where Jeff Bezos is a hedge fund investor, Howard Shultz is a salesman, and Reed Hastings is a teacher.… ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 24 Comments

Sign in

Sign in Stay updated on your professional world

Stay updated on your professional world Sign in By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement Privacy Policy Cookie Policy New to LinkedIn? Join now

New to LinkedIn? Join now

Explore topics

Explore topics • Sales
• Marketing

Marketing • IT Services
IT Services • Business Administration
Business Administration • HR Management
HR Management • Engineering
Engineering • Soft Skills
Soft Skills • See All
See All","**Welcome to The Era of Evals**



Key points:

• ""urn:li:member:0"" ""true"" ""true"" Reinforcement Learning (RL) is driving the most exciting advancements in AI.

• RL is becoming so effective that models will be able to saturate any evaluation.

• This means that the primary barrier to applying agents to the entire economy is building evals for everything.

• However, AI labs are facing a dire shortage of relevant evaluations.

• Academic evaluations that labs goal on don’t reflect what consumers and enterprises demand in the economy.

• Reinforcement Learning (RL) is driving the most exciting advancements in AI.

RL is becoming so effective that models will be able to saturate any evaluation. This means that the primary barrier to applying agents to the entire economy is building evals for everything. However, AI labs are facing a dire shortage of relevant evaluations. Academic evaluations that labs goal on don’t reflect what consumers and enterprises demand in the economy. Evals are the new PRD. Progress in accelerating knowledge work will converge on building environments and evaluations that map real workspaces and deliverables. This new RL-centric paradigm of human data is vastly more data efficient than pretraining, SFT, or RLHF. Most knowledge work includes recurring workflows as variable costs, but creating an environment or evaluation can transform that into a one-time fixed cost. Evals are the new PRD. Progress in accelerating knowledge work will converge on building environments and evaluations that map real workspaces and deliverables. This new RL-centric paradigm of human data is vastly more data efficient than pretraining, SFT, or RLHF. Most knowledge work includes recurring workflows as variable costs, but creating an environment or evaluation can transform that into a one-time fixed cost. Training on Verifiable Rewards

Key points:

• Training on Verifiable Rewards RL environments allow for rewarding outcomes and intermediate steps in an evaluation.

• Models take many attempts at a problem, using test-time compute to ""think"" before it answers.

• Human created autograders reward the attempts which were ""good"".

• Reinforcing on those ""good"" trajectories upweights the chains of thought that were used to get to the answer.

• This teaches models to think correctly about different types of problems as researchers iteratively hill climb evals.

• These environments can be thought of as existing on a spectrum of rigidity between two categories:.

Key points:

• RL environments allow for rewarding outcomes and intermediate steps in an evaluation.

• Models take many attempts at a problem, using test-time compute to ""think"" before it answers.

• Human created autograders reward the attempts which were ""good"".

• Reinforcing on those ""good"" trajectories upweights the chains of thought that were used to get to the answer.

• This teaches models to think correctly about different types of problems as researchers iteratively hill climb evals.

• These environments can be thought of as existing on a spectrum of rigidity between two categories: Objective domains: Games, like pac-man, chess, and Go, have clear states spaces, action spaces, and desired outcomes.

Math, code, and even some tasks in biology, can often be formulated with near game-like verifiability. This is where RL has achieved early massive success already, notably, AlphaProof, AlphaFold, and DeepSeek R1 and the many code generation models on the market today. Subjective domains: It’s more difficult to measure accuracy in many real world tasks such as generating investment memos, making legal briefs, providing therapy. This makes it difficult to verify that a model achieved desired outcomes. Additionally, experts often support multiple valid opinions about desired processes and outcomes. Rubric-based rewards serve as a way to learn from the messiness of expert human opinions. How to evaluate and train with rubrics as environments is an exciting area of research with roots laid as early as constitutional AI and RLAIF work from Anthropic.

Key points:

• • Objective domains: Games, like pac-man, chess, and Go, have clear states spaces, action spaces, and desired outcomes.

• Math, code, and even some tasks in biology, can often be formulated with near game-like verifiability.

• This is where RL has achieved early massive success already, notably, AlphaProof, AlphaFold, and DeepSeek R1 and the many code generation models on the market today.

• Objective domains: Games, like pac-man, chess, and Go, have clear states spaces, action spaces, and desired outcomes.

• Math, code, and even some tasks in biology, can often be formulated with near game-like verifiability.

• This is where RL has achieved early massive success already, notably, AlphaProof, AlphaFold, and DeepSeek R1 and the many code generation models on the market today.

• Subjective domains: It’s more difficult to measure accuracy in many real world tasks such as generating investment memos, making legal briefs, providing therapy. This makes it difficult to verify that a model achieved desired outcomes. Additionally, experts often support multiple valid opinions about desired processes and outcomes. Rubric-based rewards serve as a way to learn from the messiness of expert human opinions. How to evaluate and train with rubrics as environments is an exciting area of research with roots laid as early as constitutional AI and RLAIF work from Anthropic. Subjective domains: It’s more difficult to measure accuracy in many real world tasks such as generating investment memos, making legal briefs, providing therapy. This makes it difficult to verify that a model achieved desired outcomes. Additionally, experts often support multiple valid opinions about desired processes and outcomes. Rubric-based rewards serve as a way to learn from the messiness of expert human opinions. How to evaluate and train with rubrics as environments is an exciting area of research with roots laid as early as constitutional AI and RLAIF work from Anthropic. Computer-use agents sit somewhere in the middle. For most of the tasks humans do on computers, goals start to become ambiguous and multi-faceted. Once defined, the actions and outcomes are programmatic and verifiable. These could include planning trips, responding to emails, shopping, or posting on social media. In all of these cases, containerized environments allow for horizontal scaling to learn online from thousands of interactions in parallel. Computer-use agents sit somewhere in the middle. For most of the tasks humans do on computers, goals start to become ambiguous and multi-faceted. Once defined, the actions and outcomes are programmatic and verifiable. These could include planning trips, responding to emails, shopping, or posting on social media. In all of these cases, containerized environments allow for horizontal scaling to learn online from thousands of interactions in parallel. Environments Create Experience

Key points:

• Environments Create Experience Eventually, our AI systems will learn automatically from signals in the real world like pupils’ test scores increasing, sales closing, maybe even bridges being built.

• However, intermediate rewards will always remain critical.

• Similar to how humans learn from other people, models will need guidance on which styles of teaching and sales techniques are most effective.

• Humans will remain an integral part of the environments models learn from.

• Eventually, our AI systems will learn automatically from signals in the real world like pupils’ test scores increasing, sales closing, maybe even bridges being built.

• However, intermediate rewards will always remain critical.

Similar to how humans learn from other people, models will need guidance on which styles of teaching and sales techniques are most effective. Humans will remain an integral part of the environments models learn from. We will never escape the era of data; it must follow us to the frontier. That frontier is human created environments that provide durable sources of experiential data. These environments can serve to train and evaluate models. We will never escape the era of data; it must follow us to the frontier. That frontier is human created environments that provide durable sources of experiential data. These environments can serve to train and evaluate models. The Path Forward

Key points:

• The Path Forward Meeting today’s data demand requires rethinking the way we generate signal from human efforts.

• Creating evals and RL environments is the highest leverage and most durable use of people’s time.

• Mercor has helped pioneer environment generation using autograders and continues to push the boundaries of RL data with simulated workspaces, multi-turn support, and multi-modality.

• Meeting today’s data demand requires rethinking the way we generate signal from human efforts.

• Creating evals and RL environments is the highest leverage and most durable use of people’s time.

• Mercor has helped pioneer environment generation using autograders and continues to push the boundaries of RL data with simulated workspaces, multi-turn support, and multi-modality.

Knowledge work will quickly converge on building RL environments and evaluations for agents to learn from. As AI enters the workforce and operates over proprietary information and under unique professional contexts, these environments codify knowledge and goals for agents. Once individual steps of agentic workflows reach sufficient reliability, all that will be left will be RL training on the goals laid out by humankind. Knowledge work will quickly converge on building RL environments and evaluations for agents to learn from. As AI enters the workforce and operates over proprietary information and under unique professional contexts, these environments codify knowledge and goals for agents. Once individual steps of agentic workflows reach sufficient reliability, all that will be left will be RL training on the goals laid out by humankind. ""https://www.linkedin.com/feed/update/urn:li:ugcPost:7345546932519411712"" ""urn:li:ugcPost:7345546932519411712"" ""urn:li:ugcPost:7345546932519411712"" ""https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Fwelcome-era-evals-brendan-foody-ezykc%2F"" ""%reactionType% %selectionState%"" Celebrate Support Insightful Comment ""Link copied to clipboard."" ""Something went wrong while copying to clipboard."" • Copy
• LinkedIn

LinkedIn • Facebook
Facebook • Twitter
Twitter ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 81 Comments Daniel Fiott Chief Executive Officer @ Cloud Continuous

Chief Executive Officer @ Cloud Continuous • Report this comment
Report this comment Era of Evals has a nice ring to it. Love seeing this actually happening at enterprise scale. Era of Evals has a nice ring to it. Love seeing this actually happening at enterprise scale. 1 Reaction Kavyasree Kuruva Intern @GQT | Java Developer

Intern @GQT | Java Developer • Report this comment
Report this comment Thank you for the opportunity. I am a fresher. This is my email id kavyasreekoli@gmail.com

Key points:

• Thank you for the opportunity.

• This is my email id kavyasreekoli@gmail.com 1 Reaction • Report this comment
Report this comment Huge milestone, Brendan! Working with 6 out of the Magnificent 7 is no small feat.

• At agenQ, we’re big believers in how AI talent is reshaping the enterprise,Mercor’s momentum is proof.

• Excited to see what’s next! Huge milestone, Brendan! Working with 6 out of the Magnificent 7 is no small feat.

• At agenQ, we’re big believers in how AI talent is reshaping the enterprise,Mercor’s momentum is proof.

Excited to see what’s next! 1 Reaction 2 Reactions Satwik Behera Machine Learning Engineer | Data Scientist

Machine Learning Engineer | Data Scientist • Report this comment
Report this comment satwikbehera12@gmail.com

satwikbehera12@gmail.com 1 Reaction Archana B Research Scholar @ VNIT, Nagpur

|| Information Retrieval || Image Processing || Computer Vision || NLP

Research Scholar @ VNIT, Nagpur

|| Information Retrieval || Image Processing || Computer Vision || NLP • Report this comment
Report this comment archana.narayandas07@gmail.com

archana.narayandas07@gmail.com 1 Reaction See more comments To view or add a comment, sign in

To view or add a comment, sign in

More articles by Brendan Foody

More articles by Brendan Foody • The Secret Mercor Master Plan

Mar 4, 2025

The Secret Mercor Master Plan

Imagine a world where Jeff Bezos is a hedge fund investor, Howard Shultz is a salesman, and Reed Hastings is a teacher.…

292

• Comments
The Secret Mercor Master Plan Mar 4, 2025

The Secret Mercor Master Plan

The Secret Mercor Master Plan Imagine a world where Jeff Bezos is a hedge fund investor, Howard Shultz is a salesman, and Reed Hastings is a teacher.…

Imagine a world where Jeff Bezos is a hedge fund investor, Howard Shultz is a salesman, and Reed Hastings is a teacher.… ""%numReactions% Reaction"" ""%numReactions% Reactions"" ""https://static.licdn.com/aero-v1/sc/h/cyfai5zw4nrqhyyhl0p7so58v"" ""https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2"" ""https://static.licdn.com/aero-v1/sc/h/22ifp2etz8kb9tgjqn65s9ics"" ""https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu"" ""https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671"" ""https://static.licdn.com/aero-v1/sc/h/cryzkreqrh52ja5bc6njlrupa"" ""https://static.licdn.com/aero-v1/sc/h/2tzoeodxy0zug4455msr0oq0v"" 24 Comments

Sign in

Sign in Stay updated on your professional world

Stay updated on your professional world Sign in By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement Privacy Policy Cookie Policy New to LinkedIn? Join now

New to LinkedIn? Join now

Explore topics

Explore topics • Sales
• Marketing

Marketing • IT Services
IT Services • Business Administration
Business Administration • HR Management
HR Management • Engineering
Engineering • Soft Skills
Soft Skills • See All
See All",1.0
https://yidingjiang.github.io/blog/post/exploration/,The Era of Exploration,Artificial Intelligence,6325,2025-07-08T16:23:33.906689,yidingjiang.github.io,Sai,U07TKPGTKDF,2025-07-07T11:21:22.591239,,article,,,"Large language models are the unintended byproduct of about three decades worth of freely accessible human text online. Ilya Sutskever compared this reservoir of information to fossil fuel, abundant but ultimately finite. Some studies suggest that, at current token‑consumption rates, frontier labs could exhaust the highest‑quality English web text well before the decade ends. Even if those projections prove overly pessimistic, one fact is clear: today’s models consume data far faster than humans can produce it. Large language models are the unintended byproduct of about three decades worth of freely accessible human text online. Ilya Sutskever compared this reservoir of information to fossil fuel, abundant but ultimately finite. Some studies suggest that, at current token‑consumption rates, frontier labs could exhaust the highest‑quality English web text well before the decade ends. Even if those projections prove overly pessimistic, one fact is clear: today’s models consume data far faster than humans can produce it. David Silver and Richard Sutton call this coming phase the “Era of Experience,”  where meaningful progress will depend on data that learning agents generate for themselves. In this post, I want to build on their statement further: the bottleneck is not having just any experience but collecting the right kind of experience that benefits learning. The next wave of AI progress will hinge less on stacking parameters and more on exploration, the process of acquiring new and informative experience. David Silver and Richard Sutton call this coming phase the “ Era of Experience,”  where meaningful progress will depend on data that learning agents generate for themselves. In this post, I want to build on their statement further: the bottleneck is not having just experience but collecting the kind of experience that benefits learning. The next wave of AI progress will hinge less on stacking parameters and more on exploration, the process of acquiring new and informative experience. To talk about experience collection, we must also ask what it costs to collect them. Scaling is, in the end, a question of resources – compute cycles, synthetic‑data generation, data curation pipelines, human oversight, any expenditure that creates learning signal. For simplicity, I’ll fold all of these costs into a single bookkeeping unit I call flops. Strictly speaking, a flop is one floating‑point operation, but the term has become a lingua franca for “how much effort did this system consume?” I’m co‑opting it here not for its engineering precision but because it gives us a common abstract currency. My discussion depends only on relative spend, not on the particular mix of silicon, data, or human time. Treat flops as shorthand for “whatever scarce resource constrains scale.”

To talk about experience collection, we must also ask what it costs to collect them. Scaling is, in the end, a question of resources – compute cycles, synthetic‑data generation, data curation pipelines, human oversight, any expenditure that creates learning signal. For simplicity, I’ll fold all of these costs into a single bookkeeping unit I call. Strictly speaking, a flop is one floating‑point operation, but the term has become a lingua franca for “how much effort did this system consume?” I’m co‑opting it here not for its engineering precision but because it gives us a common abstract currency. My discussion depends only on relative spend, not on the particular mix of silicon, data, or human time. Treat flops as shorthand for “whatever scarce resource constrains scale.” In the sections that follow, I’ll lay out a handful of observations and connect ideas that usually appear in different contexts. Exploration is most often used in the context of reinforcement learning (RL), but I will also use “exploration” in a broader sense – much wider than its usual role in RL – because every data-driven system has to decide which experiences to collect before it can learn from them. This usage of exploration is also inspired by my friend Minqi’s excellent article “General intelligence requires rethinking exploration.”

In the sections that follow, I’ll lay out a handful of observations and connect ideas that usually appear in different contexts. Exploration is most often used in the context of reinforcement learning (RL), but I will also use “exploration” in a broader sense – much wider than its usual role in RL – because every data-driven system has to decide which experiences to collect before it can learn from them. This usage of exploration is also inspired by my friend ’s excellent article “ General intelligence requires rethinking exploration The rest of the post is organized as the following: first, how pre‑training inadvertently solved a part of the exploration problem, second, why better exploration translates into better generalization, and finally, where we should spend the next hundred thousand GPU‑years. The rest of the post is organized as the following: first, how pre‑training inadvertently solved a part of the exploration problem, second, why better exploration translates into better generalization, and finally, where we should spend the next hundred thousand GPU‑years. Pretraining is exploration

Pretraining is exploration The standard LLM pipeline is to first pretrain a large model on next-token prediction with a large amount of text and then finetune the model with RL to achieve some desired objectives. Without large-scale pretraining, the RL step would struggle to make any progress. This contrast suggests that pretraining has accomplished something that is difficult for tabula rasa RL (i.e., from scratch). The standard LLM pipeline is to first pretrain a large model on next-token prediction with a large amount of text and then finetune the model with RL to achieve some desired objectives. Without large-scale pretraining, the RL step would struggle to make any progress. This contrast suggests that pretraining has accomplished something that is difficult for tabula rasa RL (i.e., from scratch). A seemingly contradictory and widely observed trend in recent research is that smaller models can demonstrate significantly improved reasoning abilities once distilled using the chain-of-thought generated by larger, more capable models. Some interpret this as evidence that large scale is not a prerequisite for effective reasoning. In my opinion, this conclusion is misguided. The question we should ask is: if model capacity is not the bottleneck for reasoning, why do small models need to distill from a larger model at all? A seemingly contradictory and widely observed trend in recent research is that smaller models can demonstrate significantly improved reasoning abilities once distilled using the chain-of-thought generated by larger, more capable models. Some interpret this as evidence that large scale is not a prerequisite for effective reasoning. In my opinion, this conclusion is misguided. The question we should ask is: if model capacity is not the bottleneck for reasoning, why do small models need to distill from a larger model at all? A compelling explanation for both observations is that the immense cost of pretraining is effectively paying a massive, upfront “exploration tax.” By themselves, models with no pretraining or smaller pretrained models have a much harder time reliably exploring the solution space and discovering good solutions on their own1. The pretraining stage pays this tax by spending vast amounts of compute on diverse data to learn a rich sampling distribution under which the correct continuations are likely. Distillation, in turn, is a mechanism for letting a smaller model inherit that payment, bootstrapping its exploration capabilities from the massive investment made in the larger model. A compelling explanation for both observations is that the immense cost of pretraining is effectively paying a massive, upfront “ exploration tax.” By themselves, models with no pretraining or smaller pretrained models have a much harder time reliably exploring the solution space and discovering good solutions on their own. The pretraining stage pays this tax by spending vast amounts of compute on diverse data to learn a rich sampling distribution under which the correct continuations are likely. Distillation, in turn, is a mechanism for letting a smaller model inherit that payment, bootstrapping its exploration capabilities from the massive investment made in the larger model. Why is this pre-paid exploration so important? In its most general form, the RL loop looks something like this:

Why is this pre-paid exploration so important? In its most general form, the RL loop looks something like this: • Exploration. The agent generates some randomized exploration trajectories. Exploration. The agent generates some randomized exploration trajectories. • Reinforce. The good trajectories are up-weighted and the bad ones are down-weighted. Reinforce. The good trajectories are up-weighted and the bad ones are down-weighted. For this learning loop to be effective, the agent must be capable of generating at least a minimal number of “good” trajectories during its exploration phase. This concept is sometimes called coverage in RL. In LLMs, this exploration is typically achieved through sampling from the model’s autoregressive output distribution. Under this exploration scheme, the correct solutions need to already be likely in the naive sampling distribution. If a lower‑capacity model rarely stumbles on a valid solution by random sampling, it would have nothing useful to reinforce. For this learning loop to be effective, the agent must be capable of generating at least a minimal number of “good” trajectories during its exploration phase. This concept is sometimes called coverage in RL. In LLMs, this exploration is typically achieved through sampling from the model’s autoregressive output distribution. Under this exploration scheme, the correct solutions need to already be likely in the naive sampling distribution. If a lower‑capacity model rarely stumbles on a valid solution by random sampling, it would have nothing useful to reinforce. Exploration without any prior information is a very difficult process. Even in the simplest tabular RL setting, where every situation (a state) and every action can be listed out in a table, theory says learning needs a lot of trials. A well known lower-bound on the sample complexity on the number of episodes for tabular RL is \(\Omega(\frac{SAH^2}{\epsilon^2})\) (Dann & Brunskill, 2015), where \(S\) is the size of the state space, \(A\) is the size of the action space, \(H\) is the horizon, and \(\epsilon\) is the “distance” to the best possible solution. This means that the minimum number of episodes grows linearly with the number of state-action pairs, and quadratically with the horizon. For LLMs, the state space now includes every possible text prefix, and the action space is any next token, both of which are very large. Without any prior information, RL in this setting would be practically impossible. Exploration without any prior information is a very difficult process. Even in the simplest tabular RL setting, where every situation (a state) and every action can be listed out in a table, theory says learning needs a lot of trials. A well known lower-bound on the sample complexity on the number of episodes for tabular RL is \(\Omega(\frac{SAH^2}{\epsilon^2})\) (Dann & Brunskill, 2015), where \(S\) is the size of the state space, \(A\) is the size of the action space, \(H\) is the horizon, and \(\epsilon\) is the “distance” to the best possible solution. This means that the minimum number of episodes grows linearly with the number of state-action pairs, and quadratically with the horizon. For LLMs, the state space now includes every possible text prefix, and the action space is any next token, both of which are very large. Without any prior information, RL in this setting would be practically impossible. Up to now, the hard work of exploration has been largely done by pretraining and learning a better prior from which to sample trajectories. However, this also means that the types of trajectories the model can sample naively are heavily constrained by the prior. To progress further, we must figure out how to move beyond the prior. Up to now, the hard work of exploration has been largely done by pretraining and learning a better prior from which to sample trajectories. However, this also means that the types of trajectories the model can sample naively are heavily constrained by the prior. To progress further, we must figure out how to move beyond the prior. Exploration helps generalization

Exploration helps generalization Historically, RL research has focused on solving a single environment at a time, like Atari or MuJoCo. This is equivalent to training and testing on the same datapoint. However, how well a given model does in the single environment does not say much about how well the model can handle truly novel situations. Machine learning is ultimately about generalization: for many hard problems, if we know them in advance, we can engineer a bespoke solution. What matters is succeeding on problems we haven’t seen or even anticipated. Historically, RL research has focused on solving a single environment at a time, like Atari or MuJoCo. This is equivalent to training and testing on the same datapoint. However, how well a given model does in the single environment does not say much about how well the model can handle truly novel situations. Machine learning is ultimately about generalization: for many hard problems, if we know them in advance, we can engineer a bespoke solution. What matters is succeeding on problems we haven’t seen or even anticipated. The generalization performance of RL is critical for language models. During training, an LLM sees only a finite set of prompts, yet at deployment it must handle arbitrary user queries that could be very different from the ones seen during training. Notably, current LLMs excel at tasks with a verifiable reward (e.g., coding puzzles or formal proofs) because the correctness can be easily checked. The tougher question is to generalize these capabilities to fuzzier domains (e.g., generate a research report or write a novel) where feedback is sparse or ambiguous, and large-scale training and data collection are difficult. The generalization performance of RL is critical for language models. During training, an LLM sees only a finite set of prompts, yet at deployment it must handle arbitrary user queries that could be very different from the ones seen during training. Notably, current LLMs excel at tasks with a verifiable reward (e.g., coding puzzles or formal proofs) because the correctness can be easily checked. The tougher question is to generalize these capabilities to fuzzier domains (e.g., generate a research report or write a novel) where feedback is sparse or ambiguous, and large-scale training and data collection are difficult. What are some options we have for training a generalizable model? A recurring theme of deep learning is that data diversity drives robust generalization. Exploration directly controls the diversity of the data. In supervised learning, a labeled example reveals all its details in a single forward pass2 so the only way to increase data diversity is to collect more data. In RL, by contrast, each interaction only exposes a narrow slice of the environment. As a result, the agent must gather a sufficiently varied set of trajectories to build a representative picture. If the trajectories collected lack diversity (e.g., naive random sampling), the policy can overfit to that narrow slice and stumble even within the very same environment. What are some options we have for training a generalizable model? A recurring theme of deep learning is that data diversity drives robust generalization. Exploration directly controls the diversity of the data. In supervised learning, a labeled example reveals all its details in a single forward pass so the only way to increase data diversity is to collect more data. In RL, by contrast, each interaction only exposes a narrow slice of the environment. As a result, the agent must gather a sufficiently varied set of trajectories to build a representative picture. If the trajectories collected lack diversity (e.g., naive random sampling), the policy can overfit to that narrow slice and stumble even within the very same environment. This problem compounds when there are multiple environments. A popular RL generalization benchmark is Procgen, which is a collection of Atari-like games that have procedurally generated environments, so each game in principle contains “infinitely” many environments. The objective is to train on a fixed number of environments for a fixed number of steps and generalize to completely unseen environments3. This problem compounds when there are multiple environments. A popular RL generalization benchmark is Procgen, which is a collection of Atari-like games that have procedurally generated environments, so each game in principle contains “infinitely” many environments. The objective is to train on a fixed number of environments for a fixed number of steps and generalize to completely unseen environments Many existing approaches for this benchmark treat the problem as a representation learning problem and apply regularization techniques adapted from supervised learning (e.g., dropout or data augmentation). These help, but they overlook exploration, one of the most important structural components of RL. Since the agents collect their own data, they can improve generalization by changing exploration. In a previous work, my coauthors and I showed that pairing an existing RL algorithm with a stronger exploration strategy can double its generalization performance on Procgen without explicit regularization. In a more recent work, we found that better exploration also lets the model leverage more expressive model architectures and computational resources, and generalize better on Procgen as the result4. Many existing approaches for this benchmark treat the problem as a representation learning problem and apply regularization techniques adapted from supervised learning (e.g., dropout or data augmentation). These help, but they overlook exploration, one of the most important structural components of RL. Since the agents collect their own data, they can improve generalization by changing exploration. In a previous work, my coauthors and I showed that pairing an existing RL algorithm with a stronger exploration strategy can double its generalization performance on Procgen without explicit regularization. In a more recent work, we found that better exploration also lets the model leverage more expressive model architectures and computational resources, and generalize better on Procgen as the result While Procgen is certaintly not as difficult and complex as the problems LLMs are trained to solve today, the overall problem structure is essentially the same – the RL agent is trained on a finite set of problems and tested on new problems at test time without further training. The way we do exploration with LLMs today is fairly simple, typically limited to sampling from the model’s autoregressive distribution with tweaks to temperature or entropy bonus, so there is a large design space for potentially better exploration approaches. Admittedly, there have not been many successful examples in this direction. This could be because it is a very hard problem, it is not flop-efficient enough to be practical, or we just haven’t tried hard enough. However, if Procgen-style exploration gains do translate, we’re leaving efficiency – and perhaps entirely new capabilities – on the table. The next section discusses where we might look first. While Procgen is certaintly not as difficult and complex as the problems LLMs are trained to solve today, the overall problem structure is essentially the same – the RL agent is trained on a finite set of problems and tested on new problems at test time without further training. The way we do exploration with LLMs today is fairly simple, typically limited to sampling from the model’s autoregressive distribution with tweaks to temperature or entropy bonus, so there is a large design space for potentially better exploration approaches. Admittedly, there have not been many successful examples in this direction. This could be because it is a very hard problem, it is not flop-efficient enough to be practical, or we just haven’t tried hard enough. However, if Procgen-style exploration gains do translate, we’re leaving efficiency – and perhaps entirely new capabilities – on the table. The next section discusses where we might look first. Two axes of scaling exploration

Two axes of scaling exploration Exploration, in the broad sense I’m using here, is deciding what data the learner will see. That decision happens on two distinct axes:

Exploration, in the broad sense I’m using here, is deciding what data the learner will see. That decision happens on two distinct axes: • World sampling – deciding where to learn. World here refers to a particular problem that needs to be solved. In supervised learning (or unsupervised pretraining), this axis covers data collection, synthetic generation and curation: gathering and filtering raw documents, images, or code, each of which corresponds to a “world”. In RL, this corresponds to designing or generating environments, such as a single math puzzle or a coding problem. We can even arrange the worlds into curricula. In both cases, world sampling is fundamentally about what “data points” the learner is allowed to see. This also decides the limit on all the information any agent can possibly learn. World sampling – deciding where to learn. World here refers to a particular problem that needs to be solved. In supervised learning (or unsupervised pretraining), this axis covers data collection, synthetic generation and curation: gathering and filtering raw documents, images, or code, each of which corresponds to a “world”. In RL, this corresponds to designing or generating environments, such as a single math puzzle or a coding problem. We can even arrange the worlds into curricula. In both cases, world sampling is fundamentally about what “data points” the learner is allowed to see. This also decides the limit on all the information any agent can possibly learn. • Path sampling – deciding how to gather data inside a world. This step is unique to RL. Once a world is chosen, the agent still has to pick which trajectories to collect: random walks, curiosity‑driven policies, tree search, tool-use, etc. Different path‑sampling strategies can have different computation cost and produce very different training distributions even when the underlying world is identical. In short, path sampling is about what the learner “wants” to see. Path sampling – deciding how to gather data inside a world. This step is unique to RL. Once a world is chosen, the agent still has to pick which trajectories to collect: random walks, curiosity‑driven policies, tree search, tool-use, etc. Different path‑sampling strategies can have different computation cost and produce very different training distributions even when the underlying world is identical. In short, path sampling is about what the learner “wants” to see. In supervised learning or unsupervised pretraining, the second axis incurs a constant cost because a single forward (and backward) pass has access to all the information each data point contains (e.g., cross-entropy loss). Because there is no obvious way to “dig deeper” inside a single example (other than make the models larger), the exploration cost lives almost entirely on the first axis – world sampling. The flops can either go into acquire new worlds (e.g., new data points) or processing existing worlds (e.g., curation and synthetic data). In supervised learning or unsupervised pretraining, the second axis incurs a constant cost because a single forward (and backward) pass has access to all the information each data point contains (e.g., cross-entropy loss). Because there is no obvious way to “dig deeper” inside a single example (other than make the models larger), the exploration cost lives almost entirely on the first axis – world sampling. The flops can either go into acquire new worlds (e.g., new data points) or processing existing worlds (e.g., curation and synthetic data). In contrast, RL has much more flexibility in the second axis (in addition to the first axis). Because most random trajectories reveal little information about the ideal behavior, the information density (useful bits per flop) in RL is far lower than in supervised learning or pretraining. If we naïvely sample trajectory, we risk wasting flops on noise. So it’s imporant to be judicious about how we spend our flops5. There are also more options for spending flops to explore within each world. For example, we can either sample more trajectories from a single environment or we can spend more flops thinking about how to sample the next trajectory to discover high-value states and actions. In contrast, RL has much more flexibility in the second axis (in addition to the first axis). Because most random trajectories reveal little information about the ideal behavior, the information density (useful bits per flop) in RL is far lower than in supervised learning or pretraining. If we naïvely sample trajectory, we risk wasting flops on noise. So it’s imporant to be judicious about how we spend our flops. There are also more options for spending flops to explore within each world. For example, we can either sample more trajectories from a single environment or we can spend more flops thinking about how to sample the next trajectory to discover high-value states and actions. For most, if not all, machine learning problems, the high-level goal can be understood as maximizing information per flop. For that purpose, these two levers form a trade-off curve. If one spends too much resources on world sampling and not enough on path sampling, the agent may not extract any meaningful experience from the sampled worlds. Vice versa, if one spends too much resources on a small set of worlds, the agent could overfit to the training worlds and would not learn generalizable behavior that transfer across worlds. The ideal scenario happens somewhere in between where the resources are divided between sampling new worlds and running algorithms (i.e., better than random sampling) that can extract more information from a single world. For most, if not all, machine learning problems, the high-level goal can be understood as maximizing information per flop. For that purpose, these two levers form a trade-off curve. If one spends too much resources on world sampling and not enough on path sampling, the agent may not extract any meaningful experience from the sampled worlds. Vice versa, if one spends too much resources on a small set of worlds, the agent could overfit to the training worlds and would not learn generalizable behavior that transfer across worlds. The ideal scenario happens somewhere in between where the resources are divided between sampling new worlds and running algorithms (i.e., better than random sampling) that can extract more information from a single world. If you are familiar with scaling laws, what I just described sounds a lot like the Chinchilla scaling laws but the two axes correspond to the compute used for different types of sampling rather than parameters and data. At each performance level, one should to be able to trace out an isoperformance curve where the x-axis and y-axis are the compute put into interacting with any given environment and the compute given to the environments, whether it is for generating the environment or for running the environment (e.g., a generative verifier with CoT). If you are familiar with scaling laws, what I just described sounds a lot like the Chinchilla scaling laws but the two axes correspond to the compute used for different types of sampling rather than parameters and data. At each performance level, one should to be able to trace out an isoperformance curve where the x-axis and y-axis are the compute put into interacting with any given environment and the compute given to the environments, whether it is for generating the environment or for running the environment (e.g., a generative verifier with CoT). Of the two axes, path sampling is a relatively well-defined problem. A principled approach for doing exploration within an environment is to reduce the model’s uncertainty6. Many existing approaches for exploration have very strong sample complexity but they tend to be prohibitively expensive. Nonetheless, there is arguably a well-defined objective for path sampling and the main obstacle is to figure out a computationally efficient approximation. On the other hand, it is much less clear what the objective is for world sampling. One appealing idea is open-ended learning but even open-ended learning requires defining the universe of all environments (i.e., environment specs) or a subjective observer that judges whether an outcome is “interesting”. Of the two axes, path sampling is a relatively well-defined problem. A principled approach for doing exploration within an environment is to reduce the model’s uncertainty. Many existing approaches for exploration have very strong sample complexity but they tend to be prohibitively expensive. Nonetheless, there is arguably a well-defined objective for path sampling and the main obstacle is to figure out a computationally efficient approximation. On the other hand, it is much less clear what the objective is for world sampling. One appealing idea is open-ended learning but even open-ended learning requires defining the universe of all environments (i.e., environment specs) or a subjective observer that judges whether an outcome is “interesting”. What objective should world sampling optimize? The unfortunate reality (or fortunate, depending on your perspective) is that the space of environments is infinite, but our resources are finite. If we want to do something useful, then we must express some preference over the environments. I suspect that the problem of designing environments will eventually become similar to selecting pretraining data. It will be hard to say exactly why one environment will help another environment and we will need a lot of them. In other words, there may not be a single clean and nice objective for designing environment specs. What objective should world sampling optimize? The unfortunate reality (or fortunate, depending on your perspective) is that the space of environments is infinite, but our resources are finite. If we want to do something useful, then we must express some preference over the environments. I suspect that the problem of designing environments will eventually become similar to selecting pretraining data. It will be hard to say exactly why one environment will help another environment and we will need a lot of them. In other words, there may not be a single clean and nice objective for designing environment specs. The more likely scenario (probably already happening) is that everyone will start designing specs within their own expertise or domain of interest. When we have enough “human-approved” and “useful” specs, maybe we can try to learn some common principles and eventually automate the process by learning from them, much like how pretraining data is selected today. It would be inconvenient if we need the same amount of environments as pretraining data to achieve the same level of generality for decision making, but there are some preliminary evidence that this may not be the case. In a recent work, we found that a fairly small number of environments is enough to train an agent capable of general exploration and decision making in entirely out-of-distribution environments. Furthermore, using existing LLMs could also greatly accelerate the design process. The more likely scenario (probably already happening) is that everyone will start designing specs within their own expertise or domain of interest. When we have enough “human-approved” and “useful” specs, maybe we can try to learn some common principles and eventually automate the process by learning from them, much like how pretraining data is selected today. It would be inconvenient if we need the same amount of environments as pretraining data to achieve the same level of generality for decision making, but there are some preliminary evidence that this may not be the case. In a recent work, we found that a fairly small number of environments is enough to train an agent capable of general exploration and decision making in entirely out-of-distribution environments. Furthermore, using existing LLMs could also greatly accelerate the design process. Of course, these are all very high-level musings and how one exactly scales these two axes is much less obvious than scaling pretraining. However, if we were able to figure out a reliable way to introduce scale into world sampling, and a more intelligent way of path sampling, we should be able to see isoperformance curves that bends inwards towards the origin (maybe they won’t be as smooth). This style of scaling laws would teach us the best way to allocate computation resource between the environments and the agent. Of course, these are all very high-level musings and how one exactly scales these two axes is much less obvious than scaling pretraining. However, if we were able to figure out a reliable way to introduce scale into world sampling, and a more intelligent way of path sampling, we should be able to see isoperformance curves that bends inwards towards the origin (maybe they won’t be as smooth). This style of scaling laws would teach us the best way to allocate computation resource between the environments and the agent. Final thoughts

Final thoughts I could keep unfolding more tangents – better curiosity objectives, open-endedness, meta‑exploration that learns how to explore – but I think it is more important to make the high-level point clear. I could keep unfolding more tangents – better curiosity objectives open-endedness meta‑exploration that learns how to explore – but I think it is more important to make the high-level point clear. Existing paradigms of scaling have been incredibly effective but all paradigms will eventually saturate. The question is where to pour the next orders of magnitude of compute. I’ve argued that exploration – both world sampling and path sampling – offers a promising direction. We don’t yet know the right scaling laws, the right environment generators, or the right exploration objectives, but intuitively they should be possible. The coming years will decide whether exploration can stretch our flops further on top of the existing paradigms. I think the bet is worth making. Existing paradigms of scaling have been incredibly effective but all paradigms will eventually saturate. The question is where to pour the next orders of magnitude of compute. I’ve argued that exploration – both world sampling and path sampling – offers a promising direction. We don’t yet know the right scaling laws, the right environment generators, or the right exploration objectives, but intuitively they should be possible. The coming years will decide whether exploration can stretch our flops further on top of the existing paradigms. I think the bet is worth making. Acknowledgement

Acknowledgement Many thanks to Allan Zhou, Sam Sokota, Minqi Jiang, Ellie Haber, Alex Robey, Swaminathan Gurumurthy, Kevin Li, Calvin Luo, Abitha Thankaraj, and Zico Kolter for their feedback and discussion on the draft. Many thanks to Allan Zhou, Sam Sokota, Minqi Jiang, Ellie Haber, Alex Robey, Swaminathan Gurumurthy, Kevin Li, Calvin Luo, Abitha Thankaraj, and Zico Kolter for their feedback and discussion on the draft. • A valid alternative possibility is that the RL optimization objective does not work well with smaller models, but this is likely not the case because before LLMs most successful applications of RL involved very small models. ︎
A valid alternative possibility is that the RL optimization objective does not work well with smaller models, but this is likely not the case because before LLMs most successful applications of RL involved very small models. ︎

A valid alternative possibility is that the RL optimization objective does not work well with smaller models, but this is likely not the case because before LLMs most successful applications of RL involved very small models. • This doesn’t mean the model can full exploit this information because the model can be limited in its computation power. It just means that if it wants to, that information is fully available. ︎
This doesn’t mean the model can full exploit this information because the model can be limited in its computation power. It just means that if it wants to, that information is fully available. ︎

This doesn’t mean the model can full exploit this information because the model can be limited in its computation power. It just means that if it wants to, that information is fully available. • For generalization to be tractable, we must assume that a “good enough” policy exists for all environments. This is analogous to assuming there is small or no label noise in supervised learning. ︎
For generalization to be tractable, we must assume that a “good enough” policy exists for all environments. This is analogous to assuming there is small or no label noise in supervised learning. ︎

For generalization to be tractable, we must assume that a “good enough” policy exists for all environments. This is analogous to assuming there is small or no label noise in supervised learning. • At the time of writing, I believe this sets a new state-of-the-art performance on the “25M easy” benchmark of ProcGen. ︎
At the time of writing, I believe this sets a new state-of-the-art performance on the “25M easy” benchmark of ProcGen. ︎

At the time of writing, I believe this sets a new state-of-the-art performance on the “25M easy” benchmark of ProcGen. • Interestingly, for many problems such as Atari, random sampling works reasonably well. I think that says much more about the environments than the exploration method itself. ︎
Interestingly, for many problems such as Atari, random sampling works reasonably well. I think that says much more about the environments than the exploration method itself. ︎

Interestingly, for many problems such as Atari, random sampling works reasonably well. I think that says much more about the environments than the exploration method itself. • There is a wide family of RL algorithms under the names of posterior sampling or information-directed sampling that try to direct the exploration to reduce the model’s uncertainty, but they are generally too expensive to be done exactly at the scale of LLMs. Various approximations exist but they have not been widely used for LLMs to the best of my knowledge. ︎
There is a wide family of RL algorithms under the names of posterior sampling or information-directed sampling that try to direct the exploration to reduce the model’s uncertainty, but they are generally too expensive to be done exactly at the scale of LLMs. Various approximations exist but they have not been widely used for LLMs to the best of my knowledge. ︎

There is a wide family of RL algorithms under the names of posterior sampling information-directed sampling that try to direct the exploration to reduce the model’s uncertainty, but they are generally too expensive to be done exactly at the scale of LLMs. Various approximations exist but they have not been widely used for LLMs to the best of my knowledge.","## The Era of Exploration

- **Large Language Models (LLMs)**
  - Result from three decades of freely accessible human text online.
  - Ilya Sutskever compares this information reservoir to **fossil fuel**: abundant but finite.
  - Studies suggest that frontier labs may exhaust high-quality English web text before the decade ends.
  - Current models consume data faster than humans can produce it.

- **Era of Experience**
  - Coined by David Silver and Richard Sutton.
  - Progress depends on data generated by learning agents themselves.
  - The bottleneck is not just any experience, but the **right kind of experience** that benefits learning.
  - Future AI progress will focus on **exploration**: acquiring new and informative experiences.

- **Cost of Experience Collection**
  - Scaling involves resources such as:
    - Compute cycles
    - Synthetic-data generation
    - Data curation pipelines
    - Human oversight
    - Any expenditure that creates learning signals.
  - All costs are simplified into a unit called **flops**:
    - A flop is one floating-point operation.
    - Used as a common currency to measure effort consumed by systems.
    - Discussion focuses on relative spend, not specific resources.

- **Exploration in Data-Driven Systems**
  - Exploration is crucial for every data-driven system to decide which experiences to collect.
  - Broader definition of exploration beyond reinforcement learning (RL).
  - Inspired by Minqi’s article: **General intelligence requires rethinking exploration**.

- **Post Organization**
  - The following sections will cover:
    1. How pre-training solved part of the exploration problem.
    2. Why better exploration leads to better generalization.
    3. Where to allocate the next hundred thousand GPU-years.

- **Pretraining as Exploration**
  - Standard LLM pipeline:
    - Pretrain a large model on next-token prediction using extensive text.
    - Fine-tune the model with RL for desired objectives.
  - Without pretraining, RL struggles to progress.
  - Smaller models can improve reasoning through distillation from larger models, suggesting:
    - Large scale is not a prerequisite for effective reasoning.
    - However, the need for distillation indicates that model capacity is not the only factor.
  - **Exploration Tax**:
    - Pretraining incurs a significant upfront cost to enhance exploration capabilities.
    - Models without pretraining find it harder to explore the solution space effectively.
    - Pretraining invests in diverse data to learn a rich sampling distribution for correct continuations.
    - Distillation allows smaller models to inherit exploration capabilities from larger models.

## The Era of Exploration

### Importance of Pre-Paid Exploration
- **Smaller pretrained models** struggle to explore the solution space effectively.
- **Pretraining** involves significant compute resources to learn a rich sampling distribution for likely correct continuations.
- **Distillation** allows smaller models to inherit the exploration capabilities from larger models.

### Reinforcement Learning (RL) Loop
- The general RL loop consists of:
  - **Exploration**: The agent generates randomized exploration trajectories.
  - **Reinforce**: Good trajectories are up-weighted; bad ones are down-weighted.
- For effective learning:
  - The agent must generate a minimal number of **“good” trajectories** during exploration (known as **coverage** in RL).
  - In **Large Language Models (LLMs)**, exploration is achieved through sampling from the model’s autoregressive output distribution.
  - Correct solutions must be likely in the naive sampling distribution.
  - Lower-capacity models may struggle to find valid solutions through random sampling, leading to ineffective reinforcement.

### Challenges of Exploration
- Exploration without prior information is difficult:
  - In tabular RL, every state and action can be listed, but learning requires many trials.
  - A known lower-bound on sample complexity is: 
    - \(\Omega(\frac{SAH^2}{\epsilon^2})\) (Dann & Brunskill, 2015)
      - Where:
        - **S** = size of the state space
        - **A** = size of the action space
        - **H** = horizon
        - **ε** = distance to the best solution
  - Minimum episodes grow linearly with state-action pairs and quadratically with the horizon.
  - For LLMs, the state space includes all possible text prefixes, and the action space includes any next token, making exploration nearly impossible without prior information.

### Role of Pretraining
- Pretraining has largely handled the exploration challenge by learning a better prior for sampling trajectories.
- However, this constrains the types of trajectories that can be sampled naively.
- To advance, we need to explore beyond the prior.

### Exploration and Generalization
- Historically, RL research focused on single environments (e.g., Atari, MuJoCo):
  - This is akin to training and testing on the same data point.
  - Performance in a single environment does not indicate how well a model can handle novel situations.
- **Generalization** is crucial in machine learning:
  - Success on unseen problems is more important than solving known issues.
- Generalization performance is critical for LLMs:
  - During training, LLMs see a limited set of prompts but must handle diverse user queries at deployment.
  - Current LLMs perform well on tasks with verifiable rewards (e.g., coding puzzles) but struggle with ambiguous tasks (e.g., writing a novel).

### Options for Training Generalizable Models
- **Data diversity** is key for robust generalization in deep learning.
- Exploration influences data diversity:
  - In supervised learning, each labeled example reveals all details in one pass, requiring more data for diversity.
  - In RL, each interaction reveals a narrow slice of the environment.
  - Agents must collect varied trajectories to build a representative picture.
  - Lack of diversity in collected trajectories can lead to overfitting within the same environment.

## The Era of Exploration

### Supervised Learning vs. Reinforcement Learning (RL)
- **Supervised Learning**: 
  - Labeled examples reveal all details in a single forward pass.
  - Data diversity can only be increased by collecting more data.

- **Reinforcement Learning (RL)**: 
  - Each interaction exposes a narrow slice of the environment.
  - Agents must gather varied trajectories to build a representative picture.
  - Lack of diversity in trajectories (e.g., naive random sampling) can lead to overfitting.

### Challenges with Multiple Environments
- **Procgen Benchmark**: 
  - A collection of Atari-like games with procedurally generated environments.
  - Each game theoretically contains “infinitely” many environments.
  - Objective: Train on a fixed number of environments and generalize to unseen ones.

### Current Approaches and Limitations
- Many approaches treat the problem as a **representation learning** issue:
  - Apply regularization techniques from supervised learning (e.g., dropout, data augmentation).
  - These techniques help but often overlook **exploration**, a crucial component of RL.

- **Exploration Strategies**: 
  - Agents can improve generalization by changing exploration methods.
  - Previous work showed that pairing RL algorithms with stronger exploration strategies can:
    - Double generalization performance on Procgen without explicit regularization.
    - Allow models to leverage more expressive architectures and resources.

### Exploration in LLMs
- While Procgen is simpler than current LLM challenges, the problem structure is similar:
  - RL agents are trained on a finite set of problems and tested on new ones without further training.
- Current exploration methods for LLMs are basic:
  - Typically involve sampling from the model’s autoregressive distribution with tweaks (e.g., temperature, entropy bonus).
  - There is significant potential for better exploration approaches.

- **Challenges in Exploration**: 
  - Few successful examples of improved exploration strategies.
  - Possible reasons for limited success:
    - Difficulty of the problem.
    - Inefficiency in computational resources.
    - Lack of effort in exploring new methods.
  - If Procgen-style exploration gains translate, we may be missing out on efficiency and new capabilities.

### Two Axes of Scaling Exploration
- **Exploration**: Deciding what data the learner will see occurs on two axes:
  1. **World Sampling**:
     - Refers to the specific problem to be solved.
     - In supervised learning, it involves data collection, synthetic generation, and curation.
     - In RL, it involves designing or generating environments (e.g., math puzzles, coding problems).
     - Arranging worlds into curricula is also possible.
     - Determines the limit on the information any agent can learn.
  2. **Path Sampling**:
     - Unique to RL; involves deciding how to gather data within a chosen world.
     - Agents select trajectories to collect (e.g., random walks, curiosity-driven policies, tree search, tool-use).
     - Different strategies incur varying computational costs and produce different training distributions.
     - Essentially, it’s about what the learner “wants” to see.

- In supervised learning or unsupervised pretraining, the second axis incurs a constant cost:
  - A single forward (and backward) pass accesses all information in each data point.
  - Exploration cost primarily resides in the first axis (world sampling).
  - Computational resources can be allocated to acquiring new worlds or processing existing ones.

## The Era of Exploration

- **Supervised Learning vs. Reinforcement Learning (RL)**
  - In supervised learning or unsupervised pretraining:
    - The second axis incurs a **constant cost** due to access to all information in each data point (e.g., **cross-entropy loss**).
    - Exploration cost primarily resides on the **first axis** – **world sampling**.
    - Flops can be allocated to:
      - Acquiring new worlds (new data points)
      - Processing existing worlds (curation and synthetic data)

- **Flexibility in RL**
  - RL offers more flexibility on both axes:
    - Random trajectories often reveal little about ideal behavior.
    - Information density (useful bits per flop) in RL is lower than in supervised learning or pretraining.
    - Naïve trajectory sampling risks wasting flops on noise.
    - Important to be judicious in spending flops:
      - Options include:
        - Sampling more trajectories from a single environment.
        - Allocating flops to strategize the next trajectory for discovering high-value states and actions.

- **Maximizing Information per Flop**
  - High-level goal in machine learning:
    - Maximize information per flop.
    - Two levers create a **trade-off curve**:
      - Excessive resources on world sampling without path sampling may yield no meaningful experience.
      - Over-investing in a small set of worlds may lead to overfitting and lack of generalizable behavior.
    - Ideal scenario:
      - Balanced resource allocation between sampling new worlds and running algorithms to extract more information from a single world.

- **Chinchilla Scaling Laws**
  - Similar to Chinchilla scaling laws:
    - Two axes correspond to compute used for different types of sampling rather than parameters and data.
    - At each performance level, trace an **isoperformance curve**:
      - X-axis: Compute for interacting with environments.
      - Y-axis: Compute for generating or running environments (e.g., generative verifier with CoT).

- **Path Sampling vs. World Sampling**
  - Path sampling is a well-defined problem:
    - Objective: Reduce model uncertainty.
    - Existing exploration approaches have strong sample complexity but can be expensive.
  - World sampling objectives are less clear:
    - Open-ended learning requires defining the universe of environments or a subjective observer to judge interesting outcomes.

- **Challenges in World Sampling**
  - Objective for world sampling:
    - The space of environments is **infinite**, but resources are **finite**.
    - Must express preferences over environments for useful outcomes.
    - Designing environments may resemble selecting pretraining data:
      - Hard to determine why one environment aids another.
      - Need a variety of environments.
  - Likely scenario:
    - Designing specs within individual expertise or domain of interest.
    - Accumulating enough “human-approved” and “useful” specs may lead to learning common principles and automating the process, similar to pretraining data selection.
    - Preliminary evidence suggests fewer environments may suffice for achieving generality in decision-making.

## The Era of Exploration

- **Objective**: Train an agent for general exploration and decision-making in out-of-distribution environments.

- **Design Process Acceleration**: 
  - Utilizing existing **Large Language Models (LLMs)** can significantly speed up the design process.
  - Anticipated trend: Individuals will design specifications within their own areas of expertise.

- **Learning from Specifications**: 
  - Once enough ""human-approved"" and ""useful"" specifications are gathered, we can:
    - Identify common principles.
    - Automate the design process, similar to current pretraining data selection.

- **Generalization Concerns**: 
  - It may be inconvenient to require as many environments as pretraining data for achieving similar decision-making generality.
  - Preliminary evidence suggests that a small number of environments can suffice for training agents in out-of-distribution scenarios.

- **Scaling Challenges**: 
  - Scaling exploration and decision-making is less straightforward than scaling pretraining.
  - Potential solutions include:
    - Reliable methods for **world sampling**.
    - Intelligent approaches for **path sampling**.
  - Expected outcome: Isoperformance curves that bend inward towards the origin, indicating efficient resource allocation between environments and agents.

## Final Thoughts

- **Exploration as a Key Focus**: 
  - While many tangents (e.g., better curiosity objectives, open-endedness, meta-exploration) could be explored, the main point is:
    - Existing scaling paradigms are effective but will eventually reach saturation.
    - The next focus should be on where to allocate additional compute resources.

- **Promising Directions**: 
  - Exploration, specifically world and path sampling, is a promising avenue for future research.
  - Current unknowns include:
    - Appropriate scaling laws.
    - Effective environment generators.
    - Suitable exploration objectives.
  - The coming years will determine if exploration can enhance computational efficiency beyond existing paradigms.

## Acknowledgements

- Special thanks to:
  - Allan Zhou
  - Sam Sokota
  - Minqi Jiang
  - Ellie Haber
  - Alex Robey
  - Swaminathan Gurumurthy
  - Kevin Li
  - Calvin Luo
  - Abitha Thankaraj
  - Zico Kolter
- Their feedback and discussions were invaluable in shaping this draft.

## Additional Insights

- **Alternative Possibilities**: 
  - The RL optimization objective may not perform well with smaller models, but this is unlikely since prior successful RL applications involved small models.

- **Information Availability**: 
  - Models may not fully exploit available information due to computational limitations, but the information remains accessible if desired.

- **Generalization Assumption**: 
  - For generalization to be feasible, a ""good enough"" policy must exist for all environments, akin to the assumption of minimal label noise in supervised learning.

- **Performance Benchmark**: 
  - Current work reportedly sets a new state-of-the-art performance on the ""25M easy"" benchmark of **ProcGen**.

- **Random Sampling Effectiveness**: 
  - Random sampling works reasonably well for many problems (e.g., **Atari**), indicating more about the environments than the exploration methods.

- **Exploration Algorithms**: 
  - A variety of RL algorithms, such as **posterior sampling** or **information-directed sampling**, aim to reduce model uncertainty during exploration.
  - However, these methods are generally too costly for implementation at the scale of LLMs, and existing approximations have not been widely adopted.",0.3626877470355731
https://community.coda.io/t/the-chatbot-is-dead-long-live-the-orchestrator/56357,The Chatbot is Dead. Long Live the Orchestrator,Artificial Intelligence,4831,2025-07-08T16:23:54.579836,community.coda.io,Amit Shah,U03BD032PJ5,2025-07-01T21:00:42.569159,,article,,,"preload-content:

The Chatbot is Dead. Long Live the Orchestrator

The Chatbot is Dead. Long Live the Orchestrator Bill_French July 1, 2025, 3:20pm While the world was distracted by talking dolls, Grammarly quietly built an AI that could act. They didn’t use a better model. They used a better weapon: Coda. While the world was distracted by talking dolls, Grammarly quietly built an AI that could act. They didn’t use a better model. They used a better weapon: Coda. Your Prompts Are a Prayer to an Amnesiac God

Your Prompts Are a Prayer to an Amnesiac God Let’s be brutally honest. The entire AI industry is captivated by a lie. We’ve anointed “prompt engineering” as a mystical art, a high priesthood for coaxing wisdom from silicon gods. It’s a sham. We are meticulously polishing the conversational skills of a machine with terminal amnesia. Let’s be brutally honest. The entire AI industry is captivated by a lie. We’ve anointed “prompt engineering” as a mystical art, a high priesthood for coaxing wisdom from silicon gods. It’s a sham. We are meticulously polishing the conversational skills of a machine with terminal amnesia. We were promised an omniscient partner, an AI co-pilot. What we got was a brilliant intern with no long-term memory, an entity we must re-brief from scratch every five minutes. This isn’t productivity. It’s digital babysitting for a machine that can recite Shakespeare but struggles to remember your name or perform precise calculations. We were promised an omniscient partner, an AI co-pilot. What we got was a brilliant intern with no long-term memory, an entity we must re-brief from scratch every five minutes. This isn’t productivity. It’s digital babysitting for a machine that can recite Shakespeare but struggles to remember your name or perform precise calculations. Today’s large language models are amnesiacs by design. We celebrate their ballooning context windows—a million, two million tokens—as if it were memory. It’s not. It’s a bigger notepad. A volatile transcript that dissolves into the ether the moment you close the tab. This architecture condemns us to a state of digital shrapnel: your project plan lives in Slack, your research is scattered across browser tabs, your decisions are buried in email, and your draft is in a doc. We ask our AI to be intelligent, but we force it to operate blindfolded, guessing the shape of our work by touching one disconnected piece at a time. Today’s large language models are amnesiacs by design. We celebrate their ballooning context windows—a million, two million tokens—as if it were memory. It’s not. It’s a bigger notepad. A volatile transcript that dissolves into the ether the moment you close the tab. This architecture condemns us to a state of digital shrapnel: your project plan lives in Slack, your research is scattered across browser tabs, your decisions are buried in email, and your draft is in a doc. We ask our AI to be intelligent, but we force it to operate blindfolded, guessing the shape of our work by touching one disconnected piece at a time. This makes you the AI’s external hard drive. You are the connective tissue. You perform the soul-crushing labor of copying, pasting, and re-explaining context, bridging the gap with every single query. This isn’t a feature. It’s a catastrophic, unforgivable design flaw. This makes you the AI’s external hard drive. You are the connective tissue. You perform the soul-crushing labor of copying, pasting, and re-explaining context, bridging the gap with every single query. This isn’t a feature. It’s a catastrophic, unforgivable design flaw. Stop Describing. Start Commanding. Stop Describing. Start Commanding. For a moment, I thought the answer was contexting—the architectural discipline of curating a rich data environment for an AI agent. It was the right instinct, but the wrong verb. It was a step away from the vacant art of prompting, but it was still just talking at the machine. For a moment, I thought the answer was contexting—the architectural discipline of curating a rich data environment for an AI agent. It was the right instinct, but the wrong verb. It was a step away from the vacant art of prompting, but it was still just talking at the machine. You cannot build a skyscraper by describing it to a pile of bricks, no matter how eloquently. You need an architectural plan, a crane, and a crew. Prompting is the description. Coda is the crane and the blueprint. It transforms your context from a static pile of information into a dynamic set of executable instructions. You cannot build a skyscraper by describing it to a pile of bricks, no matter how eloquently. You need an architectural plan, a crane, and a crew. Prompting is the description. Coda is the crane and the blueprint. It transforms your context from a static pile of information into a dynamic set of executable instructions. You cannot build a skyscraper by describing it to a pile of bricks, no matter how eloquently. You need an architectural plan, a crane, and a crew. Prompting is the description. Coda is the crane and the blueprint. It transforms your context from a static pile of information into a dynamic set of executable instructions. The true frontier isn’t what an AI knows. It’s what it can do. This demands a new class of software: an Agentic Orchestration Framework. A system that doesn’t just talk, but commands, coordinates, and executes. It directs multiple specialized agents—AI, automation, and human—across complex, multi-step workflows with unwavering precision. The true frontier isn’t what an AI knows. It’s what it can do. This demands a new class of software: an Agentic Orchestration Framework. A system that doesn’t just talk, but commands, coordinates, and executes. It directs multiple specialized agents—AI, automation, and human—across complex, multi-step workflows with unwavering precision. The archetype for this framework has been hiding in plain sight: Coda. The archetype for this framework has been hiding in plain sight: Forget the “all-in-one doc” marketing. That was the Trojan horse. Coda is a workflow engine for manufacturing bespoke, agentic software without writing a line of code. Its architecture is the very blueprint for orchestration:

Forget the “all-in-one doc” marketing. That was the Trojan horse. Coda is a workflow engine for manufacturing bespoke, agentic software without writing a line of code. Its architecture is the very blueprint for orchestration: • Docs as the Command Center: The unified surface where human intent and AI execution converge. Docs as the Command Center: The unified surface where human intent and AI execution converge. • Structured Tables as the Memory: A shared, persistent “brain” that provides unwavering context, rendering the amnesiac chat log obsolete. Structured Tables as the Memory: A shared, persistent “brain” that provides unwavering context, rendering the amnesiac chat log obsolete. • Packs as the Limbs: The API-driven connectors that give agents power over the real world—to manipulate Google Calendar, create Jira tickets, or rewrite Salesforce records. Packs as the Limbs: The API-driven connectors that give agents power over the real world—to manipulate Google Calendar, create Jira tickets, or rewrite Salesforce records. • Automations as the Nervous System: The rule-based engine that triggers actions and executes entire workflows with inhuman speed and reliability. Automations as the Nervous System: The rule-based engine that triggers actions and executes entire workflows with inhuman speed and reliability. CleanShot 2025-07-01 at 09.14.43@2x1274×842 43.6 KB

CleanShot 2025-07-01 at 09.14.43@2x 1274×842 43.6 KB This was never a document app. It’s a factory for building intelligent actors. This was never a document app. It’s a factory for building intelligent actors. The Grammarly Gambit: An Empire in Two Moves

The Grammarly Gambit: An Empire in Two Moves While the market obsessed over chatbot demos and press releases, Grammarly executed a two-step strategic coup to build the world’s first true agentic productivity platform. Anyone who saw these as unrelated acquisitions wasn’t just missing the story; they were illiterate in the language of power. While the market obsessed over chatbot demos and press releases, Grammarly executed a two-step strategic coup to build the world’s first true agentic productivity platform. Anyone who saw these as unrelated acquisitions wasn’t just missing the story; they were illiterate in the language of power. Move 1 (Acquire the Brain): Seize the Orchestration Framework
In late 2024, Grammarly acquired Coda. They didn’t buy a popular doc app. They bought the operating system for their future AI agents. This was the foundational act of war. As undeniable proof, Coda founder Shishir Mehrotra wasn’t just given a board seat; he was installed as Grammarly’s new CEO. He isn’t running a company; he is performing a hostile takeover of its DNA, injecting Coda’s agentic framework into a platform with 40 million daily active users. Move 1 (Acquire the Brain): Seize the Orchestration Framework
In late 2024, Grammarly acquired Coda. They didn’t buy a popular doc app. They bought the operating system for their future AI agents. This was the foundational act of war. As undeniable proof, Coda founder Shishir Mehrotra wasn’t just given a board seat; he was installed as Grammarly’s new CEO. He isn’t running a company; he is performing a hostile takeover of its DNA, injecting Coda’s agentic framework into a platform with 40 million daily active users. Move 2 (Conquer the Battlefield): Seize the Critical Interface
Months later, Grammarly acquired Superhuman. This was not about adding a slick email client. Superhuman is what Mehrotra calls the “perfect staging ground for orchestrating multiple AI agents simultaneously.” Email is the chaotic nexus where work, communication, and tasks collide. Grammarly didn’t buy Superhuman for its pathetic summarization features; they bought the most valuable turf in professional life to serve as the GUI for their Coda-powered agentic backend. Imagine it: A sales agent, a support agent, and a scheduling agent collaborating within a single email draft, orchestrated by the Coda engine, pulling live context from connected Packs, executing tasks across a dozen SaaS apps. That is the gambit. Move 2 (Conquer the Battlefield): Seize the Critical Interface
Months later, Grammarly acquired Superhuman. This was not about adding a slick email client. Superhuman is what Mehrotra calls the “perfect staging ground for orchestrating multiple AI agents simultaneously.” Email is the chaotic nexus where work, communication, and tasks collide. Grammarly didn’t buy Superhuman for its pathetic summarization features; they bought the most valuable turf in professional life to serve as the GUI for their Coda-powered agentic backend. Imagine it: A sales agent, a support agent, and a scheduling agent collaborating within a single email draft, orchestrated by the Coda engine, pulling live context from connected Packs, executing tasks across a dozen SaaS apps. That is the gambit. CleanShot 2025-07-01 at 09.17.51@2x1434×992 78 KB

CleanShot 2025-07-01 at 09.17.51@2x 1434×992 78 KB

Your AI Stack is a Museum Piece

Your AI Stack is a Museum Piece The chasm between the dying paradigm and the emerging one is not an increment; it is a cliff. One is a toy, the other is a weapon. One talks, the other acts. The stack Grammarly is building does not compete with the old one. It renders it irrelevant. The chasm between the dying paradigm and the emerging one is not an increment; it is a cliff. One is a toy, the other is a weapon. One talks, the other acts. The stack Grammarly is building does not compete with the old one. It renders it irrelevant. The Moat Isn’t the Model; It’s the Machine

The Moat Isn’t the Model; It’s the Machine The winners of the AI war will not be the companies with the largest language model. That is a commodity race to the bottom. They will be the ones who own the orchestration framework that makes those models act. Building a better chatbot today is like perfecting the horse-drawn carriage in the age of the automobile. The real innovation was the assembly line and the highway system. The only defensible moat is the machine: the framework that enables action, the integrations that give it reach, the structured data that serves as its memory, and the user workflows that become its territory. The winners of the AI war will not be the companies with the largest language model. That is a commodity race to the bottom. They will be the ones who own the orchestration framework that makes those models act. Building a better chatbot today is like perfecting the horse-drawn carriage in the age of the automobile. The real innovation was the assembly line and the highway system. The only defensible moat is the machine: the framework that enables action, the integrations that give it reach, the structured data that serves as its memory, and the user workflows that become its territory. Stop asking if your AI is smart. Start demanding that it act. Stop celebrating prompts. Start building engines of execution. Stop asking if your AI is smart. Start demanding that it act. Stop celebrating prompts. Start building engines of execution. The future of work isn’t a conversation. It’s a command. The companies that understand this are building empires. The rest are polishing tombstones. The future of work isn’t a conversation. It’s a command. The companies that understand this are building empires. The rest are polishing tombstones. ps. A warm welcome to Rahul and the SuperHuman team.

ps. A warm welcome to Rahul and the SuperHuman team. 15 Likes Exciting News: Grammarly to acquire Superhuman Christiaan_Huizer July 1, 2025, 4:13pm thx for sharing your ideas. interesting & promising.
what do you believe would be a next tool to acquire @Bill_French?

thx for sharing your ideas. interesting & promising.
what do you believe would be a next tool to acquire @Bill_French 2 Likes Melanie_Teh July 1, 2025, 4:26pm my $$ is on reclaim or motion… something along the lines of calendar/task management

my $$ is on reclaim or motion… something along the lines of calendar/task management edit: oh, cannot be reclaim as they got bought by dropbox

edit: oh, cannot be reclaim as they got bought by dropbox 1 Like Bill_French July 1, 2025, 4:30pm I’m uncertain, but I have to believe someone at Grammarly (who is apparently really dialed in to the way I think) is watching Pieces, Flowith, and Dia. I’m uncertain, but I have to believe someone at Grammarly (who is apparently really dialed in to the way I think) is watching Pieces Flowith 5 Likes Agile_Dynamics July 2, 2025, 11:21pm Shishir keeps talking about ‘surfaces’. Email is the most frequent surface for Grammarly usage (=>Superhuman)
Documents and sheets are another major surface (=>Coda)

Shishir keeps talking about ‘surfaces’. Email is the most frequent surface for Grammarly usage (=>Superhuman)
Documents and sheets are another major surface (=>Coda) So whats the next-biggest surface where we spend our time? So whats the next-biggest surface where we spend our time? How about messaging and chats and forums? How about messaging and chats and forums? So maybe Grammarly has it’s eye on one of those? Not the major players maybe, but a startup with oodles of innovation and AI expertise? So maybe Grammarly has it’s eye on one of those? Not the major players maybe, but a startup with oodles of innovation and AI expertise? Another key criteria for these acquisitions is a shared vision. It was highlighted during the Coda deal. It’s highlighted again by Shishir and Rahul during the Superhuman deal. Another key criteria for these acquisitions is a shared vision. It was highlighted during the Coda deal. It’s highlighted again by Shishir and Rahul during the Superhuman deal. So the next target will have a messaging surface, an AI focus, and a clear vision that matches that of Shashir, Rahul, and their teams. So the next target will have a messaging surface, an AI focus, and a clear vision that matches that of Shashir, Rahul, and their teams. I dont know the marketplace well enough to identify candidates, but perhaps someone reading this does? I dont know the marketplace well enough to identify candidates, but perhaps someone reading this does? Just a thought. Just a thought. 2 Likes Nina_Ledid July 3, 2025, 9:03am As always, I’ve greatly enjoyed reading your perspective, @Bill_French, thanks for sharing! As always, I’ve greatly enjoyed reading your perspective, @Bill_French, thanks for sharing! I do remember from past conversations your concerns about Coda Pack’s limited support for common integration patterns (eg handling incoming webhooks, running persistent background services, or responding to external events)

I do remember from past conversations your concerns about Coda Pack’s limited support for common integration patterns (eg handling incoming webhooks, running persistent background services, or responding to external events) In your post above, you envision Coda as the blueprint for a new kind of orchestration framework. In your post above, you envision Coda as the blueprint for a new kind of orchestration framework. Do you think what once seemed like a limitation (Packs’ closed and controlled architecture) is now less of a drawback? Do you think what once seemed like a limitation (Packs’ closed and controlled architecture) is now less of a drawback? Or do you even see it potentially becoming a competitive advantage as the focus shifts from open interoperability to orchestrated execution? Or do you even see it potentially becoming a competitive advantage as the focus shifts from open interoperability to orchestrated execution? Thanks,
Nina

Thanks,
4 Likes Stefan_Stoyanov July 5, 2025, 8:17pm The reason I’m not yet bullish on Coda’s future is due to one key problem that LLMs still don’t solve particularly well: data collection. Specifically, I’ve been thinking a lot about the following limitations of Coda, as I understand its vision going forward:

The reason I’m not yet bullish on Coda’s future is due to one key problem that LLMs still don’t solve particularly well: data collection. Specifically, I’ve been thinking a lot about the following limitations of Coda, as I understand its vision going forward: • Collecting data by typing into Coda’s quadrangle notes (cells) is too slow and impractical. Instead, data should come from voice/video meetings and recordings, mobile/desktop screen activity, linked/shared content sources, mobile phone conversations, and visual/audible/touch perception input devices. Pushing data from hardware devices in real time becomes even more critical than pulling it from structured database, because if data isn’t captured quickly enough, the use of the database can become unreliable and irrelevant even within a minute. Collecting data by typing into Coda’s quadrangle notes (cells) is too slow and impractical. Instead, data should come from voice/video meetings and recordings, mobile/desktop screen activity, linked/shared content sources, mobile phone conversations, and visual/audible/touch perception input devices. Pushing data from hardware devices in real time becomes even more critical than pulling it from structured database, because if data isn’t captured quickly enough, the use of the database can become unreliable and irrelevant even within a minute. • Email is no longer a practical communication channel - it belongs to the past. As someone who has long supported email, it’s hard for me to admit this, but emails are now like the written contracts of a previous era. Today, very little needs to be formally written when people prefer adaptable, relevant content in audio/video formats. Email is no longer a practical communication channel - it belongs to the past. As someone who has long supported email, it’s hard for me to admit this, but emails are now like the written contracts of a previous era. Today, very little needs to be formally written when people prefer adaptable, relevant content in audio/video formats. • The joint venture company’s language support is essentially limited to English at the moment. In an era of LLMs—where tokenization of language is foundational to progress—this is concerning. Many LLMs still perform poorly in languages other than English. What does it mean to build a great app for the UK but a terrible one for France, simply because UX in the UK relies on quick chat-based communication, while in France it might depend on elaborate instructions—or worse, traditional filters and search? The joint venture company’s language support is essentially limited to English at the moment. In an era of LLMs—where tokenization of language is foundational to progress—this is concerning. Many LLMs still perform poorly in languages other than English. What does it mean to build a great app for the UK but a terrible one for France, simply because UX in the UK relies on quick chat-based communication, while in France it might depend on elaborate instructions—or worse, traditional filters and search? My understanding of LLMs and Coda’s vision is limited, so I’d really appreciate the community’s thoughts on these points. My understanding of LLMs and Coda’s vision is limited, so I’d really appreciate the community’s thoughts on these points. 1 Like Christiaan_Huizer July 6, 2025, 11:53am HI @Stefan_Stoyanov

@Stefan_Stoyanov Thanks for sharing your thoughts, those are valid concerns. Just so you know, I don’t have access to any internal “cookbook” or strategic plans. These are simply my best guesses and interpretations based on what’s publicly available and general market trends. Thanks for sharing your thoughts, those are valid concerns. Just so you know, I don’t have access to any internal “cookbook” or strategic plans. These are simply my best guesses and interpretations based on what’s publicly available and general market trends. Regarding your first question about data collection, there are already many AI-powered tools out there that are really good at capturing voice from meetings, whether you’re in the room or online. These tools can pull out and organize key information, remarks, and action items from conversations. When we think about the “surfaces” and “orchestration” Grammarly is building, meeting platforms are a big communication area. It makes sense that Grammarly might look to acquire a company in this field. They’d likely be looking for strong AI features, existing connections to other tools, potential for their “agents” to expand, and solid support for many languages. Companies like Sembly AI, Fireflies.ai, Otter.ai, or MeetGeek come to mind. Regarding your first question about data collection, there are already many AI-powered tools out there that are really good at capturing voice from meetings, whether you’re in the room or online. These tools can pull out and organize key information, remarks, and action items from conversations. When we think about the “surfaces” and “orchestration” Grammarly is building, meeting platforms are a big communication area. It makes sense that Grammarly might look to acquire a company in this field. They’d likely be looking for strong AI features, existing connections to other tools, potential for their “agents” to expand, and solid support for many languages. Companies like Sembly AI Fireflies.ai Otter.ai MeetGeek come to mind. As for your comment on email, that’s an an interesting thought. While other ways of communicating have certainly grown, email is still a core, if not the core, communication hub for many businesses. That’s actually why Grammarly bought Superhuman. It’s not just about email itself; it’s also about the related areas Superhuman is working on, like chats, calendars, and tasks (direct overlap with Coda). The whole idea is that all these interactions can be managed and brought together within Coda, building out an AI-powered productivity system that covers all the main places where work happens. As for your comment on email, that’s an an interesting thought. While other ways of communicating have certainly grown, email is still a core, if not core, communication hub for many businesses. That’s actually why Grammarly bought Superhuman. It’s not just about email itself; it’s also about the related areas Superhuman is working on, like chats, calendars, and tasks (direct overlap with Coda). The whole idea is that all these interactions can be managed and brought together within Coda, building out an AI-powered productivity system that covers all the main places where work happens. Finally, on your point about language support for LLMs, Superhuman already includes AI-driven translation right within its email platform. This feature allows for easy translation of messages, which is super important for international teams and global operations. This capability directly tackles language barriers in a crucial communication space, showing that Grammarly is focused on making its AI tools truly effective for a wide range of users worldwide. Finally, on your point about language support for LLMs, Superhuman already includes AI-driven translation right within its email platform. This feature allows for easy translation of messages, which is super important for international teams and global operations. This capability directly tackles language barriers in a crucial communication space, showing that Grammarly is focused on making its AI tools truly effective for a wide range of users worldwide. welcoming further feedback.

welcoming further feedback. Chers, christiaan

Chers, christiaan 3 Likes Bill_French July 7, 2025, 6:05pm Indeed. It’s why I use Pieces. It captures everything, and I can use it as a long-term memory or a funnel into Coda and other tools. Capturing contexts is probably where Grammarly is heading. Indeed. It’s why I use Pieces. It captures everything, and I can use it as a long-term memory or a funnel into Coda and other tools. Capturing contexts is probably where Grammarly is heading. For approximately 2.4 billion people, it remains practical in most cases. It’s not going anywhere, at least not in your lifetime. If you see ‘email’ when you see Superhuman, you need to recalibrate. The AI engine in this tool was perfected in one of the harshest environments, but it is applicable in many operational domains. They didn’t buy Superhuman to support email better. For approximately 2.4 billion people, it remains practical in most cases. It’s not going anywhere, at least not in your lifetime. If you see ‘email’ when you see Superhuman, you need to recalibrate. The AI engine in this tool was perfected in one of the harshest environments, but it is applicable in many operational domains. They didn’t buy Superhuman to support email better. This is not Grammarly’s problem to solve. I think some of the higher-level executives and engineers have seen solutions that will address the multilingual challenges. This is not Grammarly’s problem to solve. I think some of the higher-level executives and engineers have seen solutions that will address the multilingual challenges. 1 Like Bill_French July 7, 2025, 6:11pm Nina, thank you for the thoughtful words and for engaging so directly with these architectural questions. Nina, thank you for the thoughtful words and for engaging so directly with these architectural questions. To your point: Packs, as currently conceived, remain constrained by several limitations I’ve outlined over time—chief among them, their closed nature and limited support for dynamic integration patterns. These constraints have historically hampered Coda’s ability to serve as a true orchestration hub, especially when it comes to handling real-world, real-time data flows. To your point: Packs, as currently conceived, remain constrained by several limitations I’ve outlined over time—chief among them, their closed nature and limited support for dynamic integration patterns. These constraints have historically hampered Coda’s ability to serve as a true orchestration hub, especially when it comes to handling real-world, real-time data flows. However, with Grammarly’s expanding reach across critical “surfaces”—and their likely trajectory toward integrating voice, vision, and other modalities—there’s a strong possibility that Packs will evolve. In this emerging context, Packs could become the ideal conduit for retrieving context-rich, multimodal data from the broader Grammarly ecosystem. Rather than being a bottleneck, Packs may soon serve as the connective tissue between Coda’s orchestration framework and the vast, real-time data streams generated by Grammarly’s “mothership.”

However, with Grammarly’s expanding reach across critical “surfaces”—and their likely trajectory toward integrating voice, vision, and other modalities—there’s a strong possibility that Packs will evolve. In this emerging context, Packs could become the ideal conduit for retrieving context-rich, multimodal data from the broader Grammarly ecosystem. Rather than being a bottleneck, Packs may soon serve as the connective tissue between Coda’s orchestration framework and the vast, real-time data streams generated by Grammarly’s “mothership.” The challenge, and the opportunity, is for Packs to move beyond their current boundaries—adapting to new integration standards and supporting the kind of pervasive, intelligent data exchange that true agentic orchestration demands. The challenge, and the opportunity, is for Packs to move beyond their current boundaries—adapting to new integration standards and supporting the kind of pervasive, intelligent data exchange that true agentic orchestration demands. 1 Like Bill_French July 7, 2025, 6:19pm Perhaps relevant - I can go from email to LLM to Coda without ever copying/pasting, ChatGPT, or any intermediate tool. Dia carries the weight of shortening the line from information to curation. Perhaps relevant - I can go from email to LLM to Coda without ever copying/pasting, ChatGPT, or any intermediate tool. Dia carries the weight of shortening the line from information to curation. CleanShot 2025-07-07 at 12.15.20@2x1920×1588 146 KB

CleanShot 2025-07-07 at 12.15.20@2x 1920×1588 146 KB 2 Likes

Related topics

Related topics Replies Activity Coda AI and the User Experience October 8, 2023 Aurora - Easily Craft Prompts That SELL AI at Work Challenge August 7, 2023 Beyond The Prompt AI at Work Challenge August 16, 2023 Coda AI - It's not me its you October 6, 2023 Coda/Grammarly Self-Help Integration: Forget About It April 25, 2025:preload-content","**The Chatbot is Dead. Long Live the Orchestrator**



preload-content:

The Chatbot is Dead. Long Live the Orchestrator

The Chatbot is Dead. Long Live the Orchestrator Bill_French July 1, 2025, 3:20pm While the world was distracted by talking dolls, Grammarly quietly built an AI that could act. They didn’t use a better model. They used a better weapon: Coda. While the world was distracted by talking dolls, Grammarly quietly built an AI that could act. They didn’t use a better model. They used a better weapon: Coda. Your Prompts Are a Prayer to an Amnesiac God

Key points:

• Your Prompts Are a Prayer to an Amnesiac God Let’s be brutally honest.

• The entire AI industry is captivated by a lie.

• We’ve anointed “prompt engineering” as a mystical art, a high priesthood for coaxing wisdom from silicon gods.

• We are meticulously polishing the conversational skills of a machine with terminal amnesia.

• Let’s be brutally honest.

The entire AI industry is captivated by a lie. We’ve anointed “prompt engineering” as a mystical art, a high priesthood for coaxing wisdom from silicon gods. It’s a sham. We are meticulously polishing the conversational skills of a machine with terminal amnesia. We were promised an omniscient partner, an AI co-pilot. What we got was a brilliant intern with no long-term memory, an entity we must re-brief from scratch every five minutes. This isn’t productivity. It’s digital babysitting for a machine that can recite Shakespeare but struggles to remember your name or perform precise calculations. We were promised an omniscient partner, an AI co-pilot. What we got was a brilliant intern with no long-term memory, an entity we must re-brief from scratch every five minutes. This isn’t productivity. It’s digital babysitting for a machine that can recite Shakespeare but struggles to remember your name or perform precise calculations. Today’s large language models are amnesiacs by design. We celebrate their ballooning context windows—a million, two million tokens—as if it were memory. It’s not. It’s a bigger notepad. A volatile transcript that dissolves into the ether the moment you close the tab. This architecture condemns us to a state of digital shrapnel: your project plan lives in Slack, your research is scattered across browser tabs, your decisions are buried in email, and your draft is in a doc. We ask our AI to be intelligent, but we force it to operate blindfolded, guessing the shape of our work by touching one disconnected piece at a time. Today’s large language models are amnesiacs by design. We celebrate their ballooning context windows—a million, two million tokens—as if it were memory. It’s not. It’s a bigger notepad. A volatile transcript that dissolves into the ether the moment you close the tab. This architecture condemns us to a state of digital shrapnel: your project plan lives in Slack, your research is scattered across browser tabs, your decisions are buried in email, and your draft is in a doc. We ask our AI to be intelligent, but we force it to operate blindfolded, guessing the shape of our work by touching one disconnected piece at a time. This makes you the AI’s external hard drive. You are the connective tissue. You perform the soul-crushing labor of copying, pasting, and re-explaining context, bridging the gap with every single query. This isn’t a feature. It’s a catastrophic, unforgivable design flaw. This makes you the AI’s external hard drive. You are the connective tissue. You perform the soul-crushing labor of copying, pasting, and re-explaining context, bridging the gap with every single query. This isn’t a feature. It’s a catastrophic, unforgivable design flaw. Stop Describing. Start Commanding. Stop Describing. Start Commanding. For a moment, I thought the answer was contexting—the architectural discipline of curating a rich data environment for an AI agent. It was the right instinct, but the wrong verb. It was a step away from the vacant art of prompting, but it was still just talking at the machine. For a moment, I thought the answer was contexting—the architectural discipline of curating a rich data environment for an AI agent. It was the right instinct, but the wrong verb. It was a step away from the vacant art of prompting, but it was still just talking at the machine. You cannot build a skyscraper by describing it to a pile of bricks, no matter how eloquently. You need an architectural plan, a crane, and a crew. Prompting is the description. Coda is the crane and the blueprint. It transforms your context from a static pile of information into a dynamic set of executable instructions. You cannot build a skyscraper by describing it to a pile of bricks, no matter how eloquently. You need an architectural plan, a crane, and a crew. Prompting is the description. Coda is the crane and the blueprint. It transforms your context from a static pile of information into a dynamic set of executable instructions. You cannot build a skyscraper by describing it to a pile of bricks, no matter how eloquently. You need an architectural plan, a crane, and a crew. Prompting is the description. Coda is the crane and the blueprint. It transforms your context from a static pile of information into a dynamic set of executable instructions. The true frontier isn’t what an AI knows. It’s what it can do. This demands a new class of software: an Agentic Orchestration Framework. A system that doesn’t just talk, but commands, coordinates, and executes. It directs multiple specialized agents—AI, automation, and human—across complex, multi-step workflows with unwavering precision. The true frontier isn’t what an AI knows. It’s what it can do. This demands a new class of software: an Agentic Orchestration Framework. A system that doesn’t just talk, but commands, coordinates, and executes. It directs multiple specialized agents—AI, automation, and human—across complex, multi-step workflows with unwavering precision. The archetype for this framework has been hiding in plain sight: Coda. The archetype for this framework has been hiding in plain sight: Forget the “all-in-one doc” marketing. That was the Trojan horse. Coda is a workflow engine for manufacturing bespoke, agentic software without writing a line of code. Its architecture is the very blueprint for orchestration:

Key points:

• Forget the “all-in-one doc” marketing.

• That was the Trojan horse.

• Coda is a workflow engine for manufacturing bespoke, agentic software without writing a line of code.

• Its architecture is the very blueprint for orchestration: • Docs as the Command Center: The unified surface where human intent and AI execution converge.

• Docs as the Command Center: The unified surface where human intent and AI execution converge.

• • Structured Tables as the Memory: A shared, persistent “brain” that provides unwavering context, rendering the amnesiac chat log obsolete.

Structured Tables as the Memory: A shared, persistent “brain” that provides unwavering context, rendering the amnesiac chat log obsolete. • Packs as the Limbs: The API-driven connectors that give agents power over the real world—to manipulate Google Calendar, create Jira tickets, or rewrite Salesforce records. Packs as the Limbs: The API-driven connectors that give agents power over the real world—to manipulate Google Calendar, create Jira tickets, or rewrite Salesforce records. • Automations as the Nervous System: The rule-based engine that triggers actions and executes entire workflows with inhuman speed and reliability. Automations as the Nervous System: The rule-based engine that triggers actions and executes entire workflows with inhuman speed and reliability. CleanShot 2025-07-01 at 09.14.43@2x1274×842 43.6 KB

CleanShot 2025-07-01 at 09.14.43@2x 1274×842 43.6 KB This was never a document app. It’s a factory for building intelligent actors. This was never a document app. It’s a factory for building intelligent actors. The Grammarly Gambit: An Empire in Two Moves

Key points:

• The Grammarly Gambit: An Empire in Two Moves While the market obsessed over chatbot demos and press releases, Grammarly executed a two-step strategic coup to build the world’s first true agentic productivity platform.

• Anyone who saw these as unrelated acquisitions wasn’t just missing the story; they were illiterate in the language of power.

• While the market obsessed over chatbot demos and press releases, Grammarly executed a two-step strategic coup to build the world’s first true agentic productivity platform.

• Anyone who saw these as unrelated acquisitions wasn’t just missing the story; they were illiterate in the language of power.

• Move 1 (Acquire the Brain): Seize the Orchestration Framework
In late 2024, Grammarly acquired Coda.

• They didn’t buy a popular doc app.

They bought the operating system for their future AI agents. This was the foundational act of war. As undeniable proof, Coda founder Shishir Mehrotra wasn’t just given a board seat; he was installed as Grammarly’s new CEO. He isn’t running a company; he is performing a hostile takeover of its DNA, injecting Coda’s agentic framework into a platform with 40 million daily active users. Move 1 (Acquire the Brain): Seize the Orchestration Framework
In late 2024, Grammarly acquired Coda. They didn’t buy a popular doc app. They bought the operating system for their future AI agents. This was the foundational act of war. As undeniable proof, Coda founder Shishir Mehrotra wasn’t just given a board seat; he was installed as Grammarly’s new CEO. He isn’t running a company; he is performing a hostile takeover of its DNA, injecting Coda’s agentic framework into a platform with 40 million daily active users. Move 2 (Conquer the Battlefield): Seize the Critical Interface
Months later, Grammarly acquired Superhuman. This was not about adding a slick email client. Superhuman is what Mehrotra calls the “perfect staging ground for orchestrating multiple AI agents simultaneously.” Email is the chaotic nexus where work, communication, and tasks collide. Grammarly didn’t buy Superhuman for its pathetic summarization features; they bought the most valuable turf in professional life to serve as the GUI for their Coda-powered agentic backend. Imagine it: A sales agent, a support agent, and a scheduling agent collaborating within a single email draft, orchestrated by the Coda engine, pulling live context from connected Packs, executing tasks across a dozen SaaS apps. That is the gambit. Move 2 (Conquer the Battlefield): Seize the Critical Interface
Months later, Grammarly acquired Superhuman. This was not about adding a slick email client. Superhuman is what Mehrotra calls the “perfect staging ground for orchestrating multiple AI agents simultaneously.” Email is the chaotic nexus where work, communication, and tasks collide. Grammarly didn’t buy Superhuman for its pathetic summarization features; they bought the most valuable turf in professional life to serve as the GUI for their Coda-powered agentic backend. Imagine it: A sales agent, a support agent, and a scheduling agent collaborating within a single email draft, orchestrated by the Coda engine, pulling live context from connected Packs, executing tasks across a dozen SaaS apps. That is the gambit. CleanShot 2025-07-01 at 09.17.51@2x1434×992 78 KB

CleanShot 2025-07-01 at 09.17.51@2x 1434×992 78 KB

Your AI Stack is a Museum Piece

Key points:

• Your AI Stack is a Museum Piece The chasm between the dying paradigm and the emerging one is not an increment; it is a cliff.

• One is a toy, the other is a weapon.

• One talks, the other acts.

• The stack Grammarly is building does not compete with the old one.

• It renders it irrelevant.

• The chasm between the dying paradigm and the emerging one is not an increment; it is a cliff.

One is a toy, the other is a weapon. One talks, the other acts. The stack Grammarly is building does not compete with the old one. It renders it irrelevant. The Moat Isn’t the Model; It’s the Machine

Key points:

• The Moat Isn’t the Model; It’s the Machine The winners of the AI war will not be the companies with the largest language model.

• That is a commodity race to the bottom.

• They will be the ones who own the orchestration framework that makes those models act.

• Building a better chatbot today is like perfecting the horse-drawn carriage in the age of the automobile.

• The real innovation was the assembly line and the highway system.

• The only defensible moat is the machine: the framework that enables action, the integrations that give it reach, the structured data that serves as its memory, and the user workflows that become its territory.

The winners of the AI war will not be the companies with the largest language model. That is a commodity race to the bottom. They will be the ones who own the orchestration framework that makes those models act. Building a better chatbot today is like perfecting the horse-drawn carriage in the age of the automobile. The real innovation was the assembly line and the highway system. The only defensible moat is the machine: the framework that enables action, the integrations that give it reach, the structured data that serves as its memory, and the user workflows that become its territory. Stop asking if your AI is smart. Start demanding that it act. Stop celebrating prompts. Start building engines of execution. Stop asking if your AI is smart. Start demanding that it act. Stop celebrating prompts. Start building engines of execution. The future of work isn’t a conversation. It’s a command. The companies that understand this are building empires. The rest are polishing tombstones. The future of work isn’t a conversation. It’s a command. The companies that understand this are building empires. The rest are polishing tombstones. ps. A warm welcome to Rahul and the SuperHuman team.

ps. A warm welcome to Rahul and the SuperHuman team. 15 Likes Exciting News: Grammarly to acquire Superhuman Christiaan_Huizer July 1, 2025, 4:13pm thx for sharing your ideas. interesting & promising.
what do you believe would be a next tool to acquire @Bill_French?

thx for sharing your ideas. interesting & promising.
what do you believe would be a next tool to acquire @Bill_French 2 Likes Melanie_Teh July 1, 2025, 4:26pm my $$ is on reclaim or motion… something along the lines of calendar/task management

my $$ is on reclaim or motion… something along the lines of calendar/task management edit: oh, cannot be reclaim as they got bought by dropbox

edit: oh, cannot be reclaim as they got bought by dropbox 1 Like Bill_French July 1, 2025, 4:30pm I’m uncertain, but I have to believe someone at Grammarly (who is apparently really dialed in to the way I think) is watching Pieces, Flowith, and Dia. I’m uncertain, but I have to believe someone at Grammarly (who is apparently really dialed in to the way I think) is watching Pieces Flowith 5 Likes Agile_Dynamics July 2, 2025, 11:21pm Shishir keeps talking about ‘surfaces’. Email is the most frequent surface for Grammarly usage (=>Superhuman)
Documents and sheets are another major surface (=>Coda)

Key points:

• Shishir keeps talking about ‘surfaces’.

• Email is the most frequent surface for Grammarly usage (=>Superhuman)
Documents and sheets are another major surface (=>Coda) So whats the next-biggest surface where we spend our time? So whats the next-biggest surface where we spend our time? How about messaging and chats and forums? How about messaging and chats and forums? So maybe Grammarly has it’s eye on one of those? Not the major players maybe, but a startup with oodles of innovation and AI expertise? So maybe Grammarly has it’s eye on one of those? Not the major players maybe, but a startup with oodles of innovation and AI expertise? Another key criteria for these acquisitions is a shared vision.

• It was highlighted during the Coda deal.

• It’s highlighted again by Shishir and Rahul during the Superhuman deal.

• Another key criteria for these acquisitions is a shared vision.

• It was highlighted during the Coda deal.

It’s highlighted again by Shishir and Rahul during the Superhuman deal. So the next target will have a messaging surface, an AI focus, and a clear vision that matches that of Shashir, Rahul, and their teams. So the next target will have a messaging surface, an AI focus, and a clear vision that matches that of Shashir, Rahul, and their teams. I dont know the marketplace well enough to identify candidates, but perhaps someone reading this does? I dont know the marketplace well enough to identify candidates, but perhaps someone reading this does? Just a thought. Just a thought. 2 Likes Nina_Ledid July 3, 2025, 9:03am As always, I’ve greatly enjoyed reading your perspective, @Bill_French, thanks for sharing! As always, I’ve greatly enjoyed reading your perspective, @Bill_French, thanks for sharing! I do remember from past conversations your concerns about Coda Pack’s limited support for common integration patterns (eg handling incoming webhooks, running persistent background services, or responding to external events)

I do remember from past conversations your concerns about Coda Pack’s limited support for common integration patterns (eg handling incoming webhooks, running persistent background services, or responding to external events) In your post above, you envision Coda as the blueprint for a new kind of orchestration framework. In your post above, you envision Coda as the blueprint for a new kind of orchestration framework. Do you think what once seemed like a limitation (Packs’ closed and controlled architecture) is now less of a drawback? Do you think what once seemed like a limitation (Packs’ closed and controlled architecture) is now less of a drawback? Or do you even see it potentially becoming a competitive advantage as the focus shifts from open interoperability to orchestrated execution? Or do you even see it potentially becoming a competitive advantage as the focus shifts from open interoperability to orchestrated execution? Thanks,
Nina

Thanks,
• Likes Stefan_Stoyanov July 5, 2025, 8:17pm The reason I’m not yet bullish on Coda’s future is due to one key problem that LLMs still don’t solve particularly well: data collection. Specifically, I’ve been thinking a lot about the following limitations of Coda, as I understand its vision going forward:

Key points:

• The reason I’m not yet bullish on Coda’s future is due to one key problem that LLMs still don’t solve particularly well: data collection.

• Specifically, I’ve been thinking a lot about the following limitations of Coda, as I understand its vision going forward: • Collecting data by typing into Coda’s quadrangle notes (cells) is too slow and impractical.

• Instead, data should come from voice/video meetings and recordings, mobile/desktop screen activity, linked/shared content sources, mobile phone conversations, and visual/audible/touch perception input devices.

• Pushing data from hardware devices in real time becomes even more critical than pulling it from structured database, because if data isn’t captured quickly enough, the use of the database can become unreliable and irrelevant even within a minute.

• Collecting data by typing into Coda’s quadrangle notes (cells) is too slow and impractical.

• Instead, data should come from voice/video meetings and recordings, mobile/desktop screen activity, linked/shared content sources, mobile phone conversations, and visual/audible/touch perception input devices.

Pushing data from hardware devices in real time becomes even more critical than pulling it from structured database, because if data isn’t captured quickly enough, the use of the database can become unreliable and irrelevant even within a minute. • Email is no longer a practical communication channel - it belongs to the past. As someone who has long supported email, it’s hard for me to admit this, but emails are now like the written contracts of a previous era. Today, very little needs to be formally written when people prefer adaptable, relevant content in audio/video formats. Email is no longer a practical communication channel - it belongs to the past. As someone who has long supported email, it’s hard for me to admit this, but emails are now like the written contracts of a previous era. Today, very little needs to be formally written when people prefer adaptable, relevant content in audio/video formats. • The joint venture company’s language support is essentially limited to English at the moment. In an era of LLMs—where tokenization of language is foundational to progress—this is concerning. Many LLMs still perform poorly in languages other than English. What does it mean to build a great app for the UK but a terrible one for France, simply because UX in the UK relies on quick chat-based communication, while in France it might depend on elaborate instructions—or worse, traditional filters and search? The joint venture company’s language support is essentially limited to English at the moment. In an era of LLMs—where tokenization of language is foundational to progress—this is concerning. Many LLMs still perform poorly in languages other than English. What does it mean to build a great app for the UK but a terrible one for France, simply because UX in the UK relies on quick chat-based communication, while in France it might depend on elaborate instructions—or worse, traditional filters and search? My understanding of LLMs and Coda’s vision is limited, so I’d really appreciate the community’s thoughts on these points. My understanding of LLMs and Coda’s vision is limited, so I’d really appreciate the community’s thoughts on these points. 1 Like Christiaan_Huizer July 6, 2025, 11:53am HI @Stefan_Stoyanov

Key points:

• @Stefan_Stoyanov Thanks for sharing your thoughts, those are valid concerns.

• Just so you know, I don’t have access to any internal “cookbook” or strategic plans.

• These are simply my best guesses and interpretations based on what’s publicly available and general market trends.

• Thanks for sharing your thoughts, those are valid concerns.

• Just so you know, I don’t have access to any internal “cookbook” or strategic plans.

• These are simply my best guesses and interpretations based on what’s publicly available and general market trends.

Regarding your first question about data collection, there are already many AI-powered tools out there that are really good at capturing voice from meetings, whether you’re in the room or online. These tools can pull out and organize key information, remarks, and action items from conversations. When we think about the “surfaces” and “orchestration” Grammarly is building, meeting platforms are a big communication area. It makes sense that Grammarly might look to acquire a company in this field. They’d likely be looking for strong AI features, existing connections to other tools, potential for their “agents” to expand, and solid support for many languages. Companies like Sembly AI, Fireflies.ai, Otter.ai, or MeetGeek come to mind. Regarding your first question about data collection, there are already many AI-powered tools out there that are really good at capturing voice from meetings, whether you’re in the room or online. These tools can pull out and organize key information, remarks, and action items from conversations. When we think about the “surfaces” and “orchestration” Grammarly is building, meeting platforms are a big communication area. It makes sense that Grammarly might look to acquire a company in this field. They’d likely be looking for strong AI features, existing connections to other tools, potential for their “agents” to expand, and solid support for many languages. Companies like Sembly AI Fireflies.ai Otter.ai MeetGeek come to mind. As for your comment on email, that’s an an interesting thought. While other ways of communicating have certainly grown, email is still a core, if not the core, communication hub for many businesses. That’s actually why Grammarly bought Superhuman. It’s not just about email itself; it’s also about the related areas Superhuman is working on, like chats, calendars, and tasks (direct overlap with Coda). The whole idea is that all these interactions can be managed and brought together within Coda, building out an AI-powered productivity system that covers all the main places where work happens. As for your comment on email, that’s an an interesting thought. While other ways of communicating have certainly grown, email is still a core, if not core, communication hub for many businesses. That’s actually why Grammarly bought Superhuman. It’s not just about email itself; it’s also about the related areas Superhuman is working on, like chats, calendars, and tasks (direct overlap with Coda). The whole idea is that all these interactions can be managed and brought together within Coda, building out an AI-powered productivity system that covers all the main places where work happens. Finally, on your point about language support for LLMs, Superhuman already includes AI-driven translation right within its email platform. This feature allows for easy translation of messages, which is super important for international teams and global operations. This capability directly tackles language barriers in a crucial communication space, showing that Grammarly is focused on making its AI tools truly effective for a wide range of users worldwide. Finally, on your point about language support for LLMs, Superhuman already includes AI-driven translation right within its email platform. This feature allows for easy translation of messages, which is super important for international teams and global operations. This capability directly tackles language barriers in a crucial communication space, showing that Grammarly is focused on making its AI tools truly effective for a wide range of users worldwide. welcoming further feedback.

welcoming further feedback. Chers, christiaan

Key points:

• Chers, christiaan 3 Likes Bill_French July 7, 2025, 6:05pm Indeed.

• It’s why I use Pieces.

• It captures everything, and I can use it as a long-term memory or a funnel into Coda and other tools.

• Capturing contexts is probably where Grammarly is heading.

• It’s why I use Pieces.

It captures everything, and I can use it as a long-term memory or a funnel into Coda and other tools. Capturing contexts is probably where Grammarly is heading. For approximately 2.4 billion people, it remains practical in most cases. It’s not going anywhere, at least not in your lifetime. If you see ‘email’ when you see Superhuman, you need to recalibrate. The AI engine in this tool was perfected in one of the harshest environments, but it is applicable in many operational domains. They didn’t buy Superhuman to support email better. For approximately 2.4 billion people, it remains practical in most cases. It’s not going anywhere, at least not in your lifetime. If you see ‘email’ when you see Superhuman, you need to recalibrate. The AI engine in this tool was perfected in one of the harshest environments, but it is applicable in many operational domains. They didn’t buy Superhuman to support email better. This is not Grammarly’s problem to solve. I think some of the higher-level executives and engineers have seen solutions that will address the multilingual challenges. This is not Grammarly’s problem to solve. I think some of the higher-level executives and engineers have seen solutions that will address the multilingual challenges. 1 Like Bill_French July 7, 2025, 6:11pm Nina, thank you for the thoughtful words and for engaging so directly with these architectural questions. Nina, thank you for the thoughtful words and for engaging so directly with these architectural questions. To your point: Packs, as currently conceived, remain constrained by several limitations I’ve outlined over time—chief among them, their closed nature and limited support for dynamic integration patterns. These constraints have historically hampered Coda’s ability to serve as a true orchestration hub, especially when it comes to handling real-world, real-time data flows. To your point: Packs, as currently conceived, remain constrained by several limitations I’ve outlined over time—chief among them, their closed nature and limited support for dynamic integration patterns. These constraints have historically hampered Coda’s ability to serve as a true orchestration hub, especially when it comes to handling real-world, real-time data flows. However, with Grammarly’s expanding reach across critical “surfaces”—and their likely trajectory toward integrating voice, vision, and other modalities—there’s a strong possibility that Packs will evolve. In this emerging context, Packs could become the ideal conduit for retrieving context-rich, multimodal data from the broader Grammarly ecosystem. Rather than being a bottleneck, Packs may soon serve as the connective tissue between Coda’s orchestration framework and the vast, real-time data streams generated by Grammarly’s “mothership.”

Key points:

• However, with Grammarly’s expanding reach across critical “surfaces”—and their likely trajectory toward integrating voice, vision, and other modalities—there’s a strong possibility that Packs will evolve.

• In this emerging context, Packs could become the ideal conduit for retrieving context-rich, multimodal data from the broader Grammarly ecosystem.

• Rather than being a bottleneck, Packs may soon serve as the connective tissue between Coda’s orchestration framework and the vast, real-time data streams generated by Grammarly’s “mothership.” The challenge, and the opportunity, is for Packs to move beyond their current boundaries—adapting to new integration standards and supporting the kind of pervasive, intelligent data exchange that true agentic orchestration demands.

• The challenge, and the opportunity, is for Packs to move beyond their current boundaries—adapting to new integration standards and supporting the kind of pervasive, intelligent data exchange that true agentic orchestration demands.

• 1 Like Bill_French July 7, 2025, 6:19pm Perhaps relevant - I can go from email to LLM to Coda without ever copying/pasting, ChatGPT, or any intermediate tool.

• Dia carries the weight of shortening the line from information to curation.

Perhaps relevant - I can go from email to LLM to Coda without ever copying/pasting, ChatGPT, or any intermediate tool. Dia carries the weight of shortening the line from information to curation. CleanShot 2025-07-07 at 12.15.20@2x1920×1588 146 KB

CleanShot 2025-07-07 at 12.15.20@2x 1920×1588 146 KB 2 Likes

Related topics

Related topics Replies Activity Coda AI and the User Experience October 8, 2023 Aurora - Easily Craft Prompts That SELL AI at Work Challenge August 7, 2023 Beyond The Prompt AI at Work Challenge August 16, 2023 Coda AI - It's not me its you October 6, 2023 Coda/Grammarly Self-Help Integration: Forget About It April 25, 2025:preload-content",1.0
https://huggingface.co/apple/DiffuCoder-7B-cpGRPO,apple/DiffuCoder-7B-cpGRPO · Hugging Face,Artificial Intelligence,574,2025-07-08T16:24:04.849292,huggingface.co,Sai,U07TKPGTKDF,2025-07-02T09:58:33.027069,,website,A product page detailing the DiffuCoder-7B-cpGRPO model for code generation on Hugging Face.,,"HTML_TAG_START

DiffuCoder-7B-cpGRPO

DiffuCoder-7B-cpGRPO The DiffuCoder-7B-cpGRPO variant further refines DiffuCoder-Instruct with reinforcement learning via Coupled-GRPO. The DiffuCoder-7B-cpGRPO variant further refines DiffuCoder-Instruct with reinforcement learning via Coupled-GRPO. Training recipe:

Training recipe: • Initialized from DiffuCoder-7B-Instruct, post-training with coupled-GRPO on 21K code data (1 epoch). Initialized from DiffuCoder-7B-Instruct, post-training with coupled-GRPO on 21K code data (1 epoch). • coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4% on EvalPlus) and reduces reliance on AR bias during decoding.
coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4% on EvalPlus) and reduces reliance on AR bias during decoding. More details and usage examples:

More details and usage examples: • Paper: DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation
Paper: DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

Paper: DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation • GitHub: https://github.com/apple/ml-diffucoder
GitHub: https://github.com/apple/ml-diffucoder

GitHub: https://github.com/apple/ml-diffucoder
import torch
from transformers import AutoModel, AutoTokenizer

model_path = ""apple/DiffuCoder-7B-cpGRPO""
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to(""cuda"").eval()

query = ""Write a function to find the shared elements from the given two lists.""
prompt = f""""""<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{query.strip()}
<|im_end|>
<|im_start|>assistant
"""""" ## following the template of qwen; you can also use apply_chat_template function

TOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens

inputs = tokenizer(prompt, return_tensors=""pt"")
input_ids = inputs.input_ids.to(device=""cuda"")
attention_mask = inputs.attention_mask.to(device=""cuda"")

output = model.diffusion_generate(
input_ids,
attention_mask=attention_mask,
max_new_tokens=256,
output_history=True,
return_dict_in_generate=True,
steps=256//TOKEN_PER_STEP,
temperature=0.4,
top_p=0.95,
alg=""entropy"",
alg_temp=0.,
)
generations = [
tokenizer.decode(g[len(p):].tolist())
for p, g in zip(input_ids, output.sequences)
]

print(generations[0].split('<|dlm_pad|>')[0])

import torch
from transformers import AutoModel, AutoTokenizer

model_path = ""apple/DiffuCoder-7B-cpGRPO""
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to(""cuda"").eval()

query = ""Write a function to find the shared elements from the given two lists.""
prompt = f""""""<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{query.strip()}
<|im_end|>
<|im_start|>assistant
"""""" ## following the template of qwen; you can also use apply_chat_template function

TOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens

inputs = tokenizer(prompt, return_tensors=""pt"")
input_ids = inputs.input_ids.to(device=""cuda"")
attention_mask = inputs.attention_mask.to(device=""cuda"")

output = model.diffusion_generate(
input_ids,
attention_mask=attention_mask,
max_new_tokens=256,
output_history=True,
return_dict_in_generate=True,
steps=256//TOKEN_PER_STEP,
temperature=0.4,
top_p=0.95,
alg=""entropy"",
alg_temp=0.,
)
generations = [
tokenizer.decode(g[len(p):].tolist())
for p, g in zip(input_ids, output.sequences)
]

print(generations[0].split('<|dlm_pad|>')[0])

import torch
from transformers import AutoModel, AutoTokenizer

model_path = ""apple/DiffuCoder-7B-cpGRPO""
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to(""cuda"").eval()

query = ""Write a function to find the shared elements from the given two lists.""
prompt = f""""""<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{query.strip()}
<|im_end|>
<|im_start|>assistant
"""""" ## following the template of qwen; you can also use apply_chat_template function

TOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens

inputs = tokenizer(prompt, return_tensors=""pt"")
input_ids = inputs.input_ids.to(device=""cuda"")
attention_mask = inputs.attention_mask.to(device=""cuda"")

output = model.diffusion_generate(
input_ids,
attention_mask=attention_mask,
max_new_tokens=256,
output_history=True,
return_dict_in_generate=True,
steps=256//TOKEN_PER_STEP,
temperature=0.4,
top_p=0.95,
alg=""entropy"",
alg_temp=0.,
)
generations = [
tokenizer.decode(g[len(p):].tolist())
for p, g in zip(input_ids, output.sequences)
]

print(generations[0].split('<|dlm_pad|>')[0])

Acknowledgement

Acknowledgement To power this HuggingFace model release, we reuse Dream's modeling architecture and generation utils. To power this HuggingFace model release, we reuse 's modeling architecture and generation utils. HTML_TAG_END Downloads last month HTML_TAG_START HTML_TAG_END Safetensors Model size 7.62B params Tensor type Chat template

Chat template Files info Inference Providers This model isn't deployed by any Inference Provider. Ask for provider support

Model tree for apple/DiffuCoder-7B-cpGRPO

Model tree for apple/DiffuCoder-7B-cpGRPO Base model Qwen/Qwen2.5-7B Finetuned Qwen/Qwen2.5-Coder-7B Finetuned apple/DiffuCoder-7B-Base Finetuned apple/DiffuCoder-7B-Instruct Finetuned this model Quantizations 3 models

Collection including
apple/DiffuCoder-7B-cpGRPO

Collection including apple/DiffuCoder-7B-cpGRPO 4 items Updated 4 days ago","**Website Description:**

The Hugging Face page for DiffuCoder-7B-cpGRPO provides an in-depth overview of a refined machine learning model designed for code generation. This variant enhances the original DiffuCoder-Instruct model through reinforcement learning techniques, specifically Coupled-GRPO, which significantly boosts performance on code generation benchmarks. Users can find detailed training recipes, usage examples, and links to relevant resources, including a research paper and a GitHub repository for further exploration.

This resource is particularly valuable for developers, researchers, and data scientists interested in advanced code generation techniques and machine learning model optimization. The page serves as a comprehensive guide for understanding the capabilities of the DiffuCoder-7B-cpGRPO model, making it an essential tool for those looking to implement cutting-edge AI solutions in their projects.",1.0
